<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>GPT-OSS Attnention Sink - Go Kamoda</title>
<meta name="description" content="The bias term for preventing attention sink in GPT-OSS may have other effects than just preventing attention sink.">


  <meta name="author" content="Go Kamoda">
  
  <meta property="article:author" content="Go Kamoda">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Go Kamoda">
<meta property="og:title" content="GPT-OSS Attnention Sink">
<meta property="og:url" content="https://gokamoda.github.io/notes/oss/">


  <meta property="og:description" content="The bias term for preventing attention sink in GPT-OSS may have other effects than just preventing attention sink.">



  <meta property="og:image" content="https://gokamoda.github.io/assets/img/gptoss-bias.png">





  <meta property="article:published_time" content="2025-05-12T00:00:00+00:00">





  

  


<link rel="canonical" href="https://gokamoda.github.io/notes/oss/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Go Kamoda Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="icon" type="image/png" href=/assets/favicon/favicon-96x96.png sizes="96x96" />
<link rel="icon" type="image/svg+xml" href=/assets/favicon/favicon.svg />
<link rel="shortcut icon" href=/assets/favicon/favicon.ico />
<link rel="apple-touch-icon" sizes="180x180" href=/assets/favicon/apple-touch-icon.png />
<link rel="manifest" href=/assets/favicon/site.webmanifest />

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Color+Emoji&family=Noto+Emoji:wght@300..700&display=swap" rel="stylesheet">
<!-- end custom head snippets -->

  </head>

  <body class="layout--splash" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          G<img src=/assets/enaga-transparent-blackline.svg alt="" class="img-site-title"/>&nbsp;Kamoda
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/cv/"
                
                
              >CV</a>
            </li><li class="masthead__menu-item">
              <a
                href="/notes/"
                
                
              >Notes</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style="background-color: #59876F; background-image: url('');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          GPT-OSS Attnention Sink

        
      </h1>
      
      

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-05-12T00:00:00+00:00">May 12, 2025</time>
      </span>
    

    

    
  </p>


      
    </div>
  
  
</div>



<div id="main" role="main">
  <article class="splash" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="GPT-OSS Attnention Sink">
    <meta itemprop="description" content="The bias term for preventing attention sink in GPT-OSS may have other effects than just preventing attention sink.">
    <meta itemprop="datePublished" content="2025-05-12T00:00:00+00:00">
    
    <section class="page__content" itemprop="text">
      <p>tl;dr: The bias term for preventing attention sink in GPT-OSS may have other effects than just preventing attention sink.
(made public on 2026-01-22)</p>

<h2 id="notation">Notation</h2>

\[\begin{alignat}{4}
    &amp;\bm{X} &amp;:= &amp;
    \begin{bmatrix}
        \bm{x}_1\\
        \vdots\\
        \bm{x}_n
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{n \times d}\\
    &amp;\bm{W}^O &amp;:= &amp;
    \begin{bmatrix}
        \bm{W}^O_1\\
        \vdots\\
        \bm{W}^O_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d \times d}\\
    &amp;\bm{W}^Q &amp;:= &amp;
    \begin{bmatrix}
        \bm{W}^Q_1 &amp; \cdots &amp; \bm{W}^Q_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d \times d}&amp; \label{eq:wq_split}\\
    &amp;\bm{W}^K &amp;:= &amp;
    \begin{bmatrix}
        \bm{W}^K_1 &amp; \cdots &amp; \bm{W}^K_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d \times d}&amp; \label{eq:wk_split}\\
    &amp;\bm{W}^V &amp;:= &amp;
    \begin{bmatrix}
        \bm{W}^V_1 &amp; \cdots &amp; \bm{W}^V_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d \times d}&amp;\label{eq:wv_split}\\
    &amp;\bm{b}^Q &amp;:= &amp;
    \begin{bmatrix}
        \bm{b}^Q_1 &amp; \cdots &amp; \bm{b}^Q_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d}&amp; \label{eq:bq_split}\\
    &amp;\bm{b}^K &amp;:= &amp;
    \begin{bmatrix}
        \bm{b}^K_1 &amp; \cdots &amp; \bm{b}^K_H
    \end{bmatrix}
    &amp;\hspace{1em}\in&amp; \mathbb{R}^{d}&amp; \label{eq:bk_split}\\
    &amp;\bm{b}^V &amp;:= &amp;
    \begin{bmatrix}
        \bm{b}^V_1 &amp; \cdots &amp; \bm{b}^V_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d}&amp; \\
    &amp;\bm{I} &amp;:= &amp;
    \begin{bmatrix}
        1 &amp; 0 &amp; \cdots &amp; 0 \\
        0 &amp; 1 &amp; \cdots &amp; 0 \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 1 \\
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d\times d}&amp; \\
    &amp;\bm{1} &amp;:= &amp;
    \begin{bmatrix}
        1 &amp; \cdots &amp; 1
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d}
\end{alignat}\]

<h2 id="self-attention">Self-Attention</h2>

<p>Let query, key, value transformations of each head $h$ be expressed as follows:</p>

\[\begin{align}
    \bm{q}_h(\bm{x}) &amp;:= \bm{x}\bm{W}_h^Q + \bm{b}_h^Q\\
    \bm{k}_h(\bm{x}) &amp;:= \bm{x}\bm{W}_h^K + \bm{b}_h^K\\
    \bm{v}_h(\bm{x}) &amp;:= \bm{x}\bm{W}_h^V + \bm{b}_h^V\\
\end{align}\]

<p>Let attention weight from token position $i$ to $j$  ($i \ge j$) in head $h$ be expressed as follows:</p>

\[\begin{align}
\alpha_{i, j, h} &amp;= \frac{\exp(s_{i, j, h})}{\exp(b^{S}_h)+\sum_{j'} \exp(s_{i, j', h})}\\
s_{i, j, h} &amp;:= \frac{\bm{q}_h(\bm{x}_i)\bm{k}_h(\bm{x}_j)^\top}{\sqrt{d'}}
\end{align}\]

<p>where $dâ€™ = d/H$ is the dimension of each head, and $b^{S}_h$ is a learned scalar parameter introduced in GPT-OSS for preventing attention sink.</p>

<p>Now, let the attention weight assigned to the sink be expressed as follows:</p>

\[\alpha_{i, \text{sink}, h} := \frac{\exp(b^{S}_h)}{\exp(b^{S}_h)+\sum_{j'} \exp(s_{i, j', h})}\]

<p>Due to the presence of $b^{S}_h$, the following holds:</p>

\[\begin{align}
&amp;1 = \alpha_{i, \text{sink}, h} + \sum_j \alpha_{i, j, h}\\
&amp;\Leftrightarrow \sum_j \alpha_{i, j, h} = 1 - \alpha_{i, \text{sink}, h}
\end{align}\]

<p>The output of Attention layer of an causal model at position $i$ can be expressed as follows:</p>

\[\begin{align}
    \text{ATTN}(i, \bm{X})
        &amp;:=\left[\text{head}_1(i, \bm{X})\hspace{0.5em}\cdots\hspace{0.5em}\text{head}_H(i, \bm{X})\right]
            \bm{W}^O + \bm{b}^O\\
        &amp;=\sum_{h=1}^H \text{head}_h(i, \bm{X})\bm{W}^O_h + \bm{b}^O\\
        &amp;=\sum_{h=1}^H \left(\sum_{j=1}^i \alpha_{i, j, h} \bm{v}_h(\bm{x}_j)\right)\bm{W}^O_h + \bm{b}^O\\
        &amp;=\sum_{h=1}^H \left(\sum_{j=1}^i \alpha_{i, j, h} \left(\bm{x}_j\bm{W}^V_h + \bm{b}^V_h\right)\right)\bm{W}^O_h + \bm{b}^O\\
        &amp;=\sum_{h=1}^H \left(\sum_{j=1}^i \alpha_{i, j, h} \left(\bm{x}_j\bm{W}^V_h + \bm{b}^V_h\right)\bm{W}^O_h\right) + \bm{b}^O\\
        &amp;=\sum_{h=1}^H \left(\sum_{j=1}^i \alpha_{i, j, h} \left(\bm{x}_j\bm{W}^V_h\bm{W}^O_h + \bm{b}^V_h\bm{W}^O_h\right)\right) + \bm{b}^O\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h}\bm{x}_j\bm{W}^V_h\bm{W}^O_h + \sum_{h=1}^H \left(\sum_{j=1}^i \alpha_{i, j, h}\bm{b}^V_h\bm{W}^O_h\right) + \bm{b}^O\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h}\bm{x}_j\bm{W}^V_h\bm{W}^O_h + \sum_{h=1}^H \left(\sum_{j=1}^i \alpha_{i, j, h}\right)\bm{b}^V_h\bm{W}^O_h + \bm{b}^O\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h}\bm{x}_j\bm{W}^V_h\bm{W}^O_h + \sum_{h=1}^H \left(1-\alpha_{i, \text{sink}, h}\right)\bm{b}^V_h\bm{W}^O_h + \bm{b}^O\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h}\bm{x}_j\bm{W}^V_h\bm{W}^O_h + \sum_{h=1}^H\bm{b}^V_h\bm{W}^O_h - \sum_{h=1}^H\alpha_{i, \text{sink}, h}\bm{b}^V_h\bm{W}^O_h + \bm{b}^O\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h}\bm{x}_j\bm{W}^V_h\bm{W}^O_h + \sum_{h=1}^H\alpha_{i, \text{sink}, h}(-\bm{b}^V_h\bm{W}^O_h)+ \sum_{h=1}^H\bm{b}^V_h\bm{W}^O_h  + \bm{b}^O\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h}\bm{x}_j\bm{W}^V_h\bm{W}^O_h+ \sum_{h=1}^H\alpha_{i, \text{sink}, h}(-\bm{b}^V_h\bm{W}^O_h) + \bm{b}^V\bm{W}^O + \bm{b}^O\\
\end{align}\]

<p>Thus, in GPT-OSS, the newly introduced attention sink bias $b^{S}_h$</p>
<ul>
  <li>makes the attention weights sum to less than 1 for non-sink tokens, and</li>
  <li>changes the intensity of the bias term of the value transformation based on the attention weight assigned to the sink.</li>
</ul>

    </section>
  </article>
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        


<div class="page__footer-copyright">&copy; 2020 - 2026 <a href="https://gokamoda.github.io">Go Kamoda</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="/assets/js/copy_code.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DDQCS7FXG9"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DDQCS7FXG9', { 'anonymize_ip': false});
</script>








    
      <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js"],
      Macros: {
        bm: ["\\boldsymbol{#1}", 1],
        diag: ["\\mathrm{diag}\\left(#1\\right)", 1],
      },
      equationNumbers: { autoNumber: "all" } 
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });


  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
    alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
    alert("Math Processing Error: "+message[1]);
    });
</script>


<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </body>
</html>
