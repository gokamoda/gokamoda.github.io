<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Folding weights in transformers - Go Kamoda</title>
<meta name="description" content="Go Kamoda’s profile page.">


  <meta name="author" content="Go Kamoda">
  
  <meta property="article:author" content="Go Kamoda">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Go Kamoda">
<meta property="og:title" content="Folding weights in transformers">
<meta property="og:url" content="/notes/fold-weights/">


  <meta property="og:description" content="Go Kamoda’s profile page.">



  <meta property="og:image" content="/assets/img/folding-weights.png">





  <meta property="article:published_time" content="2025-05-12T00:00:00+00:00">





  

  


<link rel="canonical" href="/notes/fold-weights/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Go Kamoda Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="icon" type="image/png" href=/assets/favicon/favicon-96x96.png sizes="96x96" />
<link rel="icon" type="image/svg+xml" href=/assets/favicon/favicon.svg />
<link rel="shortcut icon" href=/assets/favicon/favicon.ico />
<link rel="apple-touch-icon" sizes="180x180" href=/assets/favicon/apple-touch-icon.png />
<link rel="manifest" href=/assets/favicon/site.webmanifest />

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Color+Emoji&family=Noto+Emoji:wght@300..700&display=swap" rel="stylesheet">
<!-- end custom head snippets -->

  </head>

  <body class="layout--splash" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          G<img src=/assets/enaga-transparent-blackline.svg alt="" class="img-site-title"/>&nbsp;Kamoda
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/cv/"
                
                
              >CV</a>
            </li><li class="masthead__menu-item">
              <a
                href="/notes/"
                
                
              >Notes</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style="background-color: #59876F; background-image: url('');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Folding weights in transformers

        
      </h1>
      
      

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-05-12T00:00:00+00:00">May 12, 2025</time>
      </span>
    

    

    
  </p>


      
    </div>
  
  
</div>



<div id="main" role="main">
  <article class="splash" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Folding weights in transformers">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="2025-05-12T00:00:00+00:00">
    
    <section class="page__content" itemprop="text">
      <p>This reformulation of LayerNorm and Self-Attention is used in our paper:</p>
<div>
  <ul>
    <li>
      
      

    
    <span class="underline">Kamoda, G.</span>,
    

    Heinzerling, B.,
    

    Inaba, T.,
    

    Kudo, K.,
    

    Sakaguchi, K.,
    
&amp; 
    
    Inui, K.

(2025).<br />
<strong>Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference</strong>.<br />
In
<i>Findings of the Association for Computational Linguistics: NAACL 2025</i>.

    <br />
    <span class="link-row"><a href="https://aclanthology.org/2025.findings-naacl.355/" target="_blank" class="link-btn btn">
        
        <img src="/assets/acl-logo.svg" alt="ACL Anthology logo" style="height: 1.2em; margin-right: 0.15em; vertical-align: middle;" />
        
        ACL Anthology
    </a>&nbsp;<a href="https://arxiv.org/abs/2501.15754" target="_blank" class="link-btn btn">
        
        <img src="/assets/arxiv-logomark-small.svg" alt="arXiv logo" style="height: 1.2em; margin-right: 0.15em; vertical-align: middle;" />
        
        arXiv
    </a>&nbsp;<a href="https://github.com/gokamoda/lm-detokenization" target="_blank" class="link-btn btn">
        
        <i class="fa-brands fa-github" style="margin-right: 0.15em;"></i>
        
        GitHub
    </a></span>

    </li>
  </ul>
</div>

<h2 id="notation">Notation</h2>

\[\begin{alignat}{4}
    &amp;\bm{X} &amp;:= &amp;
    \begin{bmatrix}
        \bm{x}_1\\
        \vdots\\
        \bm{x}_n
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{n \times d}\\
    &amp;\bm{W}^O &amp;:= &amp;
    \begin{bmatrix}
        \bm{W}^O_1\\
        \vdots\\
        \bm{W}^O_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d \times d}\\
    &amp;\bm{W}^Q &amp;:= &amp;
    \begin{bmatrix}
        \bm{W}^Q_1 &amp; \cdots &amp; \bm{W}^Q_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d \times d}&amp; \label{eq:wq_split}\\
    &amp;\bm{W}^K &amp;:= &amp;
    \begin{bmatrix}
        \bm{W}^K_1 &amp; \cdots &amp; \bm{W}^K_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d \times d}&amp; \label{eq:wk_split}\\
    &amp;\bm{W}^V &amp;:= &amp;
    \begin{bmatrix}
        \bm{W}^V_1 &amp; \cdots &amp; \bm{W}^V_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d \times d}&amp;\label{eq:wv_split}\\
    &amp;\bm{b}^Q &amp;:= &amp;
    \begin{bmatrix}
        \bm{b}^Q_1 &amp; \cdots &amp; \bm{b}^Q_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d}&amp; \label{eq:bq_split}\\
    &amp;\bm{b}^K &amp;:= &amp;
    \begin{bmatrix}
        \bm{b}^K_1 &amp; \cdots &amp; \bm{b}^K_H
    \end{bmatrix}
    &amp;\hspace{1em}\in&amp; \mathbb{R}^{d}&amp; \label{eq:bk_split}\\
    &amp;\bm{b}^V &amp;:= &amp;
    \begin{bmatrix}
        \bm{b}^V_1 &amp; \cdots &amp; \bm{b}^V_H
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d}&amp; \\
    &amp;\bm{I} &amp;:= &amp;
    \begin{bmatrix}
        1 &amp; 0 &amp; \cdots &amp; 0 \\
        0 &amp; 1 &amp; \cdots &amp; 0 \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 1 \\
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d\times d}&amp; \\
    &amp;\bm{1} &amp;:= &amp;
    \begin{bmatrix}
        1 &amp; \cdots &amp; 1
    \end{bmatrix}
    &amp;\hspace{1em}\in &amp;\mathbb{R}^{d}
\end{alignat}\]

<h2 id="original-layernorm">Original LayerNorm</h2>

<p>Layer Normalization can be expressed as follows (org stands for original):</p>

\[\begin{alignat}{3}
    &amp;\text{LN}(\bm{x}) &amp;:=&amp;\ \frac{\bm{x}-\mu(\bm{x})\bm{1}}{\sigma(\bm{x})}\odot\bm{\gamma} + \bm{\beta}&amp;\hspace{1em}\in&amp;\mathbb{R}^d\\
    &amp;\bm{x} &amp;:=&amp;\ 
    \begin{bmatrix}
        x^{(1)} &amp; \cdots &amp; x^{(d)}
    \end{bmatrix}
    &amp;\hspace{1em}\in&amp;\mathbb{R}^d\\
    &amp;\mu(\bm{x}) &amp;:=&amp;\ \frac{1}{d}\sum_kx^{(k)}&amp;\hspace{1em}\in&amp;\mathbb{R}\\
    &amp;\sigma(\bm{x}) &amp;:=&amp;\ \sqrt{\frac{1}{d}\sum_k^d\left(x^{(k)}-\mu(\bm{x})\right)^2+\epsilon}&amp;\hspace{1em}\in&amp;\mathbb{R}
\end{alignat}\]

<p>Now, $\mu(\bm{x})$ can be reformulated as follows:</p>

\[\begin{align}
    \mu(\bm{x})\bm{1}
    &amp;=\frac{1}{d}\left(\sum_kx^{(k)}\right)\bm{1}\\
    &amp;=\frac{1}{d}\left(\bm{x}\bm{1}^\top\right)\bm{1}\\
    &amp;=\bm{x}\left(\frac{1}{d}\bm{1}^\top\bm{1}\right)
\end{align}\]

<p>Thus $\text{LN}_{\text{org}}$ can be reformulated as follows.</p>

\[\begin{align}
    \text{LN}(\bm{x}) 
    &amp;= \frac{\bm{x}-\mu(\bm{x})\bm{1}}{\sigma(\bm{x})}\odot\bm{\gamma} + \bm{\beta}\\
    &amp;= \frac{1}{\sigma(\bm{x})} \left(\bm{x}-\bm{x}\left(\frac{1}{d}\bm{1}^\top\bm{1}\right)\right) \diag{\bm\gamma}+ \bm{\beta}\\
    &amp;= \frac{\bm{x}}{\sigma(\bm{x})}\left(\bm{I}-\frac{1}{d}\bm{1}^\top\bm{1}\right)\diag{\bm\gamma}+ \bm{\beta}
\end{align}\]

<h2 id="original-self-attention">Original Self-Attention</h2>

<p>Let query, key, value transformations of each head $h$ be expressed as follows:</p>

\[\begin{align}
    \bm{q}_h(\bm{x}) &amp;:= \bm{x}\bm{W}_h^Q + \bm{b}_h^Q\\
    \bm{k}_h(\bm{x}) &amp;:= \bm{x}\bm{W}_h^K + \bm{b}_h^K\\
    \bm{v}_h(\bm{x}) &amp;:= \bm{x}\bm{W}_h^V + \bm{b}_h^V\\
\end{align}\]

<p>Let attention weight from token position $i$ to $j$  ($i \ge j$) in head $h$ be expressed as follows:</p>

\[\alpha_{i, j, h} = \underset{\bm{x}_j,\bm{x}_j \in \bm{X}, j \leq i}{\text{softmax}}\frac{\bm{q}_h(\bm{x}_i)\bm{k}_h(\bm{x}_j)^\top}{\sqrt{d'}}\]

<p>where $d’ = d/H$ is the dimension of each head.</p>

<p>The output of Attention layer of an causal model at position $i$ can be expressed as follows:</p>

\[\begin{align}
    \text{ATTN}(i, \bm{X})
        &amp;:=\left[\text{head}_1(i, \bm{X})\hspace{0.5em}\cdots\hspace{0.5em}\text{head}_H(i, \bm{X})\right]
            \bm{W}^O + \bm{b}^O\\
        &amp;=\sum_{h=1}^H \text{head}_h(i, \bm{X})\bm{W}^O_h + \bm{b}^O\\
        &amp;=\sum_{h=1}^H \left(\sum_{j=1}^i \alpha_{i, j, h} \bm{v}_h(\bm{x}_j)\right)\bm{W}^O_h + \bm{b}^O\\
        &amp;=\sum_{h=1}^H \left(\sum_{j=1}^i \alpha_{i, j, h} \left(\bm{x}_j\bm{W}^V_h + \bm{b}^V_h\right)\right)\bm{W}^O_h + \bm{b}^O\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h}\bm{x}_j\bm{W}^V_h\bm{W}^O_h + \sum_{h=1}^H \left(\sum_{j=1}^i \alpha_{i, j, h}\bm{b}^V_h\right)\bm{W}^O_h + \bm{b}^O\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h}\bm{x}_j\bm{W}^V_h\bm{W}^O_h + \sum_{h=1}^H\bm{b}^V_h\bm{W}^O_h + \bm{b}^O\hspace{0.5em} \left(\because \sum_j \alpha_{i, j, h} = 1\right)\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h}\bm{x}_j\bm{W}^V_h\bm{W}^O_h + \bm{b}^V\bm{W}^O + \bm{b}^O\\
        &amp;= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h} \bm{x}_j\bm{W}^{VO}_h + \bm{b}^{VO}
\end{align}\]

<p>where</p>

\[\begin{align}
    \bm{W}^{VO}_h &amp;:= \bm{W}^V_h\bm{W}^O_h &amp;\hspace{1em}\in&amp;\mathbb{R}^{d \times d}\\
    \bm{b}^{VO} &amp;:= \bm{b}^V\bm{W}^O + \bm{b}^O &amp;\hspace{1em}\in&amp;\mathbb{R}^{d}\\
\end{align}\]

<h2 id="reformulating-layernorm-and-self-attention">Reformulating LayerNorm and Self-Attention</h2>

<p>LayerNorm is always followed by a linear transformation in transformers.
Thus, we can fold the weights of LayerNorm into the weights of the following linear transformation.</p>

<p>For example, in the case of LayerNorm followed by query transformation, we can fold the weights as follows:</p>

\[\begin{align}
    \bm{q}_h(\text{LN}(\bm{x}))
        &amp;= \text{LN}(\bm{x})\bm{W}^Q_h + \bm{b}^Q_h\\
        &amp;= \left(\frac{\bm{x}}{\sigma(\bm{x})}\left(\bm{I}-\frac{1}{d}\bm{1}^\top\bm{1}\right)\diag{\bm{\gamma}}+ \bm{\beta}\right)\bm{W}^Q_h + \bm{b}^Q_h\\
        &amp;= \frac{\bm{x}}{\sigma(\bm{x})}\left(\bm{I}-\frac{1}{d}\bm{1}^\top\bm{1}\right)\diag{\bm{\gamma}}\bm{W}^Q_h + \bm{\beta}\bm{W}^Q_h + \bm{b}^Q_h\\
        &amp;= \overset{\text{new}}{\text{LN}}(\bm{x})\overset{\text{new}}{\bm{W}^Q_h} + \overset{\text{new}}{\bm{b}^Q_h}
\end{align}\]

<p>where</p>

\[\begin{align}
    \overset{\text{new}}{\text{LN}}(\bm{x}) &amp;:= \frac{\bm{x}}{\sigma(\bm{x})} &amp;\hspace{1em}\in&amp;\mathbb{R}^d\\
    \overset{\text{new}}{\bm{W}^Q_h} &amp;:= \left(\bm{I}-\frac{1}{d}\bm{1}^\top\bm{1}\right)\diag{\bm{\gamma}}\bm{W}^Q_h &amp;\hspace{1em}\in&amp;\mathbb{R}^{d \times d}\\
    \overset{\text{new}}{\bm{b}^Q_h} &amp;:= \bm{\beta}\bm{W}^Q_h + \bm{b}^{\{Q, K\}}_h &amp;\hspace{1em}\in&amp;\mathbb{R}^{d}\\
\end{align}\]

<p>The same can be done for key and value transformations, and thus LayerNorm followed by self-attention can be reformulated as follows:</p>

\[\begin{align}
    \overset{\text{new}}{\text{LN}}(\bm{x}) &amp;:= \frac{\bm{x}}{\sigma(\bm{x})} &amp;\hspace{1em}\in&amp;\mathbb{R}^d\\
    \overset{\text{new}}{\text{ATTN}}(i, \bm{X})
        &amp;:= \sum_{h=1}^H \sum_{j=1}^i \alpha_{i, j, h} \bm{x}_j\overset{\text{new}}{\bm{W}^{VO}_h} + \overset{\text{new}}{\bm{b}^{VO}} &amp;\hspace{1em}\in&amp;\mathbb{R}^d\\
\end{align}\]

<p>where</p>

\[\begin{align}
    \overset{\text{new}}{\alpha_{i, j, h}} &amp;:= \underset{\bm{x}_j,\bm{x}_j \in \bm{X}, j \leq i}{\text{softmax}}\frac{\overset{\text{new}}{\bm{q}_h}(\bm{x}_i)\overset{\text{new}}{\bm{k}_h}(\bm{x}_j)^\top}{\sqrt{d'}} &amp;\hspace{1em}\in&amp;\mathbb{R}\\
    \overset{\text{new}}{\bm{q}_h}(\bm{x}) &amp;:= \bm{x}\overset{\text{new}}{\bm{W}^Q_h} + \overset{\text{new}}{\bm{b}^Q_h} &amp;\hspace{1em}\in&amp;\mathbb{R}^d\\
    \overset{\text{new}}{\bm{k}_h}(\bm{x}) &amp;:= \bm{x}\overset{\text{new}}{\bm{W}^K_h} + \overset{\text{new}}{\bm{b}^K_h} &amp;\hspace{1em}\in&amp;\mathbb{R}^d\\
    \overset{\text{new}}{\bm{W}^Q_h} &amp;:= \left(\bm{I}-\frac{1}{d}\bm{1}^\top\bm{1}\right)\diag{\bm{\gamma}}\bm{W}^Q_h &amp;\hspace{1em}\in&amp;\mathbb{R}^{d \times d}\\
    \overset{\text{new}}{\bm{W}^K_h} &amp;:= \left(\bm{I}-\frac{1}{d}\bm{1}^\top\bm{1}\right)\diag{\bm{\gamma}}\bm{W}^K_h &amp;\hspace{1em}\in&amp;\mathbb{R}^{d \times d}\\
    \overset{\text{new}}{\bm{b}^Q_h} &amp;:= \bm{\beta}\bm{W}^Q_h + \bm{b}^Q_h &amp;\hspace{1em}\in&amp;\mathbb{R}^{d}\\
    \overset{\text{new}}{\bm{b}^K_h} &amp;:= \bm{\beta}\bm{W}^K_h + \bm{b}^K_h &amp;\hspace{1em}\in&amp;\mathbb{R}^{d}\\
\end{align}\]

<p>and</p>

\[\begin{align}
    \overset{\text{new}}{\bm{W}^{VO}_h} &amp;:= \left(\bm{I}-\frac{1}{d}\bm{1}^\top\bm{1}\right)\diag{\bm{\gamma}}\bm{W}^V_h\bm{W}^O_h &amp;\hspace{1em}\in&amp;\mathbb{R}^{d \times d}\\
    \overset{\text{new}}{\bm{b}^{VO}} &amp;:= \bm{\beta}\bm{W}^V\bm{W}^O + \bm{b}^V\bm{W}^O + \bm{b}^O &amp;\hspace{1em}\in&amp;\mathbb{R}^{d}\\
\end{align}\]


    </section>
  </article>
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        


<div class="page__footer-copyright">&copy; 2020 - 2025 <a href="">Go Kamoda</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






    
      <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js"],
      Macros: {
        bm: ["\\boldsymbol{#1}", 1],
        diag: ["\\mathrm{diag}\\left(#1\\right)", 1],
      },
      equationNumbers: { autoNumber: "all" } 
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });


  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
    alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
    alert("Math Processing Error: "+message[1]);
    });
</script>


<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </body>
</html>
