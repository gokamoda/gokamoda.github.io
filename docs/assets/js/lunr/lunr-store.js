var store = [{
        "title": "RMSNorm and LayerNorm",
        "excerpt":"Preparation   Let $\\mu(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns element-wise mean of a row-vector $\\bm{x} \\in \\mathbb{R}^{d}$:   \\[\\begin{align} \\mu(\\bm{x})  &amp;= \\frac{1}{d}\\sum_{i=1}^d \\bm{x}_i\\\\ &amp;=\\frac{1}{d}\\bm{x}\\cdot \\bm{1}\\\\ &amp;=\\frac{1}{d}\\bm{x}\\bm{1}^\\top \\end{align}\\]  Let $c(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}^{d}$ be centering function, that subtracts the element-wise mean from each element of $\\bm{x}$:   \\[\\begin{aligned} \tc(\\bm{x})&amp;=\\bm{x} - \\mu(\\bm{x})\\bm{1}\\\\ \t&amp;= \\bm{x} - \\frac{1}{d}\\bm{x}\\bm{1}^\\top\\bm{1}\\\\ \t&amp;= \\bm{x} \\left(1 - \\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right) \\end{aligned}\\]  By the way, (I - \\frac{1}{d}\\bm{1}^\\top\\bm{1}) is called the  centering matrix.   Let $\\text{RMS}(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns the element-wise RMS (root mean square):   \\[\\begin{align} \t\\text{RMS}(\\bm{x})&amp;=\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2}\\\\ \t&amp;=\\frac{\\sqrt{\\sum_{i=1}^d x_i^2}}{\\sqrt{d}}\\\\ \t&amp;=\\frac{||\\bm{x}||_2}{\\sqrt{d}} \\end{align}\\]  Let $\\text{MS}(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns the squared RMS (root mean square):   \\[\\begin{align} \t\\text{MS}(\\bm{x})&amp;=\\text{RMS}(\\bm{x})^2\\\\ \t&amp;=\\frac{||\\bm{x}||_2^2}{d}\\\\ \t&amp;=\\frac{1}{d}\\sum_{i=1}^d x_i^2\\\\ \\end{align}\\]  Let $\\text{Var}(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns element-wise variance:   \\[\\begin{align} \\text{Var}(\\bm{x}) &amp;= \\frac{1}{d}\\sum_{i=1}^d (x_i - \\mu(\\bm{x}))^2\\\\ &amp;=\\frac{1}{d}\\sum_{i=1}^d (\\bm{x} - \\mu(\\bm{x})\\bm{1})^2_i\\\\ &amp;=\\frac{1}{d}\\sum_{i=1}^d c(\\bm{x})_i^2\\\\ &amp;=\\text{MS}(c(\\bm{x})) \\end{align}\\]  RMSNorm  PyTorch: RMSNorm   \\[\\text{RMSNorm}(\\bm{x}) = \\frac{\\bm{x}}{\\sqrt{\\text{MS}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma}\\]  Here, $\\odot$ is element-wise multiplication, $\\bm{\\gamma}\\in \\mathbb{R}^d$ is a learnable weight vector, and $\\varepsilon$ is a small constant for numerical stability.   LayerNorm  PyTorch: LayerNorm   In the original form:   \\[\\text{LayerNorm}(\\bm{x}) = \\frac{\\bm{x} - \\mu(\\bm{x})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma} +\\bm{\\beta}\\]  This can be rewritten using the centering function $c(\\bm{x})$ and the MS function $\\text{MS}(\\bm{x})$ as follows:   \\[\\begin{aligned} \\text{LayerNorm}(\\bm{x}) &amp;= \\frac{c(\\bm{x})}{\\sqrt{\\text{MS}(c(\\bm{x}))+\\varepsilon}}\\odot \\bm{\\gamma} + \\bm{\\beta}\\\\ \\end{aligned}\\]  Thus, the following holds: LayerNorm is equal to \"centering\" ‚Üí RMSNorm ‚Üí \"add bias\"   \\[\\text{LayerNorm}(\\bm{x}) = \\text{RMSNorm}(c(\\bm{x})) + \\bm{\\beta}\\]  Also, element-wise multiplication of $\\bm{\\gamma}$ can be expressed as matrix multiplication of $\\text{diag}(\\bm{\\gamma})$. Therefore, LayerNorm can be rewritten as:   \\[\\begin{align} \\text{LayerNorm}(\\bm{x}) &amp;= \\frac{1}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\left(\\bm{x} - \\mu(\\bm{x})\\bm{1}\\right)\\odot \\bm{\\gamma} + \\bm{\\beta}\\\\ &amp;= \\frac{\\bm{x}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\left(I - \\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\text{diag}(\\bm{\\gamma}) + \\bm{\\beta}\\\\ \\end{align}\\]  Thus only non-linear operation in LayerNorm is the division by $\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}$.   \\[\\begin{align} &amp;\\text{LayerNorm}(\\bm{x})\\\\&amp; = \\text{RMSNorm}(c(\\bm{x})) + \\bm{\\beta} \\end{align}\\] ","categories": [],
        "tags": [],
        "url": "/notes/layernorm/",
        "teaser": "/assets/img/layernorm_rmsnorm.png"
      },{
        "title": "LogitLens without bias",
        "excerpt":"   LogitLens[1] applies $\\text{LMHead}$ to the internal representations $(\\bm{h} \\in \\mathbb{R}^{1\\times d})$ of a transformer model.   \\[\\begin{equation} \\text{LMHead}(\\bm{h}) = \\text{LN}_\\text{f}(\\bm{h})\\bm{E}^O \\label{eq:lm_head} \\end{equation}\\]  Here, $\\bm{E}^O \\in \\mathbb{R}^{d\\times |\\mathcal{V}|}$ is the unembedding matrix and $\\text{LN}_\\text{f}$ is the final layer normalization of a transformer model. In this page, we assume LayerNorm (not RMSNorm) is used for $\\text{LN}_\\text{f}$ , which is defined as follows.   \\[\\begin{equation} \\text{LN}_\\text{f}(\\bm{h}) = \\frac{\\bm{x} - \\mu(\\bm{x})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma} +\\bm{\\beta} \\label{eq:lm_f} \\end{equation}\\]  Here, $\\mu(\\bm{h}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ is a function that returns element-wise mean of a row-vector $\\bm{h}$ and $\\bm{\\gamma}, \\bm{\\beta} \\in \\mathbb{R}^{d}$ are learnable parameters. $\\odot$ represents element-wise multiplication.   With LogitLens, one can project the hidden states after each transformer layers to the vocabulary space.             Example of LogitLens.   By combining Equation\\eqref{eq:lm_head} and \\eqref{eq:lm_f}, we get a bias term for the projection to vocabulary space.   \\[\\begin{align} \\text{LogitLens}(\\bm{h}) &amp;= \\left(\\frac{\\bm{h} - \\mu(\\bm{h})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma} + \\bm{\\beta}\\right)\\bm{E}^O\\\\ &amp;= \\left(\\frac{\\bm{h} - \\mu(\\bm{h})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma}\\right)\\bm{E}^O + \\bm{\\beta}\\bm{E}^O \\label{eq:lm_head_bias} \\end{align}\\]  The second term in Equation\\eqref{eq:lm_head_bias}, which is $\\bm{\\beta}\\bm{E}^O \\in \\mathbb{R}^{|\\mathcal{V}|}$ is the bias term, which is added to the result of LogitLens regardless of the input. Adding such bias may not reasonable when analyzing ‚Äúwhat the model‚Äôs intermediate states represent‚Äù as Kobayashi et al. (2023) reports that word frequency in the training corpus is encoded in this bias term of $\\text{LN}_\\text{f}$ in GPT-2 model.   By removing the bias term, we get the following result.             Vanilla LogitLens (GPT-2)            LogitLens w/o Bias (GPT-2)             Vanilla LogitLens (OPT)            LogitLens w/o Bias (OPT)   References                         Nostalgibraist 2020, interpreting GPT: the logit lens.                            Kobayashi et al. 2023, Transformer Language Models Handle Word Frequency in Prediction         Head.       ","categories": [],
        "tags": [],
        "url": "/notes/logitlens-wob/",
        "teaser": "/assets/img/logit_lens_nobias_gpt2.png"
      },{
        "title": "Folding weights in transformers",
        "excerpt":"This reformulation of LayerNorm and Self-Attention is used in our paper:                                   Kamoda, G.     ,                Heinzerling, B.     ,                Inaba, T.     ,                Kudo, K.     ,                Sakaguchi, K.     ,      &amp;                Inui, K.       (2025).  Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference.  In Findings of the Association for Computational Linguistics: NAACL 2025.       [ACL Anthology, arXiv, GitHub]            Notation   \\[\\begin{alignat}{4}     &amp;\\bm{X} &amp;:= &amp;     \\begin{bmatrix}         \\bm{x}_1\\\\         \\vdots\\\\         \\bm{x}_n     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{n \\times d}\\\\     &amp;\\bm{W}^O &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^O_1\\\\         \\vdots\\\\         \\bm{W}^O_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}\\\\     &amp;\\bm{W}^Q &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^Q_1 &amp; \\cdots &amp; \\bm{W}^Q_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp; \\label{eq:wq_split}\\\\     &amp;\\bm{W}^K &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^K_1 &amp; \\cdots &amp; \\bm{W}^K_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp; \\label{eq:wk_split}\\\\     &amp;\\bm{W}^V &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^V_1 &amp; \\cdots &amp; \\bm{W}^V_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp;\\label{eq:wv_split}\\\\     &amp;\\bm{b}^Q &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^Q_1 &amp; \\cdots &amp; \\bm{b}^Q_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d}&amp; \\label{eq:bq_split}\\\\     &amp;\\bm{b}^K &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^K_1 &amp; \\cdots &amp; \\bm{b}^K_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in&amp; \\mathbb{R}^{d}&amp; \\label{eq:bk_split}\\\\     &amp;\\bm{b}^V &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^V_1 &amp; \\cdots &amp; \\bm{b}^V_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d}&amp; \\\\     &amp;\\bm{I} &amp;:= &amp;     \\begin{bmatrix}         1 &amp; 0 &amp; \\cdots &amp; 0 \\\\         0 &amp; 1 &amp; \\cdots &amp; 0 \\\\         \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\         0 &amp; 0 &amp; \\cdots &amp; 1 \\\\     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d\\times d}&amp; \\\\     &amp;\\bm{1} &amp;:= &amp;     \\begin{bmatrix}         1 &amp; \\cdots &amp; 1     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d} \\end{alignat}\\]  Original LayerNorm   Layer Normalization can be expressed as follows (org stands for original):   \\[\\begin{alignat}{3}     &amp;\\text{LN}(\\bm{x}) &amp;:=&amp;\\ \\frac{\\bm{x}-\\mu(x)\\bm{1}}{\\sigma(\\bm{x})}\\odot\\bm{\\gamma} + \\bm{\\beta}&amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     &amp;\\bm{x} &amp;:=&amp;\\      \\begin{bmatrix}         x^{(1)} &amp; \\cdots &amp; x^{(d)}     \\end{bmatrix}     &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     &amp;\\mu(\\bm{x}) &amp;:=&amp;\\ \\frac{1}{d}\\sum_kx^{(k)}&amp;\\hspace{1em}\\in&amp;\\mathbb{R}\\\\     &amp;\\sigma(\\bm{x}) &amp;:=&amp;\\ \\sqrt{\\frac{1}{d}\\sum_k^d\\left(x^{(k)}-\\mu(\\bm{x})\\right)^2+\\epsilon}&amp;\\hspace{1em}\\in&amp;\\mathbb{R} \\end{alignat}\\]  Now, $\\mu(\\bm{x})$ can be reformulated as follows:   \\[\\begin{align}     \\mu(\\bm{x})\\bm{1}     &amp;=\\frac{1}{d}\\left(\\sum_kx^{(k)}\\right)\\bm{1}\\\\     &amp;=\\frac{1}{d}\\left(\\bm{x}\\bm{1}^\\top\\right)\\bm{1}\\\\     &amp;=\\bm{x}\\left(\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right) \\end{align}\\]  Thus $\\text{LN}_{\\text{org}}$ can be reformulated as follows.   \\[\\begin{align}     \\text{LN}(\\bm{x})      &amp;= \\frac{\\bm{x}-\\mu(\\bm{x})\\bm{1}}{\\sigma(\\bm{x})}\\odot\\bm{\\gamma} + \\bm{\\beta}\\\\     &amp;= \\frac{1}{\\sigma(\\bm{x})} \\left(\\bm{x}-\\bm{x}\\left(\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\right) \\diag{\\bm\\gamma}+ \\bm{\\beta}\\\\     &amp;= \\frac{\\bm{x}}{\\sigma(\\bm{x})}\\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm\\gamma}+ \\bm{\\beta} \\end{align}\\]  Original Self-Attention   Let query, key, value transformations of each head $h$ be expressed as follows:   \\[\\begin{align}     \\bm{q}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^Q + \\bm{b}_h^Q\\\\     \\bm{k}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^K + \\bm{b}_h^K\\\\     \\bm{v}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^V + \\bm{b}_h^V\\\\ \\end{align}\\]  Let attention weight from token position $i$ to $j$  ($i \\ge j$) in head $h$ be expressed as follows:   \\[\\alpha_{i, j, h} = \\underset{\\bm{x}_j,\\bm{x}_j \\in \\bm{X}, j \\leq i}{\\text{softmax}}\\frac{\\bm{q}_h(\\bm{x}_i)\\bm{k}_h(\\bm{x}_j)^\\top}{\\sqrt{d'}}\\]  where $d‚Äô = d/H$ is the dimension of each head.   The output of Attention layer of an causal model at position $i$ can be expressed as follows:   \\[\\begin{align}     \\text{ATTN}(i, \\bm{X})         &amp;:=\\left[\\text{head}_1(i, \\bm{X})\\hspace{0.5em}\\cdots\\hspace{0.5em}\\text{head}_H(i, \\bm{X})\\right]             \\bm{W}^O + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\text{head}_h(i, \\bm{X})\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\bm{v}_h(\\bm{x}_j)\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\left(\\bm{x}_j\\bm{W}^V_h + \\bm{b}^V_h\\right)\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h}\\bm{b}^V_h\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H\\bm{b}^V_h\\bm{W}^O_h + \\bm{b}^O\\hspace{0.5em} \\left(\\because \\sum_j \\alpha_{i, j, h} = 1\\right)\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\bm{b}^V\\bm{W}^O + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h} \\bm{x}_j\\bm{W}^{VO}_h + \\bm{b}^{VO} \\end{align}\\]  where   \\[\\begin{align}     \\bm{W}^{VO}_h &amp;:= \\bm{W}^V_h\\bm{W}^O_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\bm{b}^{VO} &amp;:= \\bm{b}^V\\bm{W}^O + \\bm{b}^O &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  Reformulating LayerNorm and Self-Attention   LayerNorm is always followed by a linear transformation in transformers. Thus, we can fold the weights of LayerNorm into the weights of the following linear transformation.   For example, in the case of LayerNorm followed by query transformation, we can fold the weights as follows:   \\[\\begin{align}     \\bm{q}_h(\\text{LN}(\\bm{x}))         &amp;= \\text{LN}(\\bm{x})\\bm{W}^Q_h + \\bm{b}^Q_h\\\\         &amp;= \\left(\\frac{\\bm{x}}{\\sigma(\\bm{x})}\\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}+ \\bm{\\beta}\\right)\\bm{W}^Q_h + \\bm{b}^Q_h\\\\         &amp;= \\frac{\\bm{x}}{\\sigma(\\bm{x})}\\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^Q_h + \\bm{\\beta}\\bm{W}^Q_h + \\bm{b}^Q_h\\\\         &amp;= \\overset{\\text{new}}{\\text{LN}}(\\bm{x})\\overset{\\text{new}}{\\bm{W}^Q_h} + \\overset{\\text{new}}{\\bm{b}^Q_h} \\end{align}\\]  where   \\[\\begin{align}     \\overset{\\text{new}}{\\text{LN}}(\\bm{x}) &amp;:= \\frac{\\bm{x}}{\\sigma(\\bm{x})} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\bm{W}^Q_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^Q_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{b}^Q_h} &amp;:= \\bm{\\beta}\\bm{W}^Q_h + \\bm{b}^{\\{Q, K\\}}_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  The same can be done for key and value transformations, and thus LayerNorm followed by self-attention can be reformulated as follows:   \\[\\begin{align}     \\overset{\\text{new}}{\\text{LN}}(\\bm{x}) &amp;:= \\frac{\\bm{x}}{\\sigma(\\bm{x})} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\text{ATTN}(i, \\bm{X})         &amp;:= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h} \\bm{x}_j\\overset{\\text{new}}{\\bm{W}^{VO}_h} + \\overset{\\text{new}}{\\bm{b}^{VO}} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\ \\end{align}\\]  where   \\[\\begin{align}     \\alpha_{i, j, h} &amp;:= \\underset{\\bm{x}_j,\\bm{x}_j \\in \\bm{X}, j \\leq i}{\\text{softmax}}\\frac{\\overset{\\text{new}}{\\bm{q}_h}(\\bm{x}_i)\\overset{\\text{new}}{\\bm{k}_h}(\\bm{x}_j)^\\top}{\\sqrt{d'}} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}\\\\     \\overset{\\text{new}}{\\bm{q}_h}(\\bm{x}) &amp;:= \\bm{x}\\overset{\\text{new}}{\\bm{W}^Q_h} + \\overset{\\text{new}}{\\bm{b}^Q_h} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\bm{k}_h}(\\bm{x}) &amp;:= \\bm{x}\\overset{\\text{new}}{\\bm{W}^K_h} + \\overset{\\text{new}}{\\bm{b}^K_h} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\bm{W}^Q_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^Q_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{W}^K_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^K_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{b}^Q_h} &amp;:= \\bm{\\beta}\\bm{W}^Q_h + \\bm{b}^Q_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\     \\overset{\\text{new}}{\\bm{b}^K_h} &amp;:= \\bm{\\beta}\\bm{W}^K_h + \\bm{b}^K_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  and   \\[\\begin{align}     \\overset{\\text{new}}{\\bm{W}^{VO}_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^V_h\\bm{W}^O_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{b}^{VO}} &amp;:= \\bm{\\beta}\\bm{W}^V\\bm{W}^O + \\bm{b}^V\\bm{W}^O + \\bm{b}^O &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  ","categories": [],
        "tags": [],
        "url": "/notes/fold-weights/",
        "teaser": null
      },{
        "title": "„É°„É¢: PCA",
        "excerpt":"$n$ ÂÄã„ÅÆ $d$ Ê¨°ÂÖÉ„ÅÆ„Éá„Éº„Çø $\\bm{X}$ „Çí„ÄÅ$d‚Äô$ Ê¨°ÂÖÉ„Å´ÂúßÁ∏Æ„Åô„Çã„Åü„ÇÅ„ÅÆÁ∑öÂΩ¢Â§âÊèõË°åÂàó $\\bm{W}$ „ÇíÊ±Ç„ÇÅ„Çã„ÄÇ   \\[\\bm{X} =\\begin{bmatrix} \\bm{x}_1 \\\\ \\bm{x}_2 \\\\ \\vdots \\\\ \\bm{x}_n \\end{bmatrix} = \\begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,d} \\\\ x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,d} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,d} \\end{bmatrix} \\in \\mathbb{R}^{n \\times d}\\]  \\[\\bm{W} = \\begin{bmatrix} \\bm{w}_1 &amp; \\bm{w}_2 &amp; \\cdots &amp; \\bm{w}_{d'} \\\\ \\end{bmatrix} = \\begin{bmatrix} w_{1,1} &amp; w_{1,2} &amp; \\cdots &amp; w_{1,d'} \\\\ w_{2,1} &amp; w_{2,2} &amp; \\cdots &amp; w_{2,d'} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_{d,1} &amp; w_{d,2} &amp; \\cdots &amp; w_{d,d'} \\end{bmatrix} \\in \\mathbb{R}^{d \\times d'}\\]  Â§âÊèõÂæå„ÅÆ„Éá„Éº„Çø $\\bm{Y}$ „ÅØÊ¨°„ÅÆ„Çà„ÅÜ„Å´Ë°®„Åï„Çå„Çã„ÄÇ \\(\\bm{Y} = \\bm{X} \\bm{W} \\in \\mathbb{R}^{n \\times d'}\\)   PCA„Åß„ÅØ„ÄÅÂúßÁ∏ÆÂæå„ÅÆÂêÑÊ¨°ÂÖÉ„ÅÆÂàÜÊï£„ÇíÊúÄÂ§ßÂåñ„Åô„Çã„Åì„Å®„ÇíËÄÉ„Åà„Çã„ÄÇ  ÂúßÁ∏ÆÂæå„ÅÆ $j\\ (1\\le j \\le d‚Äô)$Ê¨°ÂÖÉÁõÆ „ÅÆÂàÜÊï£„ÅØ„ÄÅÂÆöÁæ©„Çà„Çä‰ª•‰∏ã„ÅßË°®„Åõ„Çã:   \\[\\begin{align} s_j^2  &amp;= \\frac{1}{n}\\sum_{i=1}^{n}(y_{ij}-\\bar{y_j})^2\\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\left(     \\bm{x}_i \\bm{w}_j - \\frac{1}{n}\\sum_{k=1}^{n}\\bm{x}_k \\bm{w}_j \\right)^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\left(     \\left(\\bm{x}_i - \\frac{1}{n}\\sum_{k=1}^{n}\\bm{x}_k\\right) \\bm{w}_j \\right)^2 \\\\ \\end{align}\\]  „Åü„Å†„Åó $\\bar{y_j} = \\frac{1}{n}\\sum_{i=1}^{n}y_{ij}$ „ÅØ $j$ Ê¨°ÂÖÉÁõÆ„ÅÆÂπ≥ÂùáÂÄ§„ÇíË°®„Åô„ÄÇ   „Åì„Åì„Åß„ÄÅ$\\bar{\\bm{x}} = \\frac{1}{n}\\sum_{i=1}^{n}\\bm{x}_i \\in \\mathbb{R}^{d}$ „Å®„Åó„ÄÅ ‰∏≠ÂøÉÂåñ (ÂêÑÊ¨°ÂÖÉ„ÅÆÂπ≥Âùá„Çí„Çº„É≠„Å´) „Åï„Çå„Åü„Éá„Éº„Çø„Çí $\\bm{X}^c$ „Å®„Åô„Çã„ÄÇ $\\bm{X}^c$ „ÅØ‰ª•‰∏ã„ÅßÂÆöÁæ©„Åï„Çå„Çã„ÄÇ   \\[\\begin{align} \\bm{X}^c &amp;= \\begin{bmatrix} \\bm{x}_1 \\\\ \\bm{x}_2 \\\\ \\vdots \\\\ \\bm{x}_n \\end{bmatrix} -  \\begin{bmatrix} \\bar{\\bm{x}} \\\\ \\bar{\\bm{x}} \\\\ \\vdots \\\\ \\bar{\\bm{x}} \\\\ \\end{bmatrix} =\\begin{bmatrix} \\bm{x}_1^c \\\\ \\bm{x}_2^c \\\\ \\vdots \\\\ \\bm{x}_n^c \\end{bmatrix} \\end{align}\\]  \\[\\begin{align} \\bm{x}^c_i &amp;=\\bm{x}_i - \\bar{\\bm{x}}\\\\ &amp;=\\bm{x}_i - \\frac{1}{n}\\sum_{k=1}^{n}\\bm{x}_k \\end{align}\\]  „Åô„Çã„Å®„ÄÅÊ¨°ÂÖÉ $j\\ (1\\le j \\le d‚Äô)$ „ÅÆÂàÜÊï£„ÅØ‰ª•‰∏ã„ÅßË°®„Åõ„Çã:   \\[\\begin{align} s_j^2  &amp;= \\frac{1}{n}\\sum_{i=1}^{n}(\\bm{x}_i^c \\bm{w}_j)^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}(\\bm{x}_i\\bm{w}_j^c)^\\top (\\bm{x}_i^c \\bm{w}_j) \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\bm{w}_j^\\top(\\bm{x}_i^c\\top\\bm{x}_i^c)\\bm{w}_j \\\\ &amp;= \\frac{1}{n}\\bm{w}_j^\\top \\left(\\sum_{i=1}^{n}\\bm{x}_i^c\\top\\bm{x}_i^c\\right) \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\left(\\frac{1}{n}\\bm{X}^{c\\top}\\bm{X}^c\\right) \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\end{align}\\]  „Åì„Åì„Åß„ÄÅ$\\bm{S} \\in \\mathbb{R}^{d \\times d}$ „ÅØ„Éá„Éº„Çø$\\bm{X}$„ÅÆ„ÄÅÂÖ±ÂàÜÊï£Ë°åÂàó„Å®Âëº„Å∞„Çå„Çã„ÄÇ   PCA„Åß„ÅØ„ÄÅ$s_j^2 (1\\le j \\le d‚Äô)$„ÇíÊúÄÂ§ßÂåñ„Åô„Çã $\\bm{w}_j$ „ÇíÊ±Ç„ÇÅ„Çã„ÄÇ „Å™„Åä„ÄÅ„Åì„Åì„Åß$\\bm{w}_j$„ÇíÂ§ß„Åç„Åè„Åô„Çå„Å∞$s_j^2$„ÇÇÂ§ß„Åç„Åè„Å™„Çã„Åü„ÇÅ„ÄÅ$\\bm{w}_j$„ÅØÂçò‰Ωç„Éô„ÇØ„Éà„É´„ÄÅ„Å§„Åæ„Çä \\(\\|\\bm{w}_j\\|^2 = \\bm{w}_j^\\top \\bm{w}_j = 1\\) „ÅÆÊàêÁ¥Ñ„ÇíË®≠„Åë„Çã„ÄÇ   ÁµêÊûú„ÄÅËß£„Åç„Åü„ÅÑÂïèÈ°å„ÅØ   \\[\\underset{\\bm{w}_j}{\\text{argmax}}\\quad \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\quad \\text{subject to}\\quad  \\bm{w}_j^\\top \\bm{w}_j = 1\\]  „Åì„Çå„Çí„É©„Ç∞„É©„É≥„Ç∏„É•„ÅÆÊú™ÂÆö‰πóÊï∞Ê≥ï„ÇíÁî®„ÅÑ„Å¶Ëß£„Åè„Åì„Å®„ÇíËÄÉ„Åà„Çã„ÄÇ„É©„Ç∞„É©„É≥„Ç∏„É•Èñ¢Êï∞„ÅØ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´„Å™„Çã„ÄÇ   \\[\\begin{align} \\mathcal{L}(\\bm{w}_j, \\lambda) &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j - \\lambda (\\bm{w}_j^\\top \\bm{w}_j - 1) \\\\ \\end{align}\\]  „Åì„Çå„Çí$W_j$„ÅßÂæÆÂàÜ„Åô„Çå„Å∞„ÄÅ   \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial \\bm{w}_j} &amp;= 2 \\bm{S} \\bm{w}_j - 2 \\lambda \\bm{w}_j \\end{align}\\]  „Åó„Åü„Åå„Å£„Å¶„ÄÅ‰ª•‰∏ã„ÇíÊ∫Ä„Åü„Åô„Å®„Åç„Å´Ê•µÂÄ§„Çí„Å®„Çã„ÄÇ   \\[\\bm{S} \\bm{w}_j = \\lambda \\bm{w}_j\\]  „Åó„Åü„Åå„Å£„Å¶„ÄÅ$\\bm{w}_j$„ÅØ$\\bm{S}$„ÅÆÂõ∫Êúâ„Éô„ÇØ„Éà„É´„Åß„ÅÇ„Çä„ÄÅ$\\lambda$„ÅØÂØæÂøú„Åô„ÇãÂõ∫ÊúâÂÄ§„Åß„ÅÇ„Çã„ÄÇ  „Åæ„Åü„ÄÅ$s_j^2$„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´Êõ∏„ÅçÊèõ„Åà„Çâ„Çå„Çã„ÄÇ \\(\\begin{align} s_j^2 &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\lambda \\bm{w}_j \\\\ &amp;= \\lambda \\bm{w}_j^\\top \\bm{w}_j \\\\ &amp;= \\lambda \\end{align}\\)   PCA„Åß„ÅØ„ÄÅ$d‚Äô$ ÂÄã„ÅÆÊ¨°ÂÖÉ„ÅÆÂàÜÊï£„ÇíÊúÄÂ§ßÂåñ„Åô„Çã„Åì„Å®„ÇíËÄÉ„Åà„Çã„Åü„ÇÅ„ÄÅ$\\bm{S}$„ÅÆÂõ∫ÊúâÂÄ§„ÅåÂ§ß„Åç„ÅÑÈ†Ü„Å´ÂØæÂøú„Åô„Çã$d‚Äô$ ÂÄã„ÅÆÂõ∫Êúâ„Éô„ÇØ„Éà„É´„Çí„Å®„Çä‰∏¶„Åπ„Åü„ÇÇ„ÅÆ„Çí$\\bm{W}$„Å®„Åô„Çå„Å∞„Çà„ÅÑ„ÄÇ   (Âæ©Áøí) Âõ∫ÊúâÂÄ§ÂàÜËß£„ÅßÊ±Ç„ÇÅ„Çâ„Çå„Çã   Ë°åÂàó $\\bm{A} \\in \\mathbb{R}^{d\\times d}$ „ÅÆÂõ∫ÊúâÂÄ§ÂàÜËß£„ÅØ„ÄÅÁõ¥‰∫§Ë°åÂàó$\\bm{V}$„ÇíÁî®„ÅÑ„Å¶‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´Ë°®„Åï„Çå„Çã„ÄÇ   \\[\\bm{A} = \\bm{V} \\bm{\\Lambda} \\bm{V}^\\top\\]  „Åì„Åì„Åß„ÄÅ   \\[\\bm{\\Lambda} = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_d)\\]  \\[\\bm{V} = \\begin{bmatrix} \\bm{v}_1 &amp; \\bm{v}_2 &amp; \\cdots &amp; \\bm{v}_d \\end{bmatrix}\\]  „Å®„Åô„Çã„ÄÇ $\\lambda_i$ „ÅØ $\\bm{A}$ „ÅÆÂõ∫ÊúâÂÄ§„ÄÅ$\\bm{v}_i$ „ÅØÂØæÂøú„Åô„ÇãÂõ∫Êúâ„Éô„ÇØ„Éà„É´„Åß„ÅÇ„Çã„ÄÇ   „Åæ„Åü„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´Â§âÂΩ¢„Åß„Åç„Çã„ÄÇ   \\[\\begin{align} \\bm{A} &amp;= \\bm{V} \\bm{\\Lambda} \\bm{V}^\\top\\\\ &amp;=  \\begin{bmatrix} \\bm{v}_1 &amp; \\bm{v}_2 &amp; \\cdots &amp; \\bm{v}_d \\end{bmatrix} \\begin{bmatrix} \\lambda_1 \\bm{v}_1 \\\\ \\lambda_2 \\bm{v}_2 \\\\ \\vdots \\\\ \\lambda_d \\bm{v}_d \\end{bmatrix} \\\\ &amp;= \\sum_{i=1}^{d} \\lambda_i \\bm{v}_i \\bm{v}_i^\\top \\end{align}\\]  $\\bm{S}$„ÅÆÂõ∫ÊúâÂÄ§/„Éô„ÇØ„Éà„É´„ÅØ$X$„ÅÆÁâπÁï∞ÂÄ§ÂàÜËß£ (SVD) „ÅßÊ±Ç„ÇÅ„Çâ„Çå„Çã   ÂÜçÊé≤: $\\bm{S} \\in \\mathbb{R}^{d\\times d}$, $\\bm{X}^c \\in \\mathbb{R}^{n\\times d}$.   Â§âÊèõË°åÂàó $\\bm{W} \\in \\mathbb{R}^{d\\times d‚Äô}$ „ÇíÊ±Ç„ÇÅ„Çã„Åü„ÇÅ„Å´„ÄÅ$\\bm{S}$„ÅÆÂõ∫ÊúâÂÄ§„ÇíÊ±Ç„ÇÅ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„ÄÇ „Åì„Åì„Åß„ÄÅ$\\bm{S}$ „Çí $\\bm{X}^c$ „Å´Èñ¢„Åó„Å¶Êõ∏„ÅçÊèõ„Åà„ÄÅ$\\bm{X}^c$ „ÅÆÁâπÁï∞ÂÄ§ÂàÜËß£ (SVD) „ÇíÁî®„ÅÑ„Çã„Å®‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´„Å™„Çã„ÄÇ   \\[\\begin{align} \\bm{S} &amp;= \\frac{1}{n} \\bm{X}^{c\\top} \\bm{X}^c \\\\ &amp;= \\frac{1}{n} \\left(\\bm{U}\\bm{\\Sigma} \\bm{V}^{\\top}\\right)^\\top \\left(\\bm{U}\\bm{\\Sigma} \\bm{V}^{\\top}\\right) \\\\ &amp;= \\frac{1}{n} \\bm{V} \\bm{\\Sigma}^\\top \\bm{U}^\\top \\bm{U} \\bm{\\Sigma} \\bm{V}^{\\top} \\\\ &amp;= \\frac{1}{n} \\bm{V} \\bm{\\Sigma}^{2} \\bm{V}^{\\top}\\quad (\\because \\bm{U} \\text{ is orthogonal})\\\\ &amp;= \\bm{V} \\left(\\frac{\\bm{\\Sigma}^{2}}{n} \\right) \\bm{V}^{\\top} \\\\ \\end{align}\\]  „Å§„Åæ„Çä„ÄÅ‰ª•‰∏ã„ÅåË®Ä„Åà„Çã:     $\\bm{S}$ „ÅÆ Âõ∫ÊúâÂÄ§„ÅØ $\\bm{X}^c$ „ÇíÁâπÁï∞ÂÄ§ÂàÜËß£„Åó„Å¶Âæó„Çâ„Çå„ÇãÂõ∫ÊúâÂÄ§„Åã„Çâ„Å™„ÇãË°åÂàó$\\bm{\\Sigma}$ „ÇíÁî®„ÅÑ„Å¶Ë°®„Åï„Çå„Çã $\\bm{\\Sigma}^{2}/n$ „ÅÆÂêÑÂØæËßíÊàêÂàÜ   $\\bm{S}$ „ÅÆ Âõ∫Êúâ„Éô„ÇØ„Éà„É´„ÅØ $\\bm{X}^c$ „ÇíÁâπÁï∞ÂÄ§ÂàÜËß£„Åó„Å¶Âæó„Çâ„Çå„Çã $\\bm{V}$ „ÅÆÂêÑÂàó„Éô„ÇØ„Éà„É´„Åß„ÅÇ„Çã„ÄÇ   „Åó„Åü„Åå„Å£„Å¶„ÄÅPCA„ÇíË°å„ÅÜÈöõ„ÅØ„ÄÅ$\\bm{X}^{c\\top}\\bm{X}^c$ „ÇíË®àÁÆó„Åô„ÇãÂøÖË¶Å„ÅØ„Å™„Åè„ÄÅ$\\bm{X}^c$ „ÇíÁâπÁï∞ÂÄ§ÂàÜËß£„Åó„Å¶„ÄÅ$\\bm{V}$ „ÅÆ„ÅÜ„Å°„ÄÅÂØæÂøú„Åô„ÇãÂõ∫ÊúâÂÄ§„ÅåÂ§ß„Åç„ÅÑ $d‚Äô$ ÂÄã„ÅÆÂàó„Éô„ÇØ„Éà„É´„Çí„Å®„Çä‰∏¶„Åπ„Åü„ÇÇ„ÅÆ„Çí $\\bm{W}$ „Å®„Åô„Çå„Å∞„Çà„ÅÑ„ÄÇ  ","categories": [],
        "tags": [],
        "url": "/notes/pca/",
        "teaser": "/assets/img/pca.png"
      },{
        "title": "‚úÖ Paper accepted to the COLING 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to COLING 2025.                                    Kamoda, G.     ,                Asai, A.     ,                Brassard, A.     ,      &amp;                Sakaguchi, K.       (2025).  Quantifying the Influence of Evaluation Aspects on Long-Form Response Assessment.  In Proceedings of the 31st International Conference on Computational Linguistics (COLING 2025).       [ACL Anthology, GitHub]            -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/coling2025-accept/",
        "teaser": null
      },{
        "title": "‚úÖ Paper accepted to the ICLR 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to ICLR 2025.                                    Deguchi, H.     ,                Kamoda, G.     ,                Matsushita, Y.     ,                Taguchi, C.     ,                Waga, M.     ,                Suenaga, K.     ,      &amp;                Yokoi, S.       (2025).  SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches.  In The Thirteenth International Conference on Learning Representations (ICLR 2025).       [OpenReview, arXiv, Project Page]            -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/iclr2025-accept/",
        "teaser": null
      },{
        "title": "üé§ Presentations at NLP 2025",
        "excerpt":"Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö (NLP 2025) „Å´„Å¶„ÄÅ‰ª•‰∏ã„ÅÆ6‰ª∂„ÅÆÁô∫Ë°®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ                                       Âá∫Âè£Á••‰πã     ,                È¥®Áî∞Ë±™     ,                Êùæ‰∏ãÁ•ê‰ªã     ,                Áî∞Âè£Êô∫Â§ß     ,                Êú´Ê∞∏Âπ∏Âπ≥     ,                ÂíåË≥ÄÊ≠£Ê®π     ,                Ê®™‰∫ïÁ••       (2024).  SoftMatcha: Â§ßË¶èÊ®°„Ç≥„Éº„Éë„ÇπÊ§úÁ¥¢„ÅÆ„Åü„ÇÅ„ÅÆÊüî„Çâ„Åã„Åè„ÇÇÈ´òÈÄü„Å™„Éë„Çø„Éº„É≥„Éû„ÉÉ„ÉÅ„É£„Éº.  Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö, pp. 3310-3315.        [‰∫àÁ®ø]                                  È¥®Áî∞Ë±™     ,                Benjamin Heinzerling     ,                Á®≤ËëâÈÅîÈÉé     ,                Â∑•Ëó§ÊÖßÈü≥     ,                ÂùÇÂè£ÊÖ∂Á•ê     ,                ‰πæÂÅ•Â§™ÈÉé       (2025).  Ë®ÄË™û„É¢„Éá„É´„ÅÆ„Éë„É©„É°„Éº„Çø„Åã„ÇâÊé¢„ÇãDetokenization„É°„Ç´„Éã„Ç∫„É†.  Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö, pp. 634-639.        [‰∫àÁ®ø]                                  Â∞èÊûóÊò•Êñó     ,                ÂéüÁü•Ê≠£     ,                È¥®Áî∞Ë±™     ,                Ê®™‰∫ïÁ••       (2025).  Â±§„ÅÆÂÜóÈï∑ÊÄß„Å®Â±§ÂêåÂ£´„ÅÆÁã¨Á´ãÊÄß„Å´Âü∫„Å•„ÅèË®ÄË™û„É¢„Éá„É´„ÅÆÂ±§‰∫§Êèõ„ÅÆÊàêÂê¶„ÅÆÁâπÂæ¥„Å•„Åë.  Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö, pp. 1751-1756.        [‰∫àÁ®ø]                                  Â∑•Ëó§ÊÖßÈü≥     ,                È¥®Áî∞Ë±™     ,                Â°©ÈáéÂ§ßËºù     ,                Èà¥Êú®ÊΩ§       (2025).  Êó•Êú¨Ë™û„Éê„Ç§„ÉàÁ¨¶Âè∑Âåñ„Éû„Çπ„ÇØË®ÄË™û„É¢„Éá„É´„ÅÆÈñãÁô∫„Å®ÂàÜÊûê.  Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö, pp. 3356-3361.        [‰∫àÁ®ø, ü§óByBERT-JP, ü§óByGPT-JP]                                  ‰Ωê„ÄÖÊú®Áù¶Âè≤     ,                È´òÊ©ãËâØÂÖÅ     ,                È¥®Áî∞Ë±™     ,                Benjamin Heinzerling     ,                ÂùÇÂè£ÊÖ∂Á•ê     ,                ‰πæÂÅ•Â§™ÈÉé       (2025).  LM „ÅØÊó•Êú¨„ÅÆÊôÇÁ≥ªÂàóÊßãÈÄ†„Çí„Å©„ÅÜ„Ç®„É≥„Ç≥„Éº„Éâ„Åô„Çã„Åã.  Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö, pp. 2642-2647.        [‰∫àÁ®ø]                                  ‰ΩêËó§ÂÆè‰∫Æ     ,                È¥®Áî∞Ë±™     ,                Benjamin Heinzerling     ,                ÂùÇÂè£ÊÖ∂Á•ê       (2025).  Ë®ÄË™û„É¢„Éá„É´„ÅÆÂÜÖÈÉ®Ë°®Áèæ„Å´„Åä„Åë„ÇãÊñáÊ≥ïÊÉÖÂ†±„ÅÆÂ±ÄÊâÄÊÄß„Å´„Å§„ÅÑ„Å¶.  Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö, pp. 697-701.        [‰∫àÁ®ø]                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/nlp-presentation/",
        "teaser": null
      },{
        "title": "üëë 2 papers received awards at NLP 2025",
        "excerpt":"Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö (NLP 2025) „Å´„Å¶„ÄÅ‰ª•‰∏ã„ÅÆ2‰ª∂„ÅÆË´ñÊñá„Åå„Åù„Çå„Åû„Çå„ÄåËã•ÊâãÂ•®Âä±Ë≥û„Äç„ÄåÊó•Êú¨ÁµåÊ∏àÊñ∞ËÅûÁ§æ CDIOÂÆ§Ë≥û„Äç„ÇíÂèóË≥û„Åó„Åæ„Åó„Åü„ÄÇ                                    Â∞èÊûóÊò•Êñó     ,                ÂéüÁü•Ê≠£     ,                È¥®Áî∞Ë±™     ,                Ê®™‰∫ïÁ••       (2025).  Â±§„ÅÆÂÜóÈï∑ÊÄß„Å®Â±§ÂêåÂ£´„ÅÆÁã¨Á´ãÊÄß„Å´Âü∫„Å•„ÅèË®ÄË™û„É¢„Éá„É´„ÅÆÂ±§‰∫§Êèõ„ÅÆÊàêÂê¶„ÅÆÁâπÂæ¥„Å•„Åë.  Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö, pp. 1751-1756.                 Ëã•ÊâãÂ•®Âä±Ë≥û (20/487)        [‰∫àÁ®ø]                                    ‰Ωê„ÄÖÊú®Áù¶Âè≤     ,                È´òÊ©ãËâØÂÖÅ     ,                È¥®Áî∞Ë±™     ,                Benjamin Heinzerling     ,                ÂùÇÂè£ÊÖ∂Á•ê     ,                ‰πæÂÅ•Â§™ÈÉé       (2025).  LM „ÅØÊó•Êú¨„ÅÆÊôÇÁ≥ªÂàóÊßãÈÄ†„Çí„Å©„ÅÜ„Ç®„É≥„Ç≥„Éº„Éâ„Åô„Çã„Åã.  Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºö Á¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö, pp. 2642-2647.                 Êó•Êú¨ÁµåÊ∏àÊñ∞ËÅûÁ§æ CDIOÂÆ§Ë≥û        [‰∫àÁ®ø]            -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/nlp-awards/",
        "teaser": null
      },{
        "title": "üëë Completed Master's course and received the Dean's Award",
        "excerpt":"Êù±ÂåóÂ§ßÂ≠¶ ÊÉÖÂ†±ÁßëÂ≠¶Á†îÁ©∂Áßë Â≠¶‰ΩçÊéà‰∏éÂºè„Å´„Å¶„ÄÅÂ≠¶‰ΩçË®ò (‰øÆÂ£´: ÊÉÖÂ†±ÁßëÂ≠¶) „Å®„Å®„ÇÇ„Å´„ÄåÁ†îÁ©∂ÁßëÈï∑Ë≥û„Äç„Çí„ÅÑ„Åü„Å†„Åç„Åæ„Åó„Åü„ÄÇ  ","categories": ["News"],
        "tags": [],
        "url": "/news/dean-award/",
        "teaser": null
      },{
        "title": "üå∏ Enrollment in SOKENDAI for Doctoral Program",
        "excerpt":"Á∑èÂêàÁ†îÁ©∂Â§ßÂ≠¶Èô¢Â§ßÂ≠¶ (SOKENDAI) Êó•Êú¨Ë™ûË®ÄË™ûÁßëÂ≠¶„Ç≥„Éº„Çπ„ÅÆÂçöÂ£´ÂæåÊúüË™≤Á®ã„Å´ÈÄ≤Â≠¶„Åó„Åæ„Åó„Åü„ÄÇ  JST BOOST „ÅÆÊîØÊè¥„ÇíÂèó„Åë„ÇãSOKENDAIÁâπÂà•Á†îÁ©∂Âì°„Å®„Åó„Å¶„ÄÅÂõΩÁ´ãÂõΩË™ûÁ†îÁ©∂ÊâÄ (NINJAL) „ÅßÁ†îÁ©∂Ê¥ªÂãï„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ  ","categories": ["News"],
        "tags": [],
        "url": "/news/sokendai-enrollment/",
        "teaser": null
      },{
        "title": "‚úÖ Paper accepted to the Findings of EMNLP 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to the Findings of EMNLP 2025.                                    Inaba, T.     ,                Kamoda, G.     ,                Inui, K.     ,                Isonuma, M.     ,                Miyao, Y.     ,                Oseki, Y.     ,                Takagi, Y.     ,      &amp;                Heinzerling, B.       (2025).  How a Bilingual LM Becomes Bilingual: Tracing Internal Representations with Sparse Autoencoders.  In Findings of the Association for Computational Linguistics: EMNLP 2025.       [OpenReview]            -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/emnlp2025findings-accept/",
        "teaser": null
      },{
        "title": "‚úÖ Paper accepted to the 8th BlackboxNLP Workshop",
        "excerpt":"We are pleased to announce that our paper has been accepted to the 8th BlackboxNLP Workshop.                                    Takahashi, R.     ,                Kamoda, G.     ,                Heinzerling, B.     ,                Sakaguchi, K.     ,      &amp;                Inui, K.       (2025).  Understanding the Side Effects of Rank-One Knowledge Editing.  In The 8th BlackboxNLP Workshop.       [arXiv]            -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/bbnlp2025-accept/",
        "teaser": null
      },{
        "title": "üé§ Presentation at YANS 2025",
        "excerpt":"YANS 2025 „Å´„Å¶‰ª•‰∏ã„ÅÆÁô∫Ë°®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ                                    È¥®Áî∞Ë±™     ,                ÁÜäË∞∑ÈõÑ‰ªã     ,                Êùæ‰∫ïÂ≠ùÂ§™     ,                Ê®™‰∫ïÁ••       (2024).  ÂØÜÂ∫¶ÊØî„ÅÆÁõ¥Êé•Êé®ÂÆö„Å´Âü∫„Å•„ÅèË®ÄË™û„É¢„Éá„É´„ÅÆÂá∫ÂäõËºÉÊ≠£.  Á¨¨20ÂõûË®ÄË™ûÂá¶ÁêÜËã•Êâã„Ç∑„É≥„Éù„Ç∏„Ç¶„É† (YANS).                   -&gt; Publications  ","categories": ["Blog"],
        "tags": [],
        "url": "/blog/yans2025-presentation/",
        "teaser": null
      },{
        "title": "Test-time Augmentation for Factual Probing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202303-nlp-kamoda",
        "teaser": null
      },{
        "title": "Èï∑ÊñáÁîüÊàê„ÅÆÂ§öÈù¢ÁöÑË©ï‰æ°:‰∫∫ÊâãË©ï‰æ°„Å®Ëá™ÂãïË©ï‰æ°„ÅÆÂêë‰∏ä„ÇíÁõÆÊåá„Åó„Å¶",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202403-nlp-kamoda",
        "teaser": null
      },{
        "title": "Ë®ÄË™û„É¢„Éá„É´„Åã„Çâ„ÅÆÁü•Ë≠òÂâäÈô§ÔºöÈ†ªÂá∫ÂÆü‰Ωì„ÅÆÁü•Ë≠ò„ÅØÂâØ‰ΩúÁî®„ÅåÁ†¥ÊªÖÁöÑ",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202403-nlp-takahashi",
        "teaser": null
      },{
        "title": "Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÅÆÊÉÖÂ†±Êé®Ëñ¶„Éê„Ç§„Ç¢„Çπ„ÅÆËºÉÊ≠£",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202405-jsai-kumagae",
        "teaser": null
      },{
        "title": "Êüî„Çâ„Åã„ÅÑgrep/KWIC„Å´Âêë„Åë„Å¶ÔºöÈ´òÈÄüÂçòË™ûÂàó„Éû„ÉÉ„ÉÅ„É≥„Ç∞„ÅÆÂüã„ÇÅËæº„ÅøË°®Áèæ„Å´„Çà„ÇãÈÄ£Á∂öÂåñ",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202409-yans-deguchi",
        "teaser": null
      },{
        "title": "‰∫ãÂâçÂ≠¶Áøí‚ÄìÊñáËÑàÂÜÖÂ≠¶Áøí„Éë„É©„ÉÄ„Ç§„É†„ÅßÁîü„Åò„ÇãÈ†ªÂ∫¶„Éê„Ç§„Ç¢„Çπ„ÅÆËºÉÊ≠£",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202409-yans-ito",
        "teaser": null
      },{
        "title": "Â±§ÂêåÂ£´„ÅÆÊé•Á∂öÂèØËÉΩÊÄß„Å®ÂêÑÂ±§„ÅåÂΩ±Èüø„Çí‰∏é„Åà„ÇãÈÉ®ÂàÜÁ©∫Èñì„ÅÆÈáç„Å™„ÇäÂ∫¶Âêà„ÅÑ„ÅÆÈñ¢‰øÇÊÄß",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202409-yans-kobayashi",
        "teaser": null
      },{
        "title": "SoftMatcha: Â§ßË¶èÊ®°„Ç≥„Éº„Éë„ÇπÊ§úÁ¥¢„ÅÆ„Åü„ÇÅ„ÅÆÊüî„Çâ„Åã„Åè„ÇÇÈ´òÈÄü„Å™„Éë„Çø„Éº„É≥„Éû„ÉÉ„ÉÅ„É£„Éº",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-deguchi",
        "teaser": null
      },{
        "title": "Ë®ÄË™û„É¢„Éá„É´„ÅÆ„Éë„É©„É°„Éº„Çø„Åã„ÇâÊé¢„ÇãDetokenization„É°„Ç´„Éã„Ç∫„É†",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-kamoda",
        "teaser": null
      },{
        "title": "Â±§„ÅÆÂÜóÈï∑ÊÄß„Å®Â±§ÂêåÂ£´„ÅÆÁã¨Á´ãÊÄß„Å´Âü∫„Å•„ÅèË®ÄË™û„É¢„Éá„É´„ÅÆÂ±§‰∫§Êèõ„ÅÆÊàêÂê¶„ÅÆÁâπÂæ¥„Å•„Åë",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-kobayashi",
        "teaser": null
      },{
        "title": "Êó•Êú¨Ë™û„Éê„Ç§„ÉàÁ¨¶Âè∑Âåñ„Éû„Çπ„ÇØË®ÄË™û„É¢„Éá„É´„ÅÆÈñãÁô∫„Å®ÂàÜÊûê",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-kudo",
        "teaser": null
      },{
        "title": "LM „ÅØÊó•Êú¨„ÅÆÊôÇÁ≥ªÂàóÊßãÈÄ†„Çí„Å©„ÅÜ„Ç®„É≥„Ç≥„Éº„Éâ„Åô„Çã„Åã",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-sasaki",
        "teaser": null
      },{
        "title": "Ë®ÄË™û„É¢„Éá„É´„ÅÆÂÜÖÈÉ®Ë°®Áèæ„Å´„Åä„Åë„ÇãÊñáÊ≥ïÊÉÖÂ†±„ÅÆÂ±ÄÊâÄÊÄß„Å´„Å§„ÅÑ„Å¶",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-satoh",
        "teaser": null
      },{
        "title": "ÂØÜÂ∫¶ÊØî„ÅÆÁõ¥Êé•Êé®ÂÆö„Å´Âü∫„Å•„ÅèË®ÄË™û„É¢„Éá„É´„ÅÆÂá∫ÂäõËºÉÊ≠£",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202509-yans-kamoda",
        "teaser": null
      },{
        "title": "Test-time Augmentation for Factual Probing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202311-emnlp-kamoda",
        "teaser": null
      },{
        "title": "Quantifying the Influence of Evaluation Aspects on Long-Form Response Assessment",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202501-coling-kamoda",
        "teaser": null
      },{
        "title": "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-iclr-deguchi",
        "teaser": null
      },{
        "title": "Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-naacl-kamoda",
        "teaser": null
      },{
        "title": "Understanding the Side Effects of Rank-One Knowledge Editing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202511-blackboxnlp-takahashi",
        "teaser": null
      },{
        "title": "How a Bilingual LM Becomes Bilingual: Tracing Internal Representations with Sparse Autoencoders",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202511-emnlp-inaba",
        "teaser": null
      },{
        "title": "Can Language Models Handle a Non-Gregorian Calendar?",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202311-arxiv-sasaki",
        "teaser": null
      },{
    "title": "Notes",
    "excerpt":"","url": "/notes/"
  },{
    "title": "Publications & Activities",
    "excerpt":"## Education - Doctoral Student, April 2025 - Graduate Institute for Advanced Studies, SOKENDAI.Supervisor: Assoc. Prof. Sho Yokoi - Master of Information Science, April 2023 - March 2025.Graduate School of Information Sciences, Tohoku University.Supervisor: Prof. Jun Suzuki & Assoc. Prof. Keisuke Sakaguchi Dean Award (4/126) - Bachelor of Engineering, April 2020 - March 2023.School of Engineering, Tohoku University.Supervisor: Prof. Kentaro Inui & Assoc. Prof. Keisuke SakaguchiEarly Graduation (1/252)  ## International Conferences          {% for publication in site.pubInternationalConferences reversed %}              {% include pub-apa-international-conf.html  %}            {% endfor %}           ## Domestic Conferences          {% for publication in site.pubDomesticConferences reversed %}              {% include pub-anlp-domestic-conf.html  %}            {% endfor %}        {% if site.pubPreprint.size > 0 %} ## Preprints         {% for publication in site.pubPreprint reversed %}              {% include pub-apa-international-conf.html  %}            {% endfor %}      {% endif %}   ## Experiences - [2025.04 -] SOKENDAISpecial Researcher Program (Supported by JST BOOST) - [2025.04 -] NINJALPart-time Researcher - [2023.10 -] Joint Research with Hakuhodo DY holdings Inc. - [2024.04 - 2025.03] Tohoku UniversityGP-DS Research Assistant (Competitive research fellowship) - [2023.09] NS Solutions R&D Internship - [2023.10 - 2024.02] [AKATSUKI-SICA](https://mitouteki.jp/r4/supporters/outline/r4_b07/)([Certificate](https://www.openbadge-global.com/ns/portal/openbadge/public/assertions/detail/U3NWU05wcHViK2VHc3RSYTJZeFVhZz09))Social Impact Creators' Accelerator Program (supported by Ministry of Economy, Trade and Industry of Japan) - [2023.05 - 2024.01] [AIÁéã](https://sites.google.com/view/project-aio/competition4?pli=1)([YouTube](https://youtu.be/5pT5t6e_bLo), [News: Êù±Ê¥ãÁµåÊ∏à](https://toyokeizai.net/articles/-/732641?page=5), [News: Tech+](https://news.mynavi.jp/techplus/article/20240206-2877452/?&utm_medium=email&utm_campaign=20240213))Committee Member - [2021.03 - 2023.08] Infratop (DMM WebCamp)Programming Mentor, School Managemenent Member  ## Invited Talks         {% for talk in site.oubInvitedTalks reversed %}                       {% for speaker in talk.speakers %}           {{ speaker.name }}           {%- if forloop.last == false -%}             ,           {% endif %}         {%- endfor -%}         .         {{ talk.title }}.         {{ talk.event_name }},         {{ talk.month }}         {{ talk.year }}.         {% if talk.links %}           [           {%- for link in talk.links -%}             {{ link.name}}{% if forloop.last == false %}, {% endif %}           {%- endfor -%}           ]         {% endif %}            {% endfor %}         ","url": "/cv/"
  },{
    "title": "",
    "excerpt":"","url": "/index.html"
  },{
    "title": " - page 2",
    "excerpt":"","url": "/page/2/index.html"
  }]
