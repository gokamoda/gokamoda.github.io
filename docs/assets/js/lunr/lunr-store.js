var store = [{
        "title": "Infratop (DMM WebCamp)",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202103-infratop",
        "teaser": null
      },{
        "title": "AIç‹ Committee Member",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202305-aio",
        "teaser": null
      },{
        "title": "NS Solutions R&D Internship",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202309-nssol",
        "teaser": null
      },{
        "title": "AKATSUKI-SICA",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202310-akatsuki-sica",
        "teaser": null
      },{
        "title": "Hakuhodo DY Holdings Inc. Joint Research",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202310-hakuhodo",
        "teaser": null
      },{
        "title": "Visiting Student at NLP Department, MBZUAI",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202402-mbzuai",
        "teaser": null
      },{
        "title": "Tohoku University GP-DS Research Assistant",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202404-tu-gpds",
        "teaser": null
      },{
        "title": "NINJAL Part-time Researcher",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-ninjal-part-time-researcher",
        "teaser": null
      },{
        "title": "SOKENDAI Special Researcher Program",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-sokenda-srp",
        "teaser": null
      },{
        "title": "HHKBé…åˆ—åˆ†å‰²ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã®ãƒ¡ãƒ¢",
        "excerpt":"â€»7sProã‚’2023å¹´9æœˆã«è³¼å…¥ã—ã€ã‚±ãƒ¼ãƒ–ãƒ«ãªã©ãªã©æƒã„ã€ã‚­ãƒ¼ã‚¹ã‚¤ãƒƒãƒæƒ…å ±ç­‰æ¶ˆãˆãã†ãªã®ã§2025/12/01ã«å‚™å¿˜éŒ²çš„ã«ã¾ã¨ã‚ãŸ   èƒŒæ™¯     HHKBã‚’1å°æŒã£ã¦ã„ã‚‹ãŒã€ãƒ©ãƒœã¨å®¶ã®ä¸¡æ–¹ã§ä½¿ã„ãŸã„   åˆ†å‰²ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã€æ°—ã«ãªã‚‹            è…•ã‚’åºƒã’ãŸã¾ã¾ä½¿ãˆã¦è² æ‹…ãŒæ¸›ã‚Šãã†       çœŸã‚“ä¸­ã«iPadã¨ã‹ç½®ã‘ã¦ä¾¿åˆ©ãã†                    iPadã§è«–æ–‡è¡¨ç¤ºã—ã¦ã€PCã§ãƒ¡ãƒ¢å–ã‚‹ã¨ã‹                           HHKBã‚’2å°è²·ã†ã»ã©ã€HHKBãŒæœ€é©è§£ã ã¨æ€ãˆã¦ã„ãªã„            æœªé–‹æ‹“ã®ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã¯å¤šæ•°ã‚ã‚‹ã®ã§           ã¨ã¯ã„ãˆã€HHKBã®ã„ã„ã¨ã“ã‚ã¯ãªã‚‹ã¹ãç¶­æŒã—ãŸã„            æ‰“éµæ„Ÿ (ä½•ã«ã‚ˆã‚‹ã‚‚ã®ã‹ã¯æœ¬å½“ã«ã¯ã‚ã‹ã£ã¦ã„ãªã„)       é…åˆ—ãƒ»ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã•                    å€‹äººçš„ã« USé…åˆ— ã‹ã¤ Enter ä¸Šã« Backspace ãŒã‚ã‚‹ã®ãŒå¥½ã           ANSIé…åˆ—ã§Backspaceã®ã¨ã“ã‚ã«ã‚­ãƒ¼ãŒ2ã¤ã‚ã‚‹ã®ã‚‚å¥½ã                           çµæœ + é¸æŠç†ç”±     è‡ªä½œã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‚­ãƒƒãƒˆ 7sPro            ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ã‚„ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã®æ–¹ã«æ„›ç”¨è€…ã®å¤šã„ãƒãƒƒãƒ”ãƒ¼ãªé…åˆ—ã‚’è¸è¥²ã—ã¦ã„ã‚‹       ä¼¼ãŸã‚‚ã®ã§Choco60ã‚‚ã‚ã‚‹ãŒã€é•ã„ã¯ã‚ã¾ã‚Šã‚ã‹ã£ã¦ãªã„ã€‚ä¸€ã¤ç¢ºã‹ãªã®ã¯ã€7sProã¯ã‚¹ãƒšãƒ¼ã‚¹éƒ¨åˆ†ã®åˆ†å‰²ãŒå¤šã„ã€‚       è‡ªä½œã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãªã‚‰å¤§ä½“ãã†ã ã‚ã†ãŒã€Cherry MXäº’æ›ã‚¹ã‚¤ãƒƒãƒãŒä½¿ãˆã‚‹ (é¸æŠè‚¢ãŒå¤šã„)       ã¯ã‚“ã ä»˜ã‘ã¯è¨˜æ†¶ãŒæ­£ã—ã‘ã‚Œã°10ç®‡æ‰€ãã‚‰ã„           ã‚­ãƒ¼ã‚¹ã‚¤ãƒƒãƒ Kailh Box Mute Brown Switch            â€» ãŠãã‚‰ãå»ƒç›¤ã§æ­£å¼åç§°ãŒã‚ã‚„ãµã‚„ (2023/9/16ã¯72å€‹å…¥ã‚Šã§7,356å††ã ã£ãŸ)       HHKBã«è¿‘ã„                    HHKB Type-S: é™é›»å®¹é‡ç„¡æ¥ç‚¹æ–¹å¼ã€ã‚­ãƒ¼ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯3.8mmã€æŠ¼ä¸‹åœ§45g           Kailh Box Mute Brown Switch: ãƒ¡ã‚«ãƒ‹ã‚«ãƒ«æ–¹å¼ (ã‚¿ã‚¯ã‚¿ã‚¤ãƒ«)ã€ã‚­ãƒ¼ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯3.6Â±0.4mmã€æŠ¼ä¸‹åœ§45Â±10g                       Mute â†ã„ã„ã­       è³¼å…¥å®Ÿç¸¾                    Amazon                           ã‚­ãƒ¼ã‚­ãƒ£ãƒƒãƒ— é¦¬ã®éª¨ã®ç„¡åˆ»å°ç™½PBTã‚­ãƒ¼ã‚­ãƒ£ãƒƒãƒ— (130+ã‚­ãƒ¼) ã‚»ãƒƒãƒˆ            ä¸€èˆ¬çš„ãªã‚­ãƒ¼ã‚­ãƒ£ãƒƒãƒ—ã ã¨åˆ»å°ã¨å‰²å½“ãŒåˆã‚ãªããªã‚‹ã“ã¨ã¯ç›®ã«è¦‹ãˆã¦ã„ã‚‹ã®ã§ã€ã„ã£ãç„¡åˆ»å°ã«       ç´ æã«ã“ã ã‚ã‚Šã¯ãªã„ (å…‰ã‚‹ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã«èˆˆå‘³ã¯ãªã„ã—ã€å®‰ãã¦è‰¯ã„) ãŒã€å¼·ã„ã¦ã„ãˆã°ãƒãƒƒãƒˆãªè³ªæ„ŸãŒè‰¯ã„       ã©ã®åˆ—ã«ã‚‚å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«ãƒ•ãƒ©ãƒƒãƒˆ (c.f., ã‚¹ãƒ†ãƒƒãƒ—ãƒ»ã‚¹ã‚«ãƒ«ãƒ—ãƒãƒ£ãƒ¼) ãªã‚‚ã®ã‚’é¸æŠ       è‰²ã¯ç™½ã‹é»’ã§æ¢ã—ãŸãŒã€ãƒ’ãƒƒãƒˆã—ãŸã®ãŒç™½ã ã£ãŸ       7sProã¯ä¸€èˆ¬çš„ã«ã¯ä½¿ã‚ãªã„ã‚µã‚¤ã‚ºã®ã‚­ãƒ¼ã‚­ãƒ£ãƒƒãƒ—ã‚’è¤‡æ•°ä½¿ã†ã®ã§ã€çµæœçš„ã«130ã‚­ãƒ¼ä»¥ä¸Šã®ã‚»ãƒƒãƒˆã‚’è³¼å…¥ã™ã‚‹å¿…è¦ãŒã‚ã£ãŸ           ç„¡ç·šåŒ–            æœ€åˆã¯å®Œå…¨ç„¡ç·šåŒ–ã—ã¦ã„ãŸãŒã€å®‰å®šæ€§ã‚’æ¬ ã„ãŸã®ã¨ã€ã„ã˜ã£ã¦ã„ã‚‹é–“ã«å£Šã‚ŒãŸâ€¦?       ç¾åœ¨ã¯å®Œå…¨æœ‰ç·šã§ä½¿ç”¨ä¸­           ã‚¢ã‚¯ã‚»ã‚µãƒª            ãƒ‘ãƒ¼ãƒ ãƒ¬ã‚¹ãƒˆ       TRRSã‚±ãƒ¼ãƒ–ãƒ«                    TTRSã§ã‚ã‚‹å¿…è¦ã¯å®Ÿã¯ãªã‹ã£ãŸæ°—ãŒã™ã‚‹ãŒã€è„³æ­»ã§é¸æŠ           ä¸¡ç«¯å­ã¨ã‚‚Lå­—ã«ãªã£ã¦ã„ã‚‹ã®ãŒè‰¯ã„                       USBã‚±ãƒ¼ãƒ–ãƒ«                    ä½•ã§ã‚‚è‰¯ã„ (Keychronã®ä»˜å±ã‚±ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨ä¸­)                       USB-C Lå­—å¤‰æ›ã‚¢ãƒ€ãƒ—ã‚¿                    æ¥ç¶šéƒ¨ã®ä¿è­·ã®ãŸã‚ã«ä½¿ç”¨           ã“ã‚“ãªæ€§èƒ½ã¯ã„ã‚‰ãªã„ãŒã€ä»–ã®ã‚±ãƒ¼ãƒ–ãƒ«ã¨ã®äº’æ›ç­‰ã‚’ç†ç”± (å£å®Ÿ?) ã« (è„³æ­»ã§) é¸æŠ                           ","categories": [],
        "tags": [],
        "url": "/notes/split_hhkb/",
        "teaser": "/assets/img/7spro.jpg"
      },{
        "title": "RMSNorm and LayerNorm",
        "excerpt":"Preparation   Let $\\mu(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns element-wise mean of a row-vector $\\bm{x} \\in \\mathbb{R}^{d}$:   \\[\\begin{align} \\mu(\\bm{x})  &amp;= \\frac{1}{d}\\sum_{i=1}^d \\bm{x}_i\\\\ &amp;=\\frac{1}{d}\\bm{x}\\cdot \\bm{1}\\\\ &amp;=\\frac{1}{d}\\bm{x}\\bm{1}^\\top \\end{align}\\]  Let $c(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}^{d}$ be centering function, that subtracts the element-wise mean from each element of $\\bm{x}$:   \\[\\begin{aligned} \tc(\\bm{x})&amp;=\\bm{x} - \\mu(\\bm{x})\\bm{1}\\\\ \t&amp;= \\bm{x} - \\frac{1}{d}\\bm{x}\\bm{1}^\\top\\bm{1}\\\\ \t&amp;= \\bm{x} \\left(1 - \\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right) \\end{aligned}\\]  By the way, $I - \\frac{1}{d}\\bm{1}^\\top\\bm{1}$ is called the  centering matrix.   Let $\\text{RMS}(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns the element-wise RMS (root mean square):   \\[\\begin{align} \t\\text{RMS}(\\bm{x})&amp;=\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2}\\\\ \t&amp;=\\frac{\\sqrt{\\sum_{i=1}^d x_i^2}}{\\sqrt{d}}\\\\ \t&amp;=\\frac{||\\bm{x}||_2}{\\sqrt{d}} \\end{align}\\]  Let $\\text{MS}(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns the squared RMS (root mean square):   \\[\\begin{align} \t\\text{MS}(\\bm{x})&amp;=\\text{RMS}(\\bm{x})^2\\\\ \t&amp;=\\frac{||\\bm{x}||_2^2}{d}\\\\ \t&amp;=\\frac{1}{d}\\sum_{i=1}^d x_i^2\\\\ \\end{align}\\]  Let $\\text{Var}(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns element-wise variance:   \\[\\begin{align} \\text{Var}(\\bm{x}) &amp;= \\frac{1}{d}\\sum_{i=1}^d (x_i - \\mu(\\bm{x}))^2\\\\ &amp;=\\frac{1}{d}\\sum_{i=1}^d (\\bm{x} - \\mu(\\bm{x})\\bm{1})^2_i\\\\ &amp;=\\frac{1}{d}\\sum_{i=1}^d c(\\bm{x})_i^2\\\\ &amp;=\\text{MS}(c(\\bm{x})) \\end{align}\\]  RMSNorm  PyTorch: RMSNorm   \\[\\text{RMSNorm}(\\bm{x}) = \\frac{\\bm{x}}{\\sqrt{\\text{MS}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma}\\]  Here, $\\odot$ is element-wise multiplication, $\\bm{\\gamma}\\in \\mathbb{R}^d$ is a learnable weight vector, and $\\varepsilon$ is a small constant for numerical stability.   LayerNorm  PyTorch: LayerNorm   In the original form:   \\[\\text{LayerNorm}(\\bm{x}) = \\frac{\\bm{x} - \\mu(\\bm{x})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma} +\\bm{\\beta}\\]  This can be rewritten using the centering function $c(\\bm{x})$ and the MS function $\\text{MS}(\\bm{x})$ as follows:   \\[\\begin{aligned} \\text{LayerNorm}(\\bm{x}) &amp;= \\frac{c(\\bm{x})}{\\sqrt{\\text{MS}(c(\\bm{x}))+\\varepsilon}}\\odot \\bm{\\gamma} + \\bm{\\beta}\\\\ \\end{aligned}\\]  Thus, the following holds: LayerNorm is equal to \"centering\" â†’ RMSNorm â†’ \"add bias\"   \\[\\text{LayerNorm}(\\bm{x}) = \\text{RMSNorm}(c(\\bm{x})) + \\bm{\\beta}\\]  Also, element-wise multiplication of $\\bm{\\gamma}$ can be expressed as matrix multiplication of $\\text{diag}(\\bm{\\gamma})$. Therefore, LayerNorm can be rewritten as:   \\[\\begin{align} \\text{LayerNorm}(\\bm{x}) &amp;= \\frac{1}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\left(\\bm{x} - \\mu(\\bm{x})\\bm{1}\\right)\\odot \\bm{\\gamma} + \\bm{\\beta}\\\\ &amp;= \\frac{\\bm{x}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\left(I - \\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\text{diag}(\\bm{\\gamma}) + \\bm{\\beta}\\\\ \\end{align}\\]  Thus only non-linear operation in LayerNorm is the division by $\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}$.  ","categories": [],
        "tags": [],
        "url": "/notes/layernorm/",
        "teaser": "/assets/img/layernorm_rmsnorm.png"
      },{
        "title": "LogitLens without bias",
        "excerpt":"   LogitLens[1] applies $\\text{LMHead}$ to the internal representations $(\\bm{h} \\in \\mathbb{R}^{1\\times d})$ of a transformer model.   \\[\\begin{equation} \\text{LMHead}(\\bm{h}) = \\text{LN}_\\text{f}(\\bm{h})\\bm{E}^O \\label{eq:lm_head} \\end{equation}\\]  Here, $\\bm{E}^O \\in \\mathbb{R}^{d\\times |\\mathcal{V}|}$ is the unembedding matrix and $\\text{LN}_\\text{f}$ is the final layer normalization of a transformer model. In this page, we assume LayerNorm (not RMSNorm) is used for $\\text{LN}_\\text{f}$ , which is defined as follows.   \\[\\begin{equation} \\text{LN}_\\text{f}(\\bm{h}) = \\frac{\\bm{x} - \\mu(\\bm{x})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma} +\\bm{\\beta} \\label{eq:lm_f} \\end{equation}\\]  Here, $\\mu(\\bm{h}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ is a function that returns element-wise mean of a row-vector $\\bm{h}$ and $\\bm{\\gamma}, \\bm{\\beta} \\in \\mathbb{R}^{d}$ are learnable parameters. $\\odot$ represents element-wise multiplication.   With LogitLens, one can project the hidden states after each transformer layers to the vocabulary space.             Example of LogitLens.   By combining Equation\\eqref{eq:lm_head} and \\eqref{eq:lm_f}, we get a bias term for the projection to vocabulary space.   \\[\\begin{align} \\text{LogitLens}(\\bm{h}) &amp;= \\left(\\frac{\\bm{h} - \\mu(\\bm{h})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma} + \\bm{\\beta}\\right)\\bm{E}^O\\\\ &amp;= \\left(\\frac{\\bm{h} - \\mu(\\bm{h})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma}\\right)\\bm{E}^O + \\bm{\\beta}\\bm{E}^O \\label{eq:lm_head_bias} \\end{align}\\]  The second term in Equation\\eqref{eq:lm_head_bias}, which is $\\bm{\\beta}\\bm{E}^O \\in \\mathbb{R}^{|\\mathcal{V}|}$ is the bias term, which is added to the result of LogitLens regardless of the input. Adding such bias may not reasonable when analyzing â€œwhat the modelâ€™s intermediate states representâ€ as Kobayashi et al. (2023) reports that word frequency in the training corpus is encoded in this bias term of $\\text{LN}_\\text{f}$ in GPT-2 model.   By removing the bias term, we get the following result.             Vanilla LogitLens (GPT-2)            LogitLens w/o Bias (GPT-2)             Vanilla LogitLens (OPT)            LogitLens w/o Bias (OPT)   References                         Nostalgibraist 2020, interpreting GPT: the logit lens.                            Kobayashi et al. 2023, Transformer Language Models Handle Word Frequency in Prediction         Head.       ","categories": [],
        "tags": [],
        "url": "/notes/logitlens-wob/",
        "teaser": "/assets/img/logit_lens_nobias_gpt2.png"
      },{
        "title": "Folding weights in transformers",
        "excerpt":"This reformulation of LayerNorm and Self-Attention is used in our paper:                                   Kamoda, G.,           Heinzerling, B.,           Inaba, T.,           Kudo, K.,           Sakaguchi, K.,      &amp;           Inui, K.  (2025).  Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference.  In Findings of the Association for Computational Linguistics: NAACL 2025.                                                ACL Anthology     &nbsp;                                    arXiv     &nbsp;                                    GitHub                 Notation   \\[\\begin{alignat}{4}     &amp;\\bm{X} &amp;:= &amp;     \\begin{bmatrix}         \\bm{x}_1\\\\         \\vdots\\\\         \\bm{x}_n     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{n \\times d}\\\\     &amp;\\bm{W}^O &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^O_1\\\\         \\vdots\\\\         \\bm{W}^O_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}\\\\     &amp;\\bm{W}^Q &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^Q_1 &amp; \\cdots &amp; \\bm{W}^Q_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp; \\label{eq:wq_split}\\\\     &amp;\\bm{W}^K &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^K_1 &amp; \\cdots &amp; \\bm{W}^K_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp; \\label{eq:wk_split}\\\\     &amp;\\bm{W}^V &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^V_1 &amp; \\cdots &amp; \\bm{W}^V_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp;\\label{eq:wv_split}\\\\     &amp;\\bm{b}^Q &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^Q_1 &amp; \\cdots &amp; \\bm{b}^Q_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d}&amp; \\label{eq:bq_split}\\\\     &amp;\\bm{b}^K &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^K_1 &amp; \\cdots &amp; \\bm{b}^K_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in&amp; \\mathbb{R}^{d}&amp; \\label{eq:bk_split}\\\\     &amp;\\bm{b}^V &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^V_1 &amp; \\cdots &amp; \\bm{b}^V_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d}&amp; \\\\     &amp;\\bm{I} &amp;:= &amp;     \\begin{bmatrix}         1 &amp; 0 &amp; \\cdots &amp; 0 \\\\         0 &amp; 1 &amp; \\cdots &amp; 0 \\\\         \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\         0 &amp; 0 &amp; \\cdots &amp; 1 \\\\     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d\\times d}&amp; \\\\     &amp;\\bm{1} &amp;:= &amp;     \\begin{bmatrix}         1 &amp; \\cdots &amp; 1     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d} \\end{alignat}\\]  Original LayerNorm   Layer Normalization can be expressed as follows (org stands for original):   \\[\\begin{alignat}{3}     &amp;\\text{LN}(\\bm{x}) &amp;:=&amp;\\ \\frac{\\bm{x}-\\mu(\\bm{x})\\bm{1}}{\\sigma(\\bm{x})}\\odot\\bm{\\gamma} + \\bm{\\beta}&amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     &amp;\\bm{x} &amp;:=&amp;\\      \\begin{bmatrix}         x^{(1)} &amp; \\cdots &amp; x^{(d)}     \\end{bmatrix}     &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     &amp;\\mu(\\bm{x}) &amp;:=&amp;\\ \\frac{1}{d}\\sum_kx^{(k)}&amp;\\hspace{1em}\\in&amp;\\mathbb{R}\\\\     &amp;\\sigma(\\bm{x}) &amp;:=&amp;\\ \\sqrt{\\frac{1}{d}\\sum_k^d\\left(x^{(k)}-\\mu(\\bm{x})\\right)^2+\\epsilon}&amp;\\hspace{1em}\\in&amp;\\mathbb{R} \\end{alignat}\\]  Now, $\\mu(\\bm{x})$ can be reformulated as follows:   \\[\\begin{align}     \\mu(\\bm{x})\\bm{1}     &amp;=\\frac{1}{d}\\left(\\sum_kx^{(k)}\\right)\\bm{1}\\\\     &amp;=\\frac{1}{d}\\left(\\bm{x}\\bm{1}^\\top\\right)\\bm{1}\\\\     &amp;=\\bm{x}\\left(\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right) \\end{align}\\]  Thus $\\text{LN}_{\\text{org}}$ can be reformulated as follows.   \\[\\begin{align}     \\text{LN}(\\bm{x})      &amp;= \\frac{\\bm{x}-\\mu(\\bm{x})\\bm{1}}{\\sigma(\\bm{x})}\\odot\\bm{\\gamma} + \\bm{\\beta}\\\\     &amp;= \\frac{1}{\\sigma(\\bm{x})} \\left(\\bm{x}-\\bm{x}\\left(\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\right) \\diag{\\bm\\gamma}+ \\bm{\\beta}\\\\     &amp;= \\frac{\\bm{x}}{\\sigma(\\bm{x})}\\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm\\gamma}+ \\bm{\\beta} \\end{align}\\]  Original Self-Attention   Let query, key, value transformations of each head $h$ be expressed as follows:   \\[\\begin{align}     \\bm{q}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^Q + \\bm{b}_h^Q\\\\     \\bm{k}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^K + \\bm{b}_h^K\\\\     \\bm{v}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^V + \\bm{b}_h^V\\\\ \\end{align}\\]  Let attention weight from token position $i$ to $j$  ($i \\ge j$) in head $h$ be expressed as follows:   \\[\\begin{align} \\alpha_{i, j, h} &amp;= \\frac{\\exp(s_{i, j, h})}{\\sum_{j'} \\exp(s_{i, j', h})}\\\\ s_{i, j, h} &amp;:= \\frac{\\bm{q}_h(\\bm{x}_i)\\bm{k}_h(\\bm{x}_j)^\\top}{\\sqrt{d'}} \\end{align}\\]  where $dâ€™ = d/H$ is the dimension of each head.   The output of Attention layer of an causal model at position $i$ can be expressed as follows:   \\[\\begin{align}     \\text{ATTN}(i, \\bm{X})         &amp;:=\\left[\\text{head}_1(i, \\bm{X})\\hspace{0.5em}\\cdots\\hspace{0.5em}\\text{head}_H(i, \\bm{X})\\right]             \\bm{W}^O + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\text{head}_h(i, \\bm{X})\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\bm{v}_h(\\bm{x}_j)\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\left(\\bm{x}_j\\bm{W}^V_h + \\bm{b}^V_h\\right)\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h}\\bm{b}^V_h\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H\\bm{b}^V_h\\bm{W}^O_h + \\bm{b}^O\\hspace{0.5em} \\left(\\because \\sum_j \\alpha_{i, j, h} = 1\\right)\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\bm{b}^V\\bm{W}^O + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h} \\bm{x}_j\\bm{W}^{VO}_h + \\bm{b}^{VO} \\end{align}\\]  where   \\[\\begin{align}     \\bm{W}^{VO}_h &amp;:= \\bm{W}^V_h\\bm{W}^O_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\bm{b}^{VO} &amp;:= \\bm{b}^V\\bm{W}^O + \\bm{b}^O &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  Reformulating LayerNorm and Self-Attention   LayerNorm is always followed by a linear transformation in transformers. Thus, we can fold the weights of LayerNorm into the weights of the following linear transformation.   For example, in the case of LayerNorm followed by query transformation, we can fold the weights as follows:   \\[\\begin{align}     \\bm{q}_h(\\text{LN}(\\bm{x}))         &amp;= \\text{LN}(\\bm{x})\\bm{W}^Q_h + \\bm{b}^Q_h\\\\         &amp;= \\left(\\frac{\\bm{x}}{\\sigma(\\bm{x})}\\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}+ \\bm{\\beta}\\right)\\bm{W}^Q_h + \\bm{b}^Q_h\\\\         &amp;= \\frac{\\bm{x}}{\\sigma(\\bm{x})}\\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^Q_h + \\bm{\\beta}\\bm{W}^Q_h + \\bm{b}^Q_h\\\\         &amp;= \\overset{\\text{new}}{\\text{LN}}(\\bm{x})\\overset{\\text{new}}{\\bm{W}^Q_h} + \\overset{\\text{new}}{\\bm{b}^Q_h} \\end{align}\\]  where   \\[\\begin{align}     \\overset{\\text{new}}{\\text{LN}}(\\bm{x}) &amp;:= \\frac{\\bm{x}}{\\sigma(\\bm{x})} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\bm{W}^Q_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^Q_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{b}^Q_h} &amp;:= \\bm{\\beta}\\bm{W}^Q_h + \\bm{b}^{\\{Q, K\\}}_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  The same can be done for key and value transformations, and thus LayerNorm followed by self-attention can be reformulated as follows:   \\[\\begin{align}     \\overset{\\text{new}}{\\text{LN}}(\\bm{x}) &amp;:= \\frac{\\bm{x}}{\\sigma(\\bm{x})} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\text{ATTN}}(i, \\bm{X})         &amp;:= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h} \\bm{x}_j\\overset{\\text{new}}{\\bm{W}^{VO}_h} + \\overset{\\text{new}}{\\bm{b}^{VO}} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\ \\end{align}\\]  where   \\[\\begin{align}     \\overset{\\text{new}}{\\alpha_{i, j, h}} &amp;:= \\underset{\\bm{x}_j,\\bm{x}_j \\in \\bm{X}, j \\leq i}{\\text{softmax}}\\frac{\\overset{\\text{new}}{\\bm{q}_h}(\\bm{x}_i)\\overset{\\text{new}}{\\bm{k}_h}(\\bm{x}_j)^\\top}{\\sqrt{d'}} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}\\\\     \\overset{\\text{new}}{\\bm{q}_h}(\\bm{x}) &amp;:= \\bm{x}\\overset{\\text{new}}{\\bm{W}^Q_h} + \\overset{\\text{new}}{\\bm{b}^Q_h} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\bm{k}_h}(\\bm{x}) &amp;:= \\bm{x}\\overset{\\text{new}}{\\bm{W}^K_h} + \\overset{\\text{new}}{\\bm{b}^K_h} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\bm{W}^Q_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^Q_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{W}^K_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^K_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{b}^Q_h} &amp;:= \\bm{\\beta}\\bm{W}^Q_h + \\bm{b}^Q_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\     \\overset{\\text{new}}{\\bm{b}^K_h} &amp;:= \\bm{\\beta}\\bm{W}^K_h + \\bm{b}^K_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  and   \\[\\begin{align}     \\overset{\\text{new}}{\\bm{W}^{VO}_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^V_h\\bm{W}^O_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{b}^{VO}} &amp;:= \\bm{\\beta}\\bm{W}^V\\bm{W}^O + \\bm{b}^V\\bm{W}^O + \\bm{b}^O &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  ","categories": [],
        "tags": [],
        "url": "/notes/fold-weights/",
        "teaser": "/assets/img/folding-weights.png"
      },{
        "title": "GPT-OSS Attnention Sink",
        "excerpt":"tl;dr: The bias term for preventing attention sink in GPT-OSS may have other effects than just preventing attention sink. (made public on 2026-01-22)   Notation   \\[\\begin{alignat}{4}     &amp;\\bm{X} &amp;:= &amp;     \\begin{bmatrix}         \\bm{x}_1\\\\         \\vdots\\\\         \\bm{x}_n     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{n \\times d}\\\\     &amp;\\bm{W}^O &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^O_1\\\\         \\vdots\\\\         \\bm{W}^O_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}\\\\     &amp;\\bm{W}^Q &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^Q_1 &amp; \\cdots &amp; \\bm{W}^Q_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp; \\label{eq:wq_split}\\\\     &amp;\\bm{W}^K &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^K_1 &amp; \\cdots &amp; \\bm{W}^K_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp; \\label{eq:wk_split}\\\\     &amp;\\bm{W}^V &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^V_1 &amp; \\cdots &amp; \\bm{W}^V_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp;\\label{eq:wv_split}\\\\     &amp;\\bm{b}^Q &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^Q_1 &amp; \\cdots &amp; \\bm{b}^Q_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d}&amp; \\label{eq:bq_split}\\\\     &amp;\\bm{b}^K &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^K_1 &amp; \\cdots &amp; \\bm{b}^K_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in&amp; \\mathbb{R}^{d}&amp; \\label{eq:bk_split}\\\\     &amp;\\bm{b}^V &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^V_1 &amp; \\cdots &amp; \\bm{b}^V_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d}&amp; \\\\     &amp;\\bm{I} &amp;:= &amp;     \\begin{bmatrix}         1 &amp; 0 &amp; \\cdots &amp; 0 \\\\         0 &amp; 1 &amp; \\cdots &amp; 0 \\\\         \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\         0 &amp; 0 &amp; \\cdots &amp; 1 \\\\     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d\\times d}&amp; \\\\     &amp;\\bm{1} &amp;:= &amp;     \\begin{bmatrix}         1 &amp; \\cdots &amp; 1     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d} \\end{alignat}\\]  Self-Attention   Let query, key, value transformations of each head $h$ be expressed as follows:   \\[\\begin{align}     \\bm{q}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^Q + \\bm{b}_h^Q\\\\     \\bm{k}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^K + \\bm{b}_h^K\\\\     \\bm{v}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^V + \\bm{b}_h^V\\\\ \\end{align}\\]  Let attention weight from token position $i$ to $j$  ($i \\ge j$) in head $h$ be expressed as follows:   \\[\\begin{align} \\alpha_{i, j, h} &amp;= \\frac{\\exp(s_{i, j, h})}{\\exp(b^{S}_h)+\\sum_{j'} \\exp(s_{i, j', h})}\\\\ s_{i, j, h} &amp;:= \\frac{\\bm{q}_h(\\bm{x}_i)\\bm{k}_h(\\bm{x}_j)^\\top}{\\sqrt{d'}} \\end{align}\\]  where $dâ€™ = d/H$ is the dimension of each head, and $b^{S}_h$ is a learned scalar parameter introduced in GPT-OSS for preventing attention sink.   Now, let the attention weight assigned to the sink be expressed as follows:   \\[\\alpha_{i, \\text{sink}, h} := \\frac{\\exp(b^{S}_h)}{\\exp(b^{S}_h)+\\sum_{j'} \\exp(s_{i, j', h})}\\]  Due to the presence of $b^{S}_h$, the following holds:   \\[\\begin{align} &amp;1 = \\alpha_{i, \\text{sink}, h} + \\sum_j \\alpha_{i, j, h}\\\\ &amp;\\Leftrightarrow \\sum_j \\alpha_{i, j, h} = 1 - \\alpha_{i, \\text{sink}, h} \\end{align}\\]  The output of Attention layer of an causal model at position $i$ can be expressed as follows:   \\[\\begin{align}     \\text{ATTN}(i, \\bm{X})         &amp;:=\\left[\\text{head}_1(i, \\bm{X})\\hspace{0.5em}\\cdots\\hspace{0.5em}\\text{head}_H(i, \\bm{X})\\right]             \\bm{W}^O + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\text{head}_h(i, \\bm{X})\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\bm{v}_h(\\bm{x}_j)\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\left(\\bm{x}_j\\bm{W}^V_h + \\bm{b}^V_h\\right)\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\left(\\bm{x}_j\\bm{W}^V_h + \\bm{b}^V_h\\right)\\bm{W}^O_h\\right) + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\left(\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\bm{b}^V_h\\bm{W}^O_h\\right)\\right) + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h}\\bm{b}^V_h\\bm{W}^O_h\\right) + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h}\\right)\\bm{b}^V_h\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H \\left(1-\\alpha_{i, \\text{sink}, h}\\right)\\bm{b}^V_h\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H\\bm{b}^V_h\\bm{W}^O_h - \\sum_{h=1}^H\\alpha_{i, \\text{sink}, h}\\bm{b}^V_h\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H\\alpha_{i, \\text{sink}, h}(-\\bm{b}^V_h\\bm{W}^O_h)+ \\sum_{h=1}^H\\bm{b}^V_h\\bm{W}^O_h  + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h+ \\sum_{h=1}^H\\alpha_{i, \\text{sink}, h}(-\\bm{b}^V_h\\bm{W}^O_h) + \\bm{b}^V\\bm{W}^O + \\bm{b}^O\\\\ \\end{align}\\]  Thus, in GPT-OSS, the newly introduced attention sink bias $b^{S}_h$     makes the attention weights sum to less than 1 for non-sink tokens, and   changes the intensity of the bias term of the value transformation based on the attention weight assigned to the sink.  ","categories": [],
        "tags": [],
        "url": "/notes/oss/",
        "teaser": "/assets/img/gptoss-bias.png"
      },{
        "title": "ãƒ¡ãƒ¢: PCA",
        "excerpt":"$n$ å€‹ã® $d$ æ¬¡å…ƒã®ãƒ‡ãƒ¼ã‚¿ $\\bm{X}$ ã‚’ã€$dâ€™$ æ¬¡å…ƒã«åœ§ç¸®ã™ã‚‹ãŸã‚ã®ç·šå½¢å¤‰æ›è¡Œåˆ— $\\bm{W}$ ã‚’æ±‚ã‚ã‚‹ã€‚   \\[\\bm{X} =\\begin{bmatrix} \\bm{x}_1 \\\\ \\bm{x}_2 \\\\ \\vdots \\\\ \\bm{x}_n \\end{bmatrix} = \\begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,d} \\\\ x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,d} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,d} \\end{bmatrix} \\in \\mathbb{R}^{n \\times d}\\]  \\[\\bm{W} = \\begin{bmatrix} \\bm{w}_1 &amp; \\bm{w}_2 &amp; \\cdots &amp; \\bm{w}_{d'} \\\\ \\end{bmatrix} = \\begin{bmatrix} w_{1,1} &amp; w_{1,2} &amp; \\cdots &amp; w_{1,d'} \\\\ w_{2,1} &amp; w_{2,2} &amp; \\cdots &amp; w_{2,d'} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_{d,1} &amp; w_{d,2} &amp; \\cdots &amp; w_{d,d'} \\end{bmatrix} \\in \\mathbb{R}^{d \\times d'}\\]  å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿ $\\bm{Y}$ ã¯æ¬¡ã®ã‚ˆã†ã«è¡¨ã•ã‚Œã‚‹ã€‚ \\(\\bm{Y} = \\bm{X} \\bm{W} \\in \\mathbb{R}^{n \\times d'}\\)   PCAã§ã¯ã€åœ§ç¸®å¾Œã®å„æ¬¡å…ƒã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã€‚  åœ§ç¸®å¾Œã® $j\\ (1\\le j \\le dâ€™)$æ¬¡å…ƒç›® ã®åˆ†æ•£ã¯ã€å®šç¾©ã‚ˆã‚Šä»¥ä¸‹ã§è¡¨ã›ã‚‹:   \\[\\begin{align} s_j^2  &amp;= \\frac{1}{n}\\sum_{i=1}^{n}(y_{ij}-\\bar{y_j})^2\\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\left(     \\bm{x}_i \\bm{w}_j - \\frac{1}{n}\\sum_{k=1}^{n}\\bm{x}_k \\bm{w}_j \\right)^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\left(     \\left(\\bm{x}_i - \\frac{1}{n}\\sum_{k=1}^{n}\\bm{x}_k\\right) \\bm{w}_j \\right)^2 \\\\ \\end{align}\\]  ãŸã ã— $\\bar{y_j} = \\frac{1}{n}\\sum_{i=1}^{n}y_{ij}$ ã¯ $j$ æ¬¡å…ƒç›®ã®å¹³å‡å€¤ã‚’è¡¨ã™ã€‚   ã“ã“ã§ã€$\\bar{\\bm{x}} = \\frac{1}{n}\\sum_{i=1}^{n}\\bm{x}_i \\in \\mathbb{R}^{d}$ ã¨ã—ã€ ä¸­å¿ƒåŒ– (å„æ¬¡å…ƒã®å¹³å‡ã‚’ã‚¼ãƒ­ã«) ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ $\\bm{X}^c$ ã¨ã™ã‚‹ã€‚ $\\bm{X}^c$ ã¯ä»¥ä¸‹ã§å®šç¾©ã•ã‚Œã‚‹ã€‚   \\[\\begin{align} \\bm{X}^c &amp;= \\begin{bmatrix} \\bm{x}_1 \\\\ \\bm{x}_2 \\\\ \\vdots \\\\ \\bm{x}_n \\end{bmatrix} -  \\begin{bmatrix} \\bar{\\bm{x}} \\\\ \\bar{\\bm{x}} \\\\ \\vdots \\\\ \\bar{\\bm{x}} \\\\ \\end{bmatrix} =\\begin{bmatrix} \\bm{x}_1^c \\\\ \\bm{x}_2^c \\\\ \\vdots \\\\ \\bm{x}_n^c \\end{bmatrix} \\end{align}\\]  \\[\\begin{align} \\bm{x}^c_i &amp;=\\bm{x}_i - \\bar{\\bm{x}}\\\\ &amp;=\\bm{x}_i - \\frac{1}{n}\\sum_{k=1}^{n}\\bm{x}_k \\end{align}\\]  ã™ã‚‹ã¨ã€æ¬¡å…ƒ $j\\ (1\\le j \\le dâ€™)$ ã®åˆ†æ•£ã¯ä»¥ä¸‹ã§è¡¨ã›ã‚‹:   \\[\\begin{align} s_j^2  &amp;= \\frac{1}{n}\\sum_{i=1}^{n}(\\bm{x}_i^c \\bm{w}_j)^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}(\\bm{x}_i\\bm{w}_j^c)^\\top (\\bm{x}_i^c \\bm{w}_j) \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\bm{w}_j^\\top(\\bm{x}_i^c\\top\\bm{x}_i^c)\\bm{w}_j \\\\ &amp;= \\frac{1}{n}\\bm{w}_j^\\top \\left(\\sum_{i=1}^{n}\\bm{x}_i^c\\top\\bm{x}_i^c\\right) \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\left(\\frac{1}{n}\\bm{X}^{c\\top}\\bm{X}^c\\right) \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\end{align}\\]  ã“ã“ã§ã€$\\bm{S} \\in \\mathbb{R}^{d \\times d}$ ã¯ãƒ‡ãƒ¼ã‚¿$\\bm{X}$ã®ã€å…±åˆ†æ•£è¡Œåˆ—ã¨å‘¼ã°ã‚Œã‚‹ã€‚   PCAã§ã¯ã€$s_j^2 (1\\le j \\le dâ€™)$ã‚’æœ€å¤§åŒ–ã™ã‚‹ $\\bm{w}_j$ ã‚’æ±‚ã‚ã‚‹ã€‚ ãªãŠã€ã“ã“ã§$\\bm{w}_j$ã‚’å¤§ããã™ã‚Œã°$s_j^2$ã‚‚å¤§ãããªã‚‹ãŸã‚ã€$\\bm{w}_j$ã¯å˜ä½ãƒ™ã‚¯ãƒˆãƒ«ã€ã¤ã¾ã‚Š \\(\\|\\bm{w}_j\\|^2 = \\bm{w}_j^\\top \\bm{w}_j = 1\\) ã®æˆç´„ã‚’è¨­ã‘ã‚‹ã€‚   çµæœã€è§£ããŸã„å•é¡Œã¯   \\[\\underset{\\bm{w}_j}{\\text{argmax}}\\quad \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\quad \\text{subject to}\\quad  \\bm{w}_j^\\top \\bm{w}_j = 1\\]  ã“ã‚Œã‚’ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã‚’ç”¨ã„ã¦è§£ãã“ã¨ã‚’è€ƒãˆã‚‹ã€‚ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥é–¢æ•°ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹ã€‚   \\[\\begin{align} \\mathcal{L}(\\bm{w}_j, \\lambda) &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j - \\lambda (\\bm{w}_j^\\top \\bm{w}_j - 1) \\\\ \\end{align}\\]  ã“ã‚Œã‚’$W_j$ã§å¾®åˆ†ã™ã‚Œã°ã€   \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial \\bm{w}_j} &amp;= 2 \\bm{S} \\bm{w}_j - 2 \\lambda \\bm{w}_j \\end{align}\\]  ã—ãŸãŒã£ã¦ã€ä»¥ä¸‹ã‚’æº€ãŸã™ã¨ãã«æ¥µå€¤ã‚’ã¨ã‚‹ã€‚   \\[\\bm{S} \\bm{w}_j = \\lambda \\bm{w}_j\\]  ã—ãŸãŒã£ã¦ã€$\\bm{w}_j$ã¯$\\bm{S}$ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚Šã€$\\lambda$ã¯å¯¾å¿œã™ã‚‹å›ºæœ‰å€¤ã§ã‚ã‚‹ã€‚  ã¾ãŸã€$s_j^2$ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‰ã‚Œã‚‹ã€‚ \\(\\begin{align} s_j^2 &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\lambda \\bm{w}_j \\\\ &amp;= \\lambda \\bm{w}_j^\\top \\bm{w}_j \\\\ &amp;= \\lambda \\end{align}\\)   PCAã§ã¯ã€$dâ€™$ å€‹ã®æ¬¡å…ƒã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ãŸã‚ã€$\\bm{S}$ã®å›ºæœ‰å€¤ãŒå¤§ãã„é †ã«å¯¾å¿œã™ã‚‹$dâ€™$ å€‹ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã¨ã‚Šä¸¦ã¹ãŸã‚‚ã®ã‚’$\\bm{W}$ã¨ã™ã‚Œã°ã‚ˆã„ã€‚   (å¾©ç¿’) å›ºæœ‰å€¤åˆ†è§£   è¡Œåˆ— $\\bm{A} \\in \\mathbb{R}^{d\\times d}$ ã®å›ºæœ‰å€¤åˆ†è§£ã¯ã€ç›´äº¤è¡Œåˆ—$\\bm{V}$ã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ã•ã‚Œã‚‹ã€‚   \\[\\bm{A} = \\bm{V} \\bm{\\Lambda} \\bm{V}^\\top\\]  ã“ã“ã§ã€   \\[\\bm{\\Lambda} = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_d)\\]  \\[\\bm{V} = \\begin{bmatrix} \\bm{v}_1 &amp; \\bm{v}_2 &amp; \\cdots &amp; \\bm{v}_d \\end{bmatrix}\\]  ã¨ã™ã‚‹ã€‚ $\\lambda_i$ ã¯ $\\bm{A}$ ã®å›ºæœ‰å€¤ã€$\\bm{v}_i$ ã¯å¯¾å¿œã™ã‚‹å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹ã€‚   ã¾ãŸã€ä»¥ä¸‹ã®ã‚ˆã†ã«å¤‰å½¢ã§ãã‚‹ã€‚   \\[\\begin{align} \\bm{A} &amp;= \\bm{V} \\bm{\\Lambda} \\bm{V}^\\top\\\\ &amp;=  \\begin{bmatrix} \\bm{v}_1 &amp; \\bm{v}_2 &amp; \\cdots &amp; \\bm{v}_d \\end{bmatrix} \\begin{bmatrix} \\lambda_1 \\bm{v}_1 \\\\ \\lambda_2 \\bm{v}_2 \\\\ \\vdots \\\\ \\lambda_d \\bm{v}_d \\end{bmatrix} \\\\ &amp;= \\sum_{i=1}^{d} \\lambda_i \\bm{v}_i \\bm{v}_i^\\top \\end{align}\\]  $\\bm{S}$ã®å›ºæœ‰å€¤/ãƒ™ã‚¯ãƒˆãƒ«ã¯$X$ã®ç‰¹ç•°å€¤åˆ†è§£ (SVD) ã§æ±‚ã‚ã‚‰ã‚Œã‚‹   å†æ²: $\\bm{S} \\in \\mathbb{R}^{d\\times d}$, $\\bm{X}^c \\in \\mathbb{R}^{n\\times d}$.   å¤‰æ›è¡Œåˆ— $\\bm{W} \\in \\mathbb{R}^{d\\times dâ€™}$ ã‚’æ±‚ã‚ã‚‹ãŸã‚ã«ã€$\\bm{S}$ã®å›ºæœ‰å€¤ã‚’æ±‚ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ ã“ã“ã§ã€$\\bm{S}$ ã‚’ $\\bm{X}^c$ ã«é–¢ã—ã¦æ›¸ãæ›ãˆã€$\\bm{X}^c$ ã®ç‰¹ç•°å€¤åˆ†è§£ (SVD) ã‚’ç”¨ã„ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹ã€‚   \\[\\begin{align} \\bm{S} &amp;= \\frac{1}{n} \\bm{X}^{c\\top} \\bm{X}^c \\\\ &amp;= \\frac{1}{n} \\left(\\bm{U}\\bm{\\Sigma} \\bm{V}^{\\top}\\right)^\\top \\left(\\bm{U}\\bm{\\Sigma} \\bm{V}^{\\top}\\right) \\\\ &amp;= \\frac{1}{n} \\bm{V} \\bm{\\Sigma}^\\top \\bm{U}^\\top \\bm{U} \\bm{\\Sigma} \\bm{V}^{\\top} \\\\ &amp;= \\frac{1}{n} \\bm{V} \\bm{\\Sigma}^{2} \\bm{V}^{\\top}\\quad (\\because \\bm{U} \\text{ is orthogonal})\\\\ &amp;= \\bm{V} \\left(\\frac{\\bm{\\Sigma}^{2}}{n} \\right) \\bm{V}^{\\top} \\\\ \\end{align}\\]  ã¤ã¾ã‚Šã€ä»¥ä¸‹ãŒè¨€ãˆã‚‹:     $\\bm{S}$ ã® å›ºæœ‰å€¤ã¯ [{$\\bm{X}^c$ ã‚’ç‰¹ç•°å€¤åˆ†è§£ã—ã¦å¾—ã‚‰ã‚Œã‚‹å›ºæœ‰å€¤ã‹ã‚‰ãªã‚‹è¡Œåˆ—$\\bm{\\Sigma}$} ã‚’ç”¨ã„ã¦è¡¨ã•ã‚Œã‚‹ $\\bm{\\Sigma}^{2}/n$ ]ã®å„å¯¾è§’æˆåˆ†   $\\bm{S}$ ã® å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã¯ $\\bm{X}^c$ ã‚’ç‰¹ç•°å€¤åˆ†è§£ã—ã¦å¾—ã‚‰ã‚Œã‚‹ $\\bm{V}$ ã®å„åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹ã€‚   ã—ãŸãŒã£ã¦ã€PCAã‚’è¡Œã†éš›ã¯ã€$\\bm{X}^{c\\top}\\bm{X}^c$ ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ã¯ãªãã€$\\bm{X}^c$ ã‚’ç‰¹ç•°å€¤åˆ†è§£ã—ã¦ã€$\\bm{V}$ ã®ã†ã¡ã€å¯¾å¿œã™ã‚‹å›ºæœ‰å€¤ãŒå¤§ãã„ $dâ€™$ å€‹ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã¨ã‚Šä¸¦ã¹ãŸã‚‚ã®ã‚’ $\\bm{W}$ ã¨ã™ã‚Œã°ã‚ˆã„ã€‚   é‡ã¿ä»˜ãPCA   PCAã§ã¯ã€å„ãƒ‡ãƒ¼ã‚¿ç‚¹ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç¢ºç‡ãŒç­‰ã—ã„ã“ã¨ã‚’ä»®å®šã—ã¦ã„ã‚‹ (ã‚‚ã¡ã‚ã‚“ã€ãã®åˆ†é‡è¤‡ã‚’è¨±ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚Œã°è‰¯ã„)ã€‚ å„ç‚¹ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç¢ºç‡ $p(x)$ ã‚’è€ƒæ…®ã—ãŸé‡ã¿ä»˜ãã®PCAã«ã¤ã„ã¦è€ƒãˆã‚‹ã€‚   ã¾ãšã€ä¸­å¿ƒåŒ–ã«ã¤ã„ã¦ã€å„ç‚¹ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç¢ºç‡ã‚’è€ƒæ…®ã—ãŸå¹³å‡ $\\bar{\\bm{x}}$ ã¯ä»¥ä¸‹ã§å®šç¾©ã•ã‚Œã‚‹:   \\[\\bar{\\bm{x}} := \\sum_{i} p(x_i) \\bm{x}_i\\]  ã“ã®å¹³å‡ã‚’ä½¿ã£ã¦ä¸­å¿ƒåŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å†åº¦ $\\bm{X}^c$ ã¨ã™ã‚‹ã€‚   æ¬¡ã«åˆ†æ•£ã«ã¤ã„ã¦ã€æ¬¡å…ƒ $j\\ (1\\le j \\le dâ€™)$ ã®åˆ†æ•£ã¯ä»¥ä¸‹ã§è¡¨ã›ã‚‹:   \\[\\begin{align} s_j^2  &amp;= \\sum_{i=1}^{n}p(x_i)(\\bm{x}_i^c \\bm{w}_j)^2 \\\\ &amp;= \\sum_{i=1}^{n}p(x_i)(\\bm{x}_i\\bm{w}_j^c)^\\top (\\bm{x}_i^c \\bm{w}_j) \\\\ &amp;= \\sum_{i=1}^{n}\\bm{w}_j^\\top(p(x_i)\\bm{x}_i^c\\top\\bm{x}_i^c)\\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\left(\\sum_{i=1}^{n}\\left(\\sqrt{p(x_i)}\\bm{x}_i^c\\right)^\\top\\left(\\sqrt{p(x_i)}\\bm{x}_i^c\\right)\\right) \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\left(\\bm{P}(X) \\bm{X}^c \\right)^\\top\\left(\\bm{P}(X) \\bm{X}^c \\right)\\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\end{align}\\]  ãŸã ã— $\\bm{P}(X) = \\diag{\\sqrt{p(x_1)}, \\ldots, \\sqrt{p(x_n)}}$ ã¨ã™ã‚‹ã€‚   ã—ãŸãŒã£ã¦ã€é€šå¸¸ã®PCAã¨åŒæ§˜ã«è€ƒãˆã‚Œã°ã€PCAã¯ $\\bm{P}(X) \\bm{X}^c $ ã®å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸¦ã¹ã‚Œã°è‰¯ã„ã“ã¨ãŒã‚ã‹ã‚‹  ","categories": [],
        "tags": [],
        "url": "/notes/pca/",
        "teaser": "/assets/img/pca.png"
      },{
        "title": "RoPE Implementation",
        "excerpt":"From paper  Su et al. (2020)   Let $\\bm{q}_m$ be the query vector at position $m$ before applying RoPE, and $\\mathring{\\bm{q}}_m$ be the query vector after applying RoPE.   \\[\\begin{align} \t\\begin{bmatrix} \t\t\\mathring{q}_m^{(1)} \\\\ \\mathring{q}_m^{(2)} \\\\ \\mathring{q}_m^{(3)} \\\\ \\mathring{q}_m^{(4)} \\\\ \t    \\vdots \\\\ \\mathring{q}_m^{(d-1)}\\\\ \\mathring{q}_m^{(d)} \t\\end{bmatrix} \t&amp;= \t\\begin{bmatrix} \t\t\\cos m\\theta_1 &amp; -\\sin m\\theta_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0\\\\ \t\t\\sin m\\theta_1 &amp; \\cos m\\theta_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0\\\\ \t\t 0 &amp; 0 &amp; \\cos m\\theta_2 &amp; -\\sin m\\theta_2 &amp; \\cdots &amp; 0 &amp; 0\\\\ \t\t 0 &amp; 0 &amp; \\sin m\\theta_2 &amp; \\cos m\\theta_2 &amp;  \\cdots &amp; 0 &amp; 0\\\\ \t\t \\vdots &amp; \\vdots  &amp;\\vdots &amp; \\vdots&amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ \t\t 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\cos m\\theta_{d/2} &amp; -\\sin m\\theta_{d/2}\\\\ \t\t 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sin m\\theta_{d/2} &amp; \\cos m\\theta_{d/2} \t\t  \t\\end{bmatrix} \t\\begin{bmatrix} \t\tq_m^{(1)} \\\\ q_m^{(2)} \\\\ q_m^{(3)}\\\\ q_m^{(4)} \\\\ \\vdots \\\\ q_m^{(d-1)} \\\\ q_m^{(d)}  \t\\end{bmatrix}\\\\ \t&amp;=\\begin{bmatrix} \t\tq_m^{(1)}\\cos m\\theta_1 - q_m^{(2)}\\sin m\\theta_1 \\\\ \t\tq_m^{(1)}\\sin m\\theta_1 + q_m^{(2)}\\cos m\\theta_1 \\\\  \t\tq_m^{(2)}\\cos m\\theta_2 - q_m^{(3)}\\sin m\\theta_2 \\\\ \t\tq_m^{(2)}\\sin m\\theta_2 + q_m^{(3)}\\cos m\\theta_2 \\\\  \t\t\\vdots \\\\ \t\tq_m^{(d-1)}\\cos m\\theta_{d/2} - q_m^{(d)}\\sin m\\theta_{d/2} \\\\ \t\tq_m^{(d-1)}\\sin m\\theta_{d/2} + q_m^{(d)}\\cos m\\theta_{d/2} \\\\  \t\\end{bmatrix}\\\\ \\end{align}\\]  Here, $\\theta_i = \\theta_\\text{RoPE}^{-2(i-1)/d}$ and $\\theta_\\text{RoPE}$ is a hyperparameter (typically $10,000$).   Implementation: Llama (-Llama3)   modeling_llama.py: github  def rotate_half(x):     x1 = x[..., : x.shape[-1] // 2]     x2 = x[..., x.shape[-1] // 2 :]     return torch.cat((-x2, x1), dim=-1)  def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):     cos = cos.unsqueeze(unsqueeze_dim)     sin = sin.unsqueeze(unsqueeze_dim)     q_embed = (q * cos) + (rotate_half(q) * sin)     k_embed = (k * cos) + (rotate_half(k) * sin)     return q_embed, k_embed   In Llama implementation, the first half of the dimentions correspond to the odd dimentions in the paper. The second half correspond to the even dimentions in the paper.   \\[\\begin{align} \t\\begin{bmatrix} \t\t\\mathring{q}_m^{(1)} \\\\ \\mathring{q}_m^{(2)} \\\\ \\vdots \\\\ \\mathring{q}_m^{(d/2)} \\\\ \\mathring{q}_m^{(d/2+1)} \\\\ \\mathring{q}_m^{(d/2+2)} \\\\ \\vdots \\\\ \\mathring{q}_m^{(d)} \t\\end{bmatrix} \t&amp;= \t\\begin{bmatrix} \t\t\\cos m\\theta_1 &amp; 0 &amp; 0 &amp; 0 \\\\ \t\t 0 &amp; \\cos m\\theta_2 &amp;0 &amp; 0 &amp; &amp; &amp;\\bm{0} \\\\ \t\t \\vdots &amp; \\vdots  &amp;\\ddots &amp; \\vdots&amp; \\\\ \t\t 0 &amp; 0  &amp; 0&amp; \\cos m\\theta_{d/2}\\\\ \t\t &amp;&amp;&amp;&amp;\\cos m\\theta_1 &amp; 0 &amp; 0 &amp; 0\\\\ \t\t &amp;&amp;&amp;&amp;0 &amp; \\cos m\\theta_2 &amp;0 &amp; 0 \\\\ \t\t &amp;\\bm{0}&amp;&amp;&amp;\\vdots &amp;  \\vdots&amp; \\ddots &amp; \\vdots \\\\ \t\t &amp;&amp;&amp;&amp;0 &amp; 0  &amp; 0&amp; \\cos m\\theta_{d/2}\\\\ \t\\end{bmatrix} \t\\begin{bmatrix} \t\tq_m^{(1)} \\\\ q_m^{(2)} \\\\ \\vdots \\\\ q_m^{(d/2)} \\\\ q_m^{(d/2+1)} \\\\ q_m^{(d/2+2)} \\\\ \\vdots \\\\ q_m^{(d)} \t\\end{bmatrix}\\\\ \t&amp;\\quad + \\begin{bmatrix} \t\t\\sin m\\theta_1 &amp; 0 &amp; 0 &amp; 0 \\\\ \t\t 0 &amp; \\sin m\\theta_2 &amp;0 &amp; 0 &amp; &amp; &amp; \\bm{0} \\\\ \t\t \\vdots &amp; \\vdots  &amp;\\ddots &amp; \\vdots&amp; \\\\ \t\t 0 &amp; 0  &amp; 0&amp; \\sin m\\theta_{d/2}\\\\ \t\t &amp;&amp;&amp;&amp;\\sin m\\theta_1 &amp; 0 &amp; 0 &amp; 0\\\\ \t\t &amp;&amp;&amp;&amp;0 &amp; \\sin m\\theta_2 &amp;0 &amp; 0 \\\\ \t\t &amp;\\bm{0}&amp;&amp;&amp;\\vdots &amp;  \\vdots&amp; \\ddots &amp; \\vdots \\\\ \t\t &amp;&amp;&amp;&amp;0 &amp; 0  &amp; 0&amp; \\sin m\\theta_{d/2}\\\\ \t\\end{bmatrix} \t\\begin{bmatrix} \t\t- q_m^{(d/2+1)} \\\\ - q_m^{(d/2+2)} \\\\ \\vdots \\\\ - q_m^{(d)} \\\\ q_m^{(1)} \\\\ q_m^{(2)} \\\\ \\vdots \\\\ q_m^{(d/2)} \t\\end{bmatrix}\\\\ \t&amp;=\\begin{bmatrix} \t\tq_m^{(1)}\\cos m\\theta_1 - q_m^{(d/2 + 1)}\\sin m\\theta_1 \\\\ \t\tq_m^{(2)}\\cos m\\theta_2 - q_m^{(d/2 + 2)}\\sin m\\theta_2 \\\\  \t\t\\vdots \\\\ \t\tq_m^{(d/2)}\\cos m\\theta_{d/2} - q_m^{(d)}\\sin m\\theta_{d/2} \\\\ \t\tq_m^{(1)}\\sin m\\theta_1 + q_m^{(d/2+1)}\\cos m\\theta_1  \\\\ \t\tq_m^{(2)}\\sin m\\theta_2 + q_m^{(d/2+2)}\\cos m\\theta_2  \\\\ \t\t\\vdots \\\\ \t\tq_m^{(d/2)}\\sin m\\theta_{d/2} + q_m^{(d)}\\cos m\\theta_{d/2}  \\\\ \t\\end{bmatrix}\\\\ \\end{align}\\]  Implementation: GPT-OSS   modeling_gpt_oss.py: github  def _apply_rotary_emb(     x: torch.Tensor,     cos: torch.Tensor,     sin: torch.Tensor, ) -&gt; torch.Tensor:     first_half, second_half = torch.chunk(x, 2, dim=-1)     first_ = first_half * cos - second_half * sin     second_ = second_half * cos + first_half * sin     return torch.cat((first_, second_), dim=-1)   In GPT-OSS implementation, the core idea is the same as Llama implementation, the size of cos and sin tensors are different.  In Llama, $ \\text{cos}, \\text{sin} \\in \\mathbb{R}^{d} $ while in GPT-OSS, $ \\text{cos}, \\text{sin} \\in \\mathbb{R}^{d/2} $.   References                         Su et al. 2020, RoFormer: Enhanced Transformer with Rotary Position Embedding.                ","categories": [],
        "tags": [],
        "url": "/notes/rope-implementation/",
        "teaser": "/assets/img/rotate-half.png"
      },{
        "title": "ãƒ¡ãƒ¢: torch.nn.functional.kl_div",
        "excerpt":"ãƒ¡ãƒ¢ã®ç›®çš„     ä»¥ä¸‹ã®å•ã„ã¸ã®å›ç­”ã‚’æ›¸ã„ã¦ãŠã            $D_{\\text{KL}}(P||Q)$ ã®ã€Pã¨Qã¯ã©ã£ã¡ãŒäºˆæ¸¬åˆ†å¸ƒã§ã©ã£ã¡ãŒæ­£è§£åˆ†å¸ƒã‹       torch.nn.functional.kl_div ã®å¼•æ•°ã«ã¯ä½•ã‚’æ¸¡ã›ã°ã‚ˆã„ã®ã‹           ä¸€èˆ¬çš„ãªå¼   \\[D_{KL}(P || Q) = \\sum_{i} P(i) \\log\\frac{P(i)}{Q(i)}\\]     a measure of how much an approximating probability distribution Q is different from a true probability distribution P [Wikipedia (2025-12-15)]            (ä»–ã®å‚è€ƒæ›¸ã‚’å‚ç…§ã—ãŸã»ã†ãŒè‰¯ã„ã®ã ãŒã€ã¾ãã€‚)           ã¤ã¾ã‚Šã€$P$ ã‚’çœŸã®åˆ†å¸ƒ (æ­£è§£åˆ†å¸ƒ)ã€$Q$ ã‚’äºˆæ¸¬åˆ†å¸ƒã¨ã™ã‚‹ã€‚   torch.nn.functional.kl_div ã®å¼•æ•°     çµè«–            input ã¯äºˆæ¸¬åˆ†å¸ƒã®å¯¾æ•°ã‚’æ¸¡ã™       target ã¯æ­£è§£åˆ†å¸ƒã‚’æ¸¡ã™           å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ã‚ˆã‚Š (2025-12-15, v2.9.1):            torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False)       input : Tensor of arbitrary shape in log-probabilities       target : Tensor of the same shape as input.       size_average: Deprecated       reduce: Deprecated       reduction: Specifies the reduction to apply to the output. Default: â€˜meanâ€™                    'none': no reduction will be applied           'batchmean': the sum of the output will be divided by batchsize           'sum': the output will be summed           'mean': the output will be divided by the number of elements in the output                           ä¸Šã®çµè«–ã«è‡³ã£ãŸæ¤œè¨¼ã‚³ãƒ¼ãƒ‰  import torch  p = torch.tensor([     [1.0, 0.0, 0.0],     [0.0, 1.0, 0.0], ]) q = torch.tensor([     [0.5, 0.4, 0.1],     [0.1, 0.3, 0.6], ])  no_reduction = torch.tensor([     [-torch.log(torch.tensor(0.5)), 0.0, 0.0],     [0.0, -torch.log(torch.tensor(0.3)), 0.0], ])    å¼•æ•°ã®ç¢ºèªã¨batchmeanã®å‹•ä½œç¢ºèª   def test_kl_div(p, q):     kl_div = torch.nn.functional.kl_div(input = q.log(), target=p, reduction='batchmean')     should_equal = no_reduction.sum(dim=1).mean(dim=0)     assert torch.allclose(kl_div, should_equal), f\"{kl_div} != {should_equal}\" test_kl_div(p, q)   reduction='none' ã®å‹•ä½œç¢ºèª  def test_kl_div_none(p, q):     kl_div = torch.nn.functional.kl_div(input = q.log(), target=p, reduction='none')     assert torch.allclose(kl_div, no_reduction), f\"{kl_div} != {no_reduction}\" test_kl_div_none(p, q)   reduction='sum' ã®å‹•ä½œç¢ºèª  def test_kl_div_sum(p, q):     kl_div = torch.nn.functional.kl_div(input = q.log(), target=p, reduction='sum')     should_equal = no_reduction.sum(dim=1).sum(dim=0)     assert torch.allclose(kl_div, should_equal), f\"{kl_div} != {should_equal}\" test_kl_div_sum(p, q)   reduction='mean'  WARNING: doesnâ€™t return the true kl divergence value, please use reduction = â€˜batchmeanâ€™ which aligns with KL math definition.   def test_kl_div_mean(p, q):     kl_div = torch.nn.functional.kl_div(input = q.log(), target=p, reduction='mean')     should_equal = no_reduction.mean(dim=1).mean(dim=0)     assert torch.allclose(kl_div, should_equal), f\"{kl_div} != {should_equal}\" test_kl_div_mean(p, q)  ","categories": [],
        "tags": [],
        "url": "/notes/kl_div/",
        "teaser": "/assets/img/kl_div.png"
      },{
        "title": "ãƒ¡ãƒ¢: VSCodeã§é–‹ããƒœã‚¿ãƒ³ã‚’Macã®Finderã«è¿½åŠ ã™ã‚‹",
        "excerpt":"   Automatorã‚’é–‹ã   ç”»é¢ä¸Šéƒ¨ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒãƒ¼ã§ã€ŒFileã€-&gt;ã€ŒNewã€ã‚’é¸æŠ   ã€ŒQuick Actionã€ã‚’é¸æŠ   ã€ŒWorkflow receives currentã€ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã§ã€Œfiles or foldersã€ã‚’é¸æŠ   ã€Œinã€ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã§ã€ŒFinder.appã€ã‚’é¸æŠ   å·¦å´ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ã€ŒRun Shell Scriptã€ã‚’ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ã—ï¼Œç”»é¢å³å´ã«è¿½åŠ    ã€ŒPass input:ã€ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã§ã€Œas argumentsã€ã‚’é¸æŠ   ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å…¥åŠ›      for f in \"$@\"  do      open -a \"Visual Studio Code\" \"$f\"  done           ç”»é¢ä¸Šéƒ¨ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒãƒ¼ã§ã€ŒFileã€-&gt;ã€ŒSaveã€ã‚’é¸æŠ   ã€ŒSave As:ã€ã«ã€ŒOpen in VSCodeã€ãªã©ã¨å…¥åŠ›ã—ï¼Œã€ŒSaveã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯   â€» Terminalã‹ã‚‰ãªã‚‰codeã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†ã®ãŒæ—©ã„ï¼  ","categories": [],
        "tags": [],
        "url": "/notes/mac-vscode/",
        "teaser": "/assets/img/open-in-vscode.png"
      },{
        "title": "âœ… Paper accepted to the COLING 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to COLING 2025.                                    Kamoda, G.,           Asai, A.,           Brassard, A.,      &amp;           Sakaguchi, K.  (2025).  Quantifying the Influence of Evaluation Aspects on Long-Form Response Assessment.  In Proceedings of the 31st International Conference on Computational Linguistics (COLING 2025).                                                ACL Anthology     &nbsp;                                    GitHub                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/coling2025-accept/",
        "teaser": null
      },{
        "title": "âœ… Paper accepted to the ICLR 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to ICLR 2025.                               Deguchi, H.,                Kamoda, G.,           Matsushita, Y.,           Taguchi, C.,           Waga, M.,           Suenaga, K.,      &amp;           Yokoi, S.  (2025).  SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches.  In The Thirteenth International Conference on Learning Representations (ICLR 2025).                                                OpenReview     &nbsp;                                    arXiv     &nbsp;                                    Project Page                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/iclr2025-accept/",
        "teaser": null
      },{
        "title": "ğŸ¤ Presentations at NLP 2025",
        "excerpt":"è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š (NLP 2025) ã«ã¦ã€ä»¥ä¸‹ã®6ä»¶ã®ç™ºè¡¨ãŒã‚ã‚Šã¾ã™ã€‚                                  å‡ºå£ç¥¥ä¹‹,                é´¨ç”°è±ª,           æ¾ä¸‹ç¥ä»‹,           ç”°å£æ™ºå¤§,           æœ«æ°¸å¹¸å¹³,           å’Œè³€æ­£æ¨¹,           æ¨ªäº•ç¥¥  (2024).  SoftMatcha: å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹æ¤œç´¢ã®ãŸã‚ã®æŸ”ã‚‰ã‹ãã‚‚é«˜é€Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ£ãƒ¼.  è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š, pp. 3310-3315.                                                 äºˆç¨¿                                       é´¨ç”°è±ª,           Benjamin Heinzerling,           ç¨²è‘‰é”éƒ,           å·¥è—¤æ…§éŸ³,           å‚å£æ…¶ç¥,           ä¹¾å¥å¤ªéƒ  (2025).  è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰æ¢ã‚‹Detokenizationãƒ¡ã‚«ãƒ‹ã‚ºãƒ .  è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š, pp. 634-639.                                                 äºˆç¨¿                                  å°æ—æ˜¥æ–—,           åŸçŸ¥æ­£,                é´¨ç”°è±ª,           æ¨ªäº•ç¥¥  (2025).  å±¤ã®å†—é•·æ€§ã¨å±¤åŒå£«ã®ç‹¬ç«‹æ€§ã«åŸºã¥ãè¨€èªãƒ¢ãƒ‡ãƒ«ã®å±¤äº¤æ›ã®æˆå¦ã®ç‰¹å¾´ã¥ã‘.  è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š, pp. 1751-1756.                                                 äºˆç¨¿                                  å·¥è—¤æ…§éŸ³,                é´¨ç”°è±ª,           å¡©é‡å¤§è¼,           éˆ´æœ¨æ½¤  (2025).  æ—¥æœ¬èªãƒã‚¤ãƒˆç¬¦å·åŒ–ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã¨åˆ†æ.  è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š, pp. 3356-3361.                                                 äºˆç¨¿     &nbsp;                                    ByBERT-JP     &nbsp;                                    ByGPT-JP                                  ä½ã€…æœ¨ç¦å²,           é«˜æ©‹è‰¯å…,                é´¨ç”°è±ª,           Benjamin Heinzerling,           å‚å£æ…¶ç¥,           ä¹¾å¥å¤ªéƒ  (2025).  LM ã¯æ—¥æœ¬ã®æ™‚ç³»åˆ—æ§‹é€ ã‚’ã©ã†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‹.  è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š, pp. 2642-2647.                                                 äºˆç¨¿                                  ä½è—¤å®äº®,                é´¨ç”°è±ª,           Benjamin Heinzerling,           å‚å£æ…¶ç¥  (2025).  è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨è¡¨ç¾ã«ãŠã‘ã‚‹æ–‡æ³•æƒ…å ±ã®å±€æ‰€æ€§ã«ã¤ã„ã¦.  è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š, pp. 697-701.                                                 äºˆç¨¿                      -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/nlp-presentation/",
        "teaser": null
      },{
        "title": "ğŸ‘‘ 2 papers received awards at NLP 2025",
        "excerpt":"è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š (NLP 2025) ã«ã¦ã€ä»¥ä¸‹ã®2ä»¶ã®è«–æ–‡ãŒãã‚Œãã‚Œã€Œè‹¥æ‰‹å¥¨åŠ±è³ã€ã€Œæ—¥æœ¬çµŒæ¸ˆæ–°èç¤¾ CDIOå®¤è³ã€ã‚’å—è³ã—ã¾ã—ãŸã€‚                               å°æ—æ˜¥æ–—,           åŸçŸ¥æ­£,                é´¨ç”°è±ª,           æ¨ªäº•ç¥¥  (2025).  å±¤ã®å†—é•·æ€§ã¨å±¤åŒå£«ã®ç‹¬ç«‹æ€§ã«åŸºã¥ãè¨€èªãƒ¢ãƒ‡ãƒ«ã®å±¤äº¤æ›ã®æˆå¦ã®ç‰¹å¾´ã¥ã‘.  è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š, pp. 1751-1756.                 è‹¥æ‰‹å¥¨åŠ±è³ (20/487)                                                 äºˆç¨¿                                    ä½ã€…æœ¨ç¦å²,           é«˜æ©‹è‰¯å…,                é´¨ç”°è±ª,           Benjamin Heinzerling,           å‚å£æ…¶ç¥,           ä¹¾å¥å¤ªéƒ  (2025).  LM ã¯æ—¥æœ¬ã®æ™‚ç³»åˆ—æ§‹é€ ã‚’ã©ã†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‹.  è¨€èªå‡¦ç†å­¦ä¼š ç¬¬31å›å¹´æ¬¡å¤§ä¼š, pp. 2642-2647.                 æ—¥æœ¬çµŒæ¸ˆæ–°èç¤¾ CDIOå®¤è³                                                 äºˆç¨¿                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/nlp-awards/",
        "teaser": null
      },{
        "title": "ğŸ‘‘ Completed Master's course and received the Dean's Award",
        "excerpt":"æ±åŒ—å¤§å­¦ æƒ…å ±ç§‘å­¦ç ”ç©¶ç§‘ å­¦ä½æˆä¸å¼ã«ã¦ã€å­¦ä½è¨˜ (ä¿®å£«: æƒ…å ±ç§‘å­¦) ã¨ã¨ã‚‚ã«ã€Œç ”ç©¶ç§‘é•·è³ã€ã‚’ã„ãŸã ãã¾ã—ãŸã€‚  ","categories": ["News"],
        "tags": [],
        "url": "/news/dean-award/",
        "teaser": null
      },{
        "title": "ğŸŒ¸ Enrollment in SOKENDAI for Doctoral Program",
        "excerpt":"ç·åˆç ”ç©¶å¤§å­¦é™¢å¤§å­¦ (SOKENDAI) æ—¥æœ¬èªè¨€èªç§‘å­¦ã‚³ãƒ¼ã‚¹ã®åšå£«å¾ŒæœŸèª²ç¨‹ã«é€²å­¦ã—ã¾ã—ãŸã€‚  JST BOOST ã®æ”¯æ´ã‚’å—ã‘ã‚‹SOKENDAIç‰¹åˆ¥ç ”ç©¶å“¡ã¨ã—ã¦ã€å›½ç«‹å›½èªç ”ç©¶æ‰€ (NINJAL) ã§ç ”ç©¶æ´»å‹•ã‚’è¡Œã„ã¾ã™ã€‚  ","categories": ["News"],
        "tags": [],
        "url": "/news/sokendai-enrollment/",
        "teaser": null
      },{
        "title": "âœ… Paper accepted to the Findings of EMNLP 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to the Findings of EMNLP 2025.                               Inaba, T.,                Kamoda, G.,           Inui, K.,           Isonuma, M.,           Miyao, Y.,           Oseki, Y.,           Takagi, Y.,      &amp;           Heinzerling, B.  (2025).  How a Bilingual LM Becomes Bilingual: Tracing Internal Representations with Sparse Autoencoders.  In Findings of the Association for Computational Linguistics: EMNLP 2025.                                                ACL Anthology     &nbsp;                                    arXiv                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/emnlp2025findings-accept/",
        "teaser": null
      },{
        "title": "âœ… Paper accepted to the 8th BlackboxNLP Workshop",
        "excerpt":"We are pleased to announce that our paper has been accepted to the 8th BlackboxNLP Workshop.                               Takahashi, R.,                Kamoda, G.,           Heinzerling, B.,           Sakaguchi, K.,      &amp;           Inui, K.  (2025).  Understanding the Side Effects of Rank-One Knowledge Editing.  In BlackboxNLP 2025: The 8th Workshop on Analyzing and Interpreting Neural Networks for NLP.                                                ACL Anthology     &nbsp;                                    arXiv                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/bbnlp2025-accept/",
        "teaser": null
      },{
        "title": "ğŸ¤ Presentation at YANS 2025",
        "excerpt":"YANS 2025 ã«ã¦ä»¥ä¸‹ã®ç™ºè¡¨ãŒã‚ã‚Šã¾ã™ã€‚                                    é´¨ç”°è±ª,           ç†Šè°·é›„ä»‹,           æ¾äº•å­å¤ª,           æ¨ªäº•ç¥¥  (2025).  å¯†åº¦æ¯”ã®ç›´æ¥æ¨å®šã«åŸºã¥ãè¨€èªãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›è¼ƒæ­£.  ç¬¬20å›è¨€èªå‡¦ç†è‹¥æ‰‹ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (YANS).                   -&gt; Publications  ","categories": ["Blog"],
        "tags": [],
        "url": "/blog/yans2025-presentation/",
        "teaser": null
      },{
        "title": "âœ… Paper accepted to the IJCNLP-AACL 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to the 8th BlackboxNLP Workshop.                               Sasaki, M.,                Kamoda, G.,           Takahashi, R.,           Sato, K.,           Heinzerling, B.,           Sakaguchi, K.,      &amp;           Inui, K.  (2025).  Can Language Models Handle a Non-Gregorian Calendar? The Case of the Japanese wareki.  In Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2025).                                                arXiv                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/ijcnlpaacl2025-accept/",
        "teaser": null
      },{
        "title": "Test-time Augmentation for Factual Probing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202303-nlp-kamoda",
        "teaser": null
      },{
        "title": "é•·æ–‡ç”Ÿæˆã®å¤šé¢çš„è©•ä¾¡:äººæ‰‹è©•ä¾¡ã¨è‡ªå‹•è©•ä¾¡ã®å‘ä¸Šã‚’ç›®æŒ‡ã—ã¦",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202403-nlp-kamoda",
        "teaser": null
      },{
        "title": "è¨€èªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®çŸ¥è­˜å‰Šé™¤ï¼šé »å‡ºå®Ÿä½“ã®çŸ¥è­˜ã¯å‰¯ä½œç”¨ãŒç ´æ»…çš„",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202403-nlp-takahashi",
        "teaser": null
      },{
        "title": "å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±æ¨è–¦ãƒã‚¤ã‚¢ã‚¹ã®è¼ƒæ­£",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202405-jsai-kumagae",
        "teaser": null
      },{
        "title": "æŸ”ã‚‰ã‹ã„grep/KWICã«å‘ã‘ã¦ï¼šé«˜é€Ÿå˜èªåˆ—ãƒãƒƒãƒãƒ³ã‚°ã®åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã«ã‚ˆã‚‹é€£ç¶šåŒ–",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202409-yans-deguchi",
        "teaser": null
      },{
        "title": "äº‹å‰å­¦ç¿’â€“æ–‡è„ˆå†…å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ç”Ÿã˜ã‚‹é »åº¦ãƒã‚¤ã‚¢ã‚¹ã®è¼ƒæ­£",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202409-yans-ito",
        "teaser": null
      },{
        "title": "å±¤åŒå£«ã®æ¥ç¶šå¯èƒ½æ€§ã¨å„å±¤ãŒå½±éŸ¿ã‚’ä¸ãˆã‚‹éƒ¨åˆ†ç©ºé–“ã®é‡ãªã‚Šåº¦åˆã„ã®é–¢ä¿‚æ€§",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202409-yans-kobayashi",
        "teaser": null
      },{
        "title": "SoftMatcha: å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹æ¤œç´¢ã®ãŸã‚ã®æŸ”ã‚‰ã‹ãã‚‚é«˜é€Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ£ãƒ¼",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-deguchi",
        "teaser": null
      },{
        "title": "è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰æ¢ã‚‹Detokenizationãƒ¡ã‚«ãƒ‹ã‚ºãƒ ",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-kamoda",
        "teaser": null
      },{
        "title": "å±¤ã®å†—é•·æ€§ã¨å±¤åŒå£«ã®ç‹¬ç«‹æ€§ã«åŸºã¥ãè¨€èªãƒ¢ãƒ‡ãƒ«ã®å±¤äº¤æ›ã®æˆå¦ã®ç‰¹å¾´ã¥ã‘",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-kobayashi",
        "teaser": null
      },{
        "title": "æ—¥æœ¬èªãƒã‚¤ãƒˆç¬¦å·åŒ–ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã¨åˆ†æ",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-kudo",
        "teaser": null
      },{
        "title": "LM ã¯æ—¥æœ¬ã®æ™‚ç³»åˆ—æ§‹é€ ã‚’ã©ã†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‹",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-sasaki",
        "teaser": null
      },{
        "title": "è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨è¡¨ç¾ã«ãŠã‘ã‚‹æ–‡æ³•æƒ…å ±ã®å±€æ‰€æ€§ã«ã¤ã„ã¦",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-satoh",
        "teaser": null
      },{
        "title": "å¯†åº¦æ¯”ã®ç›´æ¥æ¨å®šã«åŸºã¥ãè¨€èªãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›è¼ƒæ­£",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202509-yans-kamoda",
        "teaser": null
      },{
        "title": "Test-time Augmentation for Factual Probing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202311-emnlp-kamoda",
        "teaser": null
      },{
        "title": "Quantifying the Influence of Evaluation Aspects on Long-Form Response Assessment",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202501-coling-kamoda",
        "teaser": null
      },{
        "title": "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-iclr-deguchi",
        "teaser": null
      },{
        "title": "Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-naacl-kamoda",
        "teaser": null
      },{
        "title": "Understanding the Side Effects of Rank-One Knowledge Editing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202511-blackboxnlp-takahashi",
        "teaser": null
      },{
        "title": "How a Bilingual LM Becomes Bilingual: Tracing Internal Representations with Sparse Autoencoders",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202511-emnlp-inaba",
        "teaser": null
      },{
        "title": "Can Language Models Handle a Non-Gregorian Calendar? The Case of the Japanese wareki",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202512-aacl-sasaki",
        "teaser": null
      },,{
    "title": "Notes",
    "excerpt":"","url": "https://gokamoda.github.io/notes/"
  },{
    "title": "Publications & Activities",
    "excerpt":"## Education - Doctoral Student [2025.04 - ] Graduate Institute for Advanced Studies, SOKENDAI.Supervisor: Assoc. Prof. Sho Yokoi - Master of Information Science [2023.04 - 2025.03]Graduate School of Information Sciences, Tohoku University.Supervisor: Prof. Jun Suzuki & Assoc. Prof. Keisuke Sakaguchi Dean Award (4/126) - Bachelor of Engineering [2020.04 - 2023.03]School of Engineering, Tohoku University.Supervisor: Prof. Kentaro Inui & Assoc. Prof. Keisuke SakaguchiEarly Graduation (1/252)  ## International Conferences          {% for publication in site.pubInternationalConferences reversed %}              {% include pub-apa-international-conf.html  %}            {% endfor %}           ## Domestic Conferences          {% for publication in site.pubDomesticConferences reversed %}              {% include pub-anlp-domestic-conf.html  %}            {% endfor %}        {% if site.pubPreprint.size > 0 %} ## Preprints         {% for publication in site.pubPreprint reversed %}              {% include pub-apa-international-conf.html  %}            {% endfor %}      {% endif %}  {% if site.experiences.size > 0 %} ## Experiences         {% for experience in site.experiences reversed %}              {% include experiences.html  %}            {% endfor %}      {% endif %}           {% for talk in site.oubInvitedTalks reversed %}                       {% for speaker in talk.speakers %}           {{ speaker.name }}           {%- if forloop.last == false -%}             ,           {% endif %}         {%- endfor -%}         .         {{ talk.title }}.         {{ talk.event_name }},         {{ talk.month }}         {{ talk.year }}.         {% if talk.links %}           [           {%- for link in talk.links -%}             {{ link.name}}{% if forloop.last == false %}, {% endif %}           {%- endfor -%}           ]         {% endif %}            {% endfor %}         ","url": "https://gokamoda.github.io/cv/"
  },{
    "title": "",
    "excerpt":"","url": "https://gokamoda.github.io/index.html"
  },{
    "title": " - page 2",
    "excerpt":"","url": "https://gokamoda.github.io/page/2/index.html"
  }]
