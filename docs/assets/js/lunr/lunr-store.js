var store = [{
        "title": "Infratop (DMM WebCamp)",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202103-infratop",
        "teaser": null
      },{
        "title": "AIçŽ‹ Committee Member",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202305-aio",
        "teaser": null
      },{
        "title": "NS Solutions R&D Internship",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202309-nssol",
        "teaser": null
      },{
        "title": "AKATSUKI-SICA",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202310-akatsuki-sica",
        "teaser": null
      },{
        "title": "Hakuhodo DY Holdings Inc. Joint Research",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202310-hakuhodo",
        "teaser": null
      },{
        "title": "Visiting Student at NLP Department, MBZUAI",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202402-mbzuai",
        "teaser": null
      },{
        "title": "Tohoku University GP-DS Research Assistant",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202404-tu-gpds",
        "teaser": null
      },{
        "title": "NINJAL Part-time Researcher",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-ninjal-part-time-researcher",
        "teaser": null
      },{
        "title": "SOKENDAI Special Researcher Program",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-sokenda-srp",
        "teaser": null
      },{
        "title": "RMSNorm and LayerNorm",
        "excerpt":"Preparation   Let $\\mu(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns element-wise mean of a row-vector $\\bm{x} \\in \\mathbb{R}^{d}$:   \\[\\begin{align} \\mu(\\bm{x})  &amp;= \\frac{1}{d}\\sum_{i=1}^d \\bm{x}_i\\\\ &amp;=\\frac{1}{d}\\bm{x}\\cdot \\bm{1}\\\\ &amp;=\\frac{1}{d}\\bm{x}\\bm{1}^\\top \\end{align}\\]  Let $c(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}^{d}$ be centering function, that subtracts the element-wise mean from each element of $\\bm{x}$:   \\[\\begin{aligned} \tc(\\bm{x})&amp;=\\bm{x} - \\mu(\\bm{x})\\bm{1}\\\\ \t&amp;= \\bm{x} - \\frac{1}{d}\\bm{x}\\bm{1}^\\top\\bm{1}\\\\ \t&amp;= \\bm{x} \\left(1 - \\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right) \\end{aligned}\\]  By the way, (I - \\frac{1}{d}\\bm{1}^\\top\\bm{1}) is called the  centering matrix.   Let $\\text{RMS}(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns the element-wise RMS (root mean square):   \\[\\begin{align} \t\\text{RMS}(\\bm{x})&amp;=\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2}\\\\ \t&amp;=\\frac{\\sqrt{\\sum_{i=1}^d x_i^2}}{\\sqrt{d}}\\\\ \t&amp;=\\frac{||\\bm{x}||_2}{\\sqrt{d}} \\end{align}\\]  Let $\\text{MS}(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns the squared RMS (root mean square):   \\[\\begin{align} \t\\text{MS}(\\bm{x})&amp;=\\text{RMS}(\\bm{x})^2\\\\ \t&amp;=\\frac{||\\bm{x}||_2^2}{d}\\\\ \t&amp;=\\frac{1}{d}\\sum_{i=1}^d x_i^2\\\\ \\end{align}\\]  Let $\\text{Var}(\\bm{x}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ be a function that returns element-wise variance:   \\[\\begin{align} \\text{Var}(\\bm{x}) &amp;= \\frac{1}{d}\\sum_{i=1}^d (x_i - \\mu(\\bm{x}))^2\\\\ &amp;=\\frac{1}{d}\\sum_{i=1}^d (\\bm{x} - \\mu(\\bm{x})\\bm{1})^2_i\\\\ &amp;=\\frac{1}{d}\\sum_{i=1}^d c(\\bm{x})_i^2\\\\ &amp;=\\text{MS}(c(\\bm{x})) \\end{align}\\]  RMSNorm  PyTorch: RMSNorm   \\[\\text{RMSNorm}(\\bm{x}) = \\frac{\\bm{x}}{\\sqrt{\\text{MS}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma}\\]  Here, $\\odot$ is element-wise multiplication, $\\bm{\\gamma}\\in \\mathbb{R}^d$ is a learnable weight vector, and $\\varepsilon$ is a small constant for numerical stability.   LayerNorm  PyTorch: LayerNorm   In the original form:   \\[\\text{LayerNorm}(\\bm{x}) = \\frac{\\bm{x} - \\mu(\\bm{x})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma} +\\bm{\\beta}\\]  This can be rewritten using the centering function $c(\\bm{x})$ and the MS function $\\text{MS}(\\bm{x})$ as follows:   \\[\\begin{aligned} \\text{LayerNorm}(\\bm{x}) &amp;= \\frac{c(\\bm{x})}{\\sqrt{\\text{MS}(c(\\bm{x}))+\\varepsilon}}\\odot \\bm{\\gamma} + \\bm{\\beta}\\\\ \\end{aligned}\\]  Thus, the following holds: LayerNorm is equal to \"centering\" â†’ RMSNorm â†’ \"add bias\"   \\[\\text{LayerNorm}(\\bm{x}) = \\text{RMSNorm}(c(\\bm{x})) + \\bm{\\beta}\\]  Also, element-wise multiplication of $\\bm{\\gamma}$ can be expressed as matrix multiplication of $\\text{diag}(\\bm{\\gamma})$. Therefore, LayerNorm can be rewritten as:   \\[\\begin{align} \\text{LayerNorm}(\\bm{x}) &amp;= \\frac{1}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\left(\\bm{x} - \\mu(\\bm{x})\\bm{1}\\right)\\odot \\bm{\\gamma} + \\bm{\\beta}\\\\ &amp;= \\frac{\\bm{x}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\left(I - \\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\text{diag}(\\bm{\\gamma}) + \\bm{\\beta}\\\\ \\end{align}\\]  Thus only non-linear operation in LayerNorm is the division by $\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}$.  ","categories": [],
        "tags": [],
        "url": "/notes/layernorm/",
        "teaser": "/assets/img/layernorm_rmsnorm.png"
      },{
        "title": "LogitLens without bias",
        "excerpt":"   LogitLens[1] applies $\\text{LMHead}$ to the internal representations $(\\bm{h} \\in \\mathbb{R}^{1\\times d})$ of a transformer model.   \\[\\begin{equation} \\text{LMHead}(\\bm{h}) = \\text{LN}_\\text{f}(\\bm{h})\\bm{E}^O \\label{eq:lm_head} \\end{equation}\\]  Here, $\\bm{E}^O \\in \\mathbb{R}^{d\\times |\\mathcal{V}|}$ is the unembedding matrix and $\\text{LN}_\\text{f}$ is the final layer normalization of a transformer model. In this page, we assume LayerNorm (not RMSNorm) is used for $\\text{LN}_\\text{f}$ , which is defined as follows.   \\[\\begin{equation} \\text{LN}_\\text{f}(\\bm{h}) = \\frac{\\bm{x} - \\mu(\\bm{x})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma} +\\bm{\\beta} \\label{eq:lm_f} \\end{equation}\\]  Here, $\\mu(\\bm{h}): \\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ is a function that returns element-wise mean of a row-vector $\\bm{h}$ and $\\bm{\\gamma}, \\bm{\\beta} \\in \\mathbb{R}^{d}$ are learnable parameters. $\\odot$ represents element-wise multiplication.   With LogitLens, one can project the hidden states after each transformer layers to the vocabulary space.             Example of LogitLens.   By combining Equation\\eqref{eq:lm_head} and \\eqref{eq:lm_f}, we get a bias term for the projection to vocabulary space.   \\[\\begin{align} \\text{LogitLens}(\\bm{h}) &amp;= \\left(\\frac{\\bm{h} - \\mu(\\bm{h})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma} + \\bm{\\beta}\\right)\\bm{E}^O\\\\ &amp;= \\left(\\frac{\\bm{h} - \\mu(\\bm{h})\\bm{1}}{\\sqrt{\\text{Var}(\\bm{x})+\\varepsilon}}\\odot \\bm{\\gamma}\\right)\\bm{E}^O + \\bm{\\beta}\\bm{E}^O \\label{eq:lm_head_bias} \\end{align}\\]  The second term in Equation\\eqref{eq:lm_head_bias}, which is $\\bm{\\beta}\\bm{E}^O \\in \\mathbb{R}^{|\\mathcal{V}|}$ is the bias term, which is added to the result of LogitLens regardless of the input. Adding such bias may not reasonable when analyzing â€œwhat the modelâ€™s intermediate states representâ€ as Kobayashi et al. (2023) reports that word frequency in the training corpus is encoded in this bias term of $\\text{LN}_\\text{f}$ in GPT-2 model.   By removing the bias term, we get the following result.             Vanilla LogitLens (GPT-2)            LogitLens w/o Bias (GPT-2)             Vanilla LogitLens (OPT)            LogitLens w/o Bias (OPT)   References                         Nostalgibraist 2020, interpreting GPT: the logit lens.                            Kobayashi et al. 2023, Transformer Language Models Handle Word Frequency in Prediction         Head.       ","categories": [],
        "tags": [],
        "url": "/notes/logitlens-wob/",
        "teaser": "/assets/img/logit_lens_nobias_gpt2.png"
      },{
        "title": "Folding weights in transformers",
        "excerpt":"This reformulation of LayerNorm and Self-Attention is used in our paper:                                   Kamoda, G.,           Heinzerling, B.,           Inaba, T.,           Kudo, K.,           Sakaguchi, K.,      &amp;           Inui, K.  (2025).  Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference.  In Findings of the Association for Computational Linguistics: NAACL 2025.                                                ACL Anthology     &nbsp;                                    arXiv     &nbsp;                                    GitHub                 Notation   \\[\\begin{alignat}{4}     &amp;\\bm{X} &amp;:= &amp;     \\begin{bmatrix}         \\bm{x}_1\\\\         \\vdots\\\\         \\bm{x}_n     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{n \\times d}\\\\     &amp;\\bm{W}^O &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^O_1\\\\         \\vdots\\\\         \\bm{W}^O_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}\\\\     &amp;\\bm{W}^Q &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^Q_1 &amp; \\cdots &amp; \\bm{W}^Q_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp; \\label{eq:wq_split}\\\\     &amp;\\bm{W}^K &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^K_1 &amp; \\cdots &amp; \\bm{W}^K_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp; \\label{eq:wk_split}\\\\     &amp;\\bm{W}^V &amp;:= &amp;     \\begin{bmatrix}         \\bm{W}^V_1 &amp; \\cdots &amp; \\bm{W}^V_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d \\times d}&amp;\\label{eq:wv_split}\\\\     &amp;\\bm{b}^Q &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^Q_1 &amp; \\cdots &amp; \\bm{b}^Q_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d}&amp; \\label{eq:bq_split}\\\\     &amp;\\bm{b}^K &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^K_1 &amp; \\cdots &amp; \\bm{b}^K_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in&amp; \\mathbb{R}^{d}&amp; \\label{eq:bk_split}\\\\     &amp;\\bm{b}^V &amp;:= &amp;     \\begin{bmatrix}         \\bm{b}^V_1 &amp; \\cdots &amp; \\bm{b}^V_H     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d}&amp; \\\\     &amp;\\bm{I} &amp;:= &amp;     \\begin{bmatrix}         1 &amp; 0 &amp; \\cdots &amp; 0 \\\\         0 &amp; 1 &amp; \\cdots &amp; 0 \\\\         \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\         0 &amp; 0 &amp; \\cdots &amp; 1 \\\\     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d\\times d}&amp; \\\\     &amp;\\bm{1} &amp;:= &amp;     \\begin{bmatrix}         1 &amp; \\cdots &amp; 1     \\end{bmatrix}     &amp;\\hspace{1em}\\in &amp;\\mathbb{R}^{d} \\end{alignat}\\]  Original LayerNorm   Layer Normalization can be expressed as follows (org stands for original):   \\[\\begin{alignat}{3}     &amp;\\text{LN}(\\bm{x}) &amp;:=&amp;\\ \\frac{\\bm{x}-\\mu(x)\\bm{1}}{\\sigma(\\bm{x})}\\odot\\bm{\\gamma} + \\bm{\\beta}&amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     &amp;\\bm{x} &amp;:=&amp;\\      \\begin{bmatrix}         x^{(1)} &amp; \\cdots &amp; x^{(d)}     \\end{bmatrix}     &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     &amp;\\mu(\\bm{x}) &amp;:=&amp;\\ \\frac{1}{d}\\sum_kx^{(k)}&amp;\\hspace{1em}\\in&amp;\\mathbb{R}\\\\     &amp;\\sigma(\\bm{x}) &amp;:=&amp;\\ \\sqrt{\\frac{1}{d}\\sum_k^d\\left(x^{(k)}-\\mu(\\bm{x})\\right)^2+\\epsilon}&amp;\\hspace{1em}\\in&amp;\\mathbb{R} \\end{alignat}\\]  Now, $\\mu(\\bm{x})$ can be reformulated as follows:   \\[\\begin{align}     \\mu(\\bm{x})\\bm{1}     &amp;=\\frac{1}{d}\\left(\\sum_kx^{(k)}\\right)\\bm{1}\\\\     &amp;=\\frac{1}{d}\\left(\\bm{x}\\bm{1}^\\top\\right)\\bm{1}\\\\     &amp;=\\bm{x}\\left(\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right) \\end{align}\\]  Thus $\\text{LN}_{\\text{org}}$ can be reformulated as follows.   \\[\\begin{align}     \\text{LN}(\\bm{x})      &amp;= \\frac{\\bm{x}-\\mu(\\bm{x})\\bm{1}}{\\sigma(\\bm{x})}\\odot\\bm{\\gamma} + \\bm{\\beta}\\\\     &amp;= \\frac{1}{\\sigma(\\bm{x})} \\left(\\bm{x}-\\bm{x}\\left(\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\right) \\diag{\\bm\\gamma}+ \\bm{\\beta}\\\\     &amp;= \\frac{\\bm{x}}{\\sigma(\\bm{x})}\\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm\\gamma}+ \\bm{\\beta} \\end{align}\\]  Original Self-Attention   Let query, key, value transformations of each head $h$ be expressed as follows:   \\[\\begin{align}     \\bm{q}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^Q + \\bm{b}_h^Q\\\\     \\bm{k}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^K + \\bm{b}_h^K\\\\     \\bm{v}_h(\\bm{x}) &amp;:= \\bm{x}\\bm{W}_h^V + \\bm{b}_h^V\\\\ \\end{align}\\]  Let attention weight from token position $i$ to $j$  ($i \\ge j$) in head $h$ be expressed as follows:   \\[\\alpha_{i, j, h} = \\underset{\\bm{x}_j,\\bm{x}_j \\in \\bm{X}, j \\leq i}{\\text{softmax}}\\frac{\\bm{q}_h(\\bm{x}_i)\\bm{k}_h(\\bm{x}_j)^\\top}{\\sqrt{d'}}\\]  where $dâ€™ = d/H$ is the dimension of each head.   The output of Attention layer of an causal model at position $i$ can be expressed as follows:   \\[\\begin{align}     \\text{ATTN}(i, \\bm{X})         &amp;:=\\left[\\text{head}_1(i, \\bm{X})\\hspace{0.5em}\\cdots\\hspace{0.5em}\\text{head}_H(i, \\bm{X})\\right]             \\bm{W}^O + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\text{head}_h(i, \\bm{X})\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\bm{v}_h(\\bm{x}_j)\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;=\\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h} \\left(\\bm{x}_j\\bm{W}^V_h + \\bm{b}^V_h\\right)\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H \\left(\\sum_{j=1}^i \\alpha_{i, j, h}\\bm{b}^V_h\\right)\\bm{W}^O_h + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\sum_{h=1}^H\\bm{b}^V_h\\bm{W}^O_h + \\bm{b}^O\\hspace{0.5em} \\left(\\because \\sum_j \\alpha_{i, j, h} = 1\\right)\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h}\\bm{x}_j\\bm{W}^V_h\\bm{W}^O_h + \\bm{b}^V\\bm{W}^O + \\bm{b}^O\\\\         &amp;= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h} \\bm{x}_j\\bm{W}^{VO}_h + \\bm{b}^{VO} \\end{align}\\]  where   \\[\\begin{align}     \\bm{W}^{VO}_h &amp;:= \\bm{W}^V_h\\bm{W}^O_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\bm{b}^{VO} &amp;:= \\bm{b}^V\\bm{W}^O + \\bm{b}^O &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  Reformulating LayerNorm and Self-Attention   LayerNorm is always followed by a linear transformation in transformers. Thus, we can fold the weights of LayerNorm into the weights of the following linear transformation.   For example, in the case of LayerNorm followed by query transformation, we can fold the weights as follows:   \\[\\begin{align}     \\bm{q}_h(\\text{LN}(\\bm{x}))         &amp;= \\text{LN}(\\bm{x})\\bm{W}^Q_h + \\bm{b}^Q_h\\\\         &amp;= \\left(\\frac{\\bm{x}}{\\sigma(\\bm{x})}\\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}+ \\bm{\\beta}\\right)\\bm{W}^Q_h + \\bm{b}^Q_h\\\\         &amp;= \\frac{\\bm{x}}{\\sigma(\\bm{x})}\\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^Q_h + \\bm{\\beta}\\bm{W}^Q_h + \\bm{b}^Q_h\\\\         &amp;= \\overset{\\text{new}}{\\text{LN}}(\\bm{x})\\overset{\\text{new}}{\\bm{W}^Q_h} + \\overset{\\text{new}}{\\bm{b}^Q_h} \\end{align}\\]  where   \\[\\begin{align}     \\overset{\\text{new}}{\\text{LN}}(\\bm{x}) &amp;:= \\frac{\\bm{x}}{\\sigma(\\bm{x})} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\bm{W}^Q_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^Q_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{b}^Q_h} &amp;:= \\bm{\\beta}\\bm{W}^Q_h + \\bm{b}^{\\{Q, K\\}}_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  The same can be done for key and value transformations, and thus LayerNorm followed by self-attention can be reformulated as follows:   \\[\\begin{align}     \\overset{\\text{new}}{\\text{LN}}(\\bm{x}) &amp;:= \\frac{\\bm{x}}{\\sigma(\\bm{x})} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\text{ATTN}}(i, \\bm{X})         &amp;:= \\sum_{h=1}^H \\sum_{j=1}^i \\alpha_{i, j, h} \\bm{x}_j\\overset{\\text{new}}{\\bm{W}^{VO}_h} + \\overset{\\text{new}}{\\bm{b}^{VO}} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\ \\end{align}\\]  where   \\[\\begin{align}     \\overset{\\text{new}}{\\alpha_{i, j, h}} &amp;:= \\underset{\\bm{x}_j,\\bm{x}_j \\in \\bm{X}, j \\leq i}{\\text{softmax}}\\frac{\\overset{\\text{new}}{\\bm{q}_h}(\\bm{x}_i)\\overset{\\text{new}}{\\bm{k}_h}(\\bm{x}_j)^\\top}{\\sqrt{d'}} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}\\\\     \\overset{\\text{new}}{\\bm{q}_h}(\\bm{x}) &amp;:= \\bm{x}\\overset{\\text{new}}{\\bm{W}^Q_h} + \\overset{\\text{new}}{\\bm{b}^Q_h} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\bm{k}_h}(\\bm{x}) &amp;:= \\bm{x}\\overset{\\text{new}}{\\bm{W}^K_h} + \\overset{\\text{new}}{\\bm{b}^K_h} &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^d\\\\     \\overset{\\text{new}}{\\bm{W}^Q_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^Q_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{W}^K_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^K_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{b}^Q_h} &amp;:= \\bm{\\beta}\\bm{W}^Q_h + \\bm{b}^Q_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\     \\overset{\\text{new}}{\\bm{b}^K_h} &amp;:= \\bm{\\beta}\\bm{W}^K_h + \\bm{b}^K_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  and   \\[\\begin{align}     \\overset{\\text{new}}{\\bm{W}^{VO}_h} &amp;:= \\left(\\bm{I}-\\frac{1}{d}\\bm{1}^\\top\\bm{1}\\right)\\diag{\\bm{\\gamma}}\\bm{W}^V_h\\bm{W}^O_h &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d \\times d}\\\\     \\overset{\\text{new}}{\\bm{b}^{VO}} &amp;:= \\bm{\\beta}\\bm{W}^V\\bm{W}^O + \\bm{b}^V\\bm{W}^O + \\bm{b}^O &amp;\\hspace{1em}\\in&amp;\\mathbb{R}^{d}\\\\ \\end{align}\\]  ","categories": [],
        "tags": [],
        "url": "/notes/fold-weights/",
        "teaser": "/assets/img/folding-weights.png"
      },{
        "title": "ãƒ¡ãƒ¢: PCA",
        "excerpt":"$n$ å€‹ã® $d$ æ¬¡å…ƒã®ãƒ‡ãƒ¼ã‚¿ $\\bm{X}$ ã‚’ã€$dâ€™$ æ¬¡å…ƒã«åœ§ç¸®ã™ã‚‹ãŸã‚ã®ç·šå½¢å¤‰æ›è¡Œåˆ— $\\bm{W}$ ã‚’æ±‚ã‚ã‚‹ã€‚   \\[\\bm{X} =\\begin{bmatrix} \\bm{x}_1 \\\\ \\bm{x}_2 \\\\ \\vdots \\\\ \\bm{x}_n \\end{bmatrix} = \\begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,d} \\\\ x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,d} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,d} \\end{bmatrix} \\in \\mathbb{R}^{n \\times d}\\]  \\[\\bm{W} = \\begin{bmatrix} \\bm{w}_1 &amp; \\bm{w}_2 &amp; \\cdots &amp; \\bm{w}_{d'} \\\\ \\end{bmatrix} = \\begin{bmatrix} w_{1,1} &amp; w_{1,2} &amp; \\cdots &amp; w_{1,d'} \\\\ w_{2,1} &amp; w_{2,2} &amp; \\cdots &amp; w_{2,d'} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_{d,1} &amp; w_{d,2} &amp; \\cdots &amp; w_{d,d'} \\end{bmatrix} \\in \\mathbb{R}^{d \\times d'}\\]  å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿ $\\bm{Y}$ ã¯æ¬¡ã®ã‚ˆã†ã«è¡¨ã•ã‚Œã‚‹ã€‚ \\(\\bm{Y} = \\bm{X} \\bm{W} \\in \\mathbb{R}^{n \\times d'}\\)   PCAã§ã¯ã€åœ§ç¸®å¾Œã®å„æ¬¡å…ƒã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã€‚  åœ§ç¸®å¾Œã® $j\\ (1\\le j \\le dâ€™)$æ¬¡å…ƒç›® ã®åˆ†æ•£ã¯ã€å®šç¾©ã‚ˆã‚Šä»¥ä¸‹ã§è¡¨ã›ã‚‹:   \\[\\begin{align} s_j^2  &amp;= \\frac{1}{n}\\sum_{i=1}^{n}(y_{ij}-\\bar{y_j})^2\\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\left(     \\bm{x}_i \\bm{w}_j - \\frac{1}{n}\\sum_{k=1}^{n}\\bm{x}_k \\bm{w}_j \\right)^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\left(     \\left(\\bm{x}_i - \\frac{1}{n}\\sum_{k=1}^{n}\\bm{x}_k\\right) \\bm{w}_j \\right)^2 \\\\ \\end{align}\\]  ãŸã ã— $\\bar{y_j} = \\frac{1}{n}\\sum_{i=1}^{n}y_{ij}$ ã¯ $j$ æ¬¡å…ƒç›®ã®å¹³å‡å€¤ã‚’è¡¨ã™ã€‚   ã“ã“ã§ã€$\\bar{\\bm{x}} = \\frac{1}{n}\\sum_{i=1}^{n}\\bm{x}_i \\in \\mathbb{R}^{d}$ ã¨ã—ã€ ä¸­å¿ƒåŒ– (å„æ¬¡å…ƒã®å¹³å‡ã‚’ã‚¼ãƒ­ã«) ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ $\\bm{X}^c$ ã¨ã™ã‚‹ã€‚ $\\bm{X}^c$ ã¯ä»¥ä¸‹ã§å®šç¾©ã•ã‚Œã‚‹ã€‚   \\[\\begin{align} \\bm{X}^c &amp;= \\begin{bmatrix} \\bm{x}_1 \\\\ \\bm{x}_2 \\\\ \\vdots \\\\ \\bm{x}_n \\end{bmatrix} -  \\begin{bmatrix} \\bar{\\bm{x}} \\\\ \\bar{\\bm{x}} \\\\ \\vdots \\\\ \\bar{\\bm{x}} \\\\ \\end{bmatrix} =\\begin{bmatrix} \\bm{x}_1^c \\\\ \\bm{x}_2^c \\\\ \\vdots \\\\ \\bm{x}_n^c \\end{bmatrix} \\end{align}\\]  \\[\\begin{align} \\bm{x}^c_i &amp;=\\bm{x}_i - \\bar{\\bm{x}}\\\\ &amp;=\\bm{x}_i - \\frac{1}{n}\\sum_{k=1}^{n}\\bm{x}_k \\end{align}\\]  ã™ã‚‹ã¨ã€æ¬¡å…ƒ $j\\ (1\\le j \\le dâ€™)$ ã®åˆ†æ•£ã¯ä»¥ä¸‹ã§è¡¨ã›ã‚‹:   \\[\\begin{align} s_j^2  &amp;= \\frac{1}{n}\\sum_{i=1}^{n}(\\bm{x}_i^c \\bm{w}_j)^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}(\\bm{x}_i\\bm{w}_j^c)^\\top (\\bm{x}_i^c \\bm{w}_j) \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\bm{w}_j^\\top(\\bm{x}_i^c\\top\\bm{x}_i^c)\\bm{w}_j \\\\ &amp;= \\frac{1}{n}\\bm{w}_j^\\top \\left(\\sum_{i=1}^{n}\\bm{x}_i^c\\top\\bm{x}_i^c\\right) \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\left(\\frac{1}{n}\\bm{X}^{c\\top}\\bm{X}^c\\right) \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\end{align}\\]  ã“ã“ã§ã€$\\bm{S} \\in \\mathbb{R}^{d \\times d}$ ã¯ãƒ‡ãƒ¼ã‚¿$\\bm{X}$ã®ã€å…±åˆ†æ•£è¡Œåˆ—ã¨å‘¼ã°ã‚Œã‚‹ã€‚   PCAã§ã¯ã€$s_j^2 (1\\le j \\le dâ€™)$ã‚’æœ€å¤§åŒ–ã™ã‚‹ $\\bm{w}_j$ ã‚’æ±‚ã‚ã‚‹ã€‚ ãªãŠã€ã“ã“ã§$\\bm{w}_j$ã‚’å¤§ããã™ã‚Œã°$s_j^2$ã‚‚å¤§ãããªã‚‹ãŸã‚ã€$\\bm{w}_j$ã¯å˜ä½ãƒ™ã‚¯ãƒˆãƒ«ã€ã¤ã¾ã‚Š \\(\\|\\bm{w}_j\\|^2 = \\bm{w}_j^\\top \\bm{w}_j = 1\\) ã®æˆç´„ã‚’è¨­ã‘ã‚‹ã€‚   çµæžœã€è§£ããŸã„å•é¡Œã¯   \\[\\underset{\\bm{w}_j}{\\text{argmax}}\\quad \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\quad \\text{subject to}\\quad  \\bm{w}_j^\\top \\bm{w}_j = 1\\]  ã“ã‚Œã‚’ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã‚’ç”¨ã„ã¦è§£ãã“ã¨ã‚’è€ƒãˆã‚‹ã€‚ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥é–¢æ•°ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹ã€‚   \\[\\begin{align} \\mathcal{L}(\\bm{w}_j, \\lambda) &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j - \\lambda (\\bm{w}_j^\\top \\bm{w}_j - 1) \\\\ \\end{align}\\]  ã“ã‚Œã‚’$W_j$ã§å¾®åˆ†ã™ã‚Œã°ã€   \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial \\bm{w}_j} &amp;= 2 \\bm{S} \\bm{w}_j - 2 \\lambda \\bm{w}_j \\end{align}\\]  ã—ãŸãŒã£ã¦ã€ä»¥ä¸‹ã‚’æº€ãŸã™ã¨ãã«æ¥µå€¤ã‚’ã¨ã‚‹ã€‚   \\[\\bm{S} \\bm{w}_j = \\lambda \\bm{w}_j\\]  ã—ãŸãŒã£ã¦ã€$\\bm{w}_j$ã¯$\\bm{S}$ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚Šã€$\\lambda$ã¯å¯¾å¿œã™ã‚‹å›ºæœ‰å€¤ã§ã‚ã‚‹ã€‚  ã¾ãŸã€$s_j^2$ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‰ã‚Œã‚‹ã€‚ \\(\\begin{align} s_j^2 &amp;= \\bm{w}_j^\\top \\bm{S} \\bm{w}_j \\\\ &amp;= \\bm{w}_j^\\top \\lambda \\bm{w}_j \\\\ &amp;= \\lambda \\bm{w}_j^\\top \\bm{w}_j \\\\ &amp;= \\lambda \\end{align}\\)   PCAã§ã¯ã€$dâ€™$ å€‹ã®æ¬¡å…ƒã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ãŸã‚ã€$\\bm{S}$ã®å›ºæœ‰å€¤ãŒå¤§ãã„é †ã«å¯¾å¿œã™ã‚‹$dâ€™$ å€‹ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã¨ã‚Šä¸¦ã¹ãŸã‚‚ã®ã‚’$\\bm{W}$ã¨ã™ã‚Œã°ã‚ˆã„ã€‚   (å¾©ç¿’) å›ºæœ‰å€¤åˆ†è§£   è¡Œåˆ— $\\bm{A} \\in \\mathbb{R}^{d\\times d}$ ã®å›ºæœ‰å€¤åˆ†è§£ã¯ã€ç›´äº¤è¡Œåˆ—$\\bm{V}$ã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ã•ã‚Œã‚‹ã€‚   \\[\\bm{A} = \\bm{V} \\bm{\\Lambda} \\bm{V}^\\top\\]  ã“ã“ã§ã€   \\[\\bm{\\Lambda} = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_d)\\]  \\[\\bm{V} = \\begin{bmatrix} \\bm{v}_1 &amp; \\bm{v}_2 &amp; \\cdots &amp; \\bm{v}_d \\end{bmatrix}\\]  ã¨ã™ã‚‹ã€‚ $\\lambda_i$ ã¯ $\\bm{A}$ ã®å›ºæœ‰å€¤ã€$\\bm{v}_i$ ã¯å¯¾å¿œã™ã‚‹å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹ã€‚   ã¾ãŸã€ä»¥ä¸‹ã®ã‚ˆã†ã«å¤‰å½¢ã§ãã‚‹ã€‚   \\[\\begin{align} \\bm{A} &amp;= \\bm{V} \\bm{\\Lambda} \\bm{V}^\\top\\\\ &amp;=  \\begin{bmatrix} \\bm{v}_1 &amp; \\bm{v}_2 &amp; \\cdots &amp; \\bm{v}_d \\end{bmatrix} \\begin{bmatrix} \\lambda_1 \\bm{v}_1 \\\\ \\lambda_2 \\bm{v}_2 \\\\ \\vdots \\\\ \\lambda_d \\bm{v}_d \\end{bmatrix} \\\\ &amp;= \\sum_{i=1}^{d} \\lambda_i \\bm{v}_i \\bm{v}_i^\\top \\end{align}\\]  $\\bm{S}$ã®å›ºæœ‰å€¤/ãƒ™ã‚¯ãƒˆãƒ«ã¯$X$ã®ç‰¹ç•°å€¤åˆ†è§£ (SVD) ã§æ±‚ã‚ã‚‰ã‚Œã‚‹   å†æŽ²: $\\bm{S} \\in \\mathbb{R}^{d\\times d}$, $\\bm{X}^c \\in \\mathbb{R}^{n\\times d}$.   å¤‰æ›è¡Œåˆ— $\\bm{W} \\in \\mathbb{R}^{d\\times dâ€™}$ ã‚’æ±‚ã‚ã‚‹ãŸã‚ã«ã€$\\bm{S}$ã®å›ºæœ‰å€¤ã‚’æ±‚ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ ã“ã“ã§ã€$\\bm{S}$ ã‚’ $\\bm{X}^c$ ã«é–¢ã—ã¦æ›¸ãæ›ãˆã€$\\bm{X}^c$ ã®ç‰¹ç•°å€¤åˆ†è§£ (SVD) ã‚’ç”¨ã„ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹ã€‚   \\[\\begin{align} \\bm{S} &amp;= \\frac{1}{n} \\bm{X}^{c\\top} \\bm{X}^c \\\\ &amp;= \\frac{1}{n} \\left(\\bm{U}\\bm{\\Sigma} \\bm{V}^{\\top}\\right)^\\top \\left(\\bm{U}\\bm{\\Sigma} \\bm{V}^{\\top}\\right) \\\\ &amp;= \\frac{1}{n} \\bm{V} \\bm{\\Sigma}^\\top \\bm{U}^\\top \\bm{U} \\bm{\\Sigma} \\bm{V}^{\\top} \\\\ &amp;= \\frac{1}{n} \\bm{V} \\bm{\\Sigma}^{2} \\bm{V}^{\\top}\\quad (\\because \\bm{U} \\text{ is orthogonal})\\\\ &amp;= \\bm{V} \\left(\\frac{\\bm{\\Sigma}^{2}}{n} \\right) \\bm{V}^{\\top} \\\\ \\end{align}\\]  ã¤ã¾ã‚Šã€ä»¥ä¸‹ãŒè¨€ãˆã‚‹:     $\\bm{S}$ ã® å›ºæœ‰å€¤ã¯ [{$\\bm{X}^c$ ã‚’ç‰¹ç•°å€¤åˆ†è§£ã—ã¦å¾—ã‚‰ã‚Œã‚‹å›ºæœ‰å€¤ã‹ã‚‰ãªã‚‹è¡Œåˆ—$\\bm{\\Sigma}$} ã‚’ç”¨ã„ã¦è¡¨ã•ã‚Œã‚‹ $\\bm{\\Sigma}^{2}/n$ ]ã®å„å¯¾è§’æˆåˆ†   $\\bm{S}$ ã® å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã¯ $\\bm{X}^c$ ã‚’ç‰¹ç•°å€¤åˆ†è§£ã—ã¦å¾—ã‚‰ã‚Œã‚‹ $\\bm{V}$ ã®å„åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹ã€‚   ã—ãŸãŒã£ã¦ã€PCAã‚’è¡Œã†éš›ã¯ã€$\\bm{X}^{c\\top}\\bm{X}^c$ ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ã¯ãªãã€$\\bm{X}^c$ ã‚’ç‰¹ç•°å€¤åˆ†è§£ã—ã¦ã€$\\bm{V}$ ã®ã†ã¡ã€å¯¾å¿œã™ã‚‹å›ºæœ‰å€¤ãŒå¤§ãã„ $dâ€™$ å€‹ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã¨ã‚Šä¸¦ã¹ãŸã‚‚ã®ã‚’ $\\bm{W}$ ã¨ã™ã‚Œã°ã‚ˆã„ã€‚  ","categories": [],
        "tags": [],
        "url": "/notes/pca/",
        "teaser": "/assets/img/pca.png"
      },{
        "title": "âœ… Paper accepted to the COLING 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to COLING 2025.                                    Kamoda, G.,           Asai, A.,           Brassard, A.,      &amp;           Sakaguchi, K.  (2025).  Quantifying the Influence of Evaluation Aspects on Long-Form Response Assessment.  In Proceedings of the 31st International Conference on Computational Linguistics (COLING 2025).                                                ACL Anthology     &nbsp;                                    GitHub                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/coling2025-accept/",
        "teaser": null
      },{
        "title": "âœ… Paper accepted to the ICLR 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to ICLR 2025.                               Deguchi, H.,                Kamoda, G.,           Matsushita, Y.,           Taguchi, C.,           Waga, M.,           Suenaga, K.,      &amp;           Yokoi, S.  (2025).  SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches.  In The Thirteenth International Conference on Learning Representations (ICLR 2025).                                                OpenReview     &nbsp;                                    arXiv     &nbsp;                                    Project Page                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/iclr2025-accept/",
        "teaser": null
      },{
        "title": "ðŸŽ¤ Presentations at NLP 2025",
        "excerpt":"è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š (NLP 2025) ã«ã¦ã€ä»¥ä¸‹ã®6ä»¶ã®ç™ºè¡¨ãŒã‚ã‚Šã¾ã™ã€‚                                  å‡ºå£ç¥¥ä¹‹,                é´¨ç”°è±ª,           æ¾ä¸‹ç¥ä»‹,           ç”°å£æ™ºå¤§,           æœ«æ°¸å¹¸å¹³,           å’Œè³€æ­£æ¨¹,           æ¨ªäº•ç¥¥  (2024).  SoftMatcha: å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹æ¤œç´¢ã®ãŸã‚ã®æŸ”ã‚‰ã‹ãã‚‚é«˜é€Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒãƒ£ãƒ¼.  è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š, pp. 3310-3315.                                                 äºˆç¨¿                                       é´¨ç”°è±ª,           Benjamin Heinzerling,           ç¨²è‘‰é”éƒŽ,           å·¥è—¤æ…§éŸ³,           å‚å£æ…¶ç¥,           ä¹¾å¥å¤ªéƒŽ  (2025).  è¨€èªžãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰æŽ¢ã‚‹Detokenizationãƒ¡ã‚«ãƒ‹ã‚ºãƒ .  è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š, pp. 634-639.                                                 äºˆç¨¿                                  å°æž—æ˜¥æ–—,           åŽŸçŸ¥æ­£,                é´¨ç”°è±ª,           æ¨ªäº•ç¥¥  (2025).  å±¤ã®å†—é•·æ€§ã¨å±¤åŒå£«ã®ç‹¬ç«‹æ€§ã«åŸºã¥ãè¨€èªžãƒ¢ãƒ‡ãƒ«ã®å±¤äº¤æ›ã®æˆå¦ã®ç‰¹å¾´ã¥ã‘.  è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š, pp. 1751-1756.                                                 äºˆç¨¿                                  å·¥è—¤æ…§éŸ³,                é´¨ç”°è±ª,           å¡©é‡Žå¤§è¼,           éˆ´æœ¨æ½¤  (2025).  æ—¥æœ¬èªžãƒã‚¤ãƒˆç¬¦å·åŒ–ãƒžã‚¹ã‚¯è¨€èªžãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã¨åˆ†æž.  è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š, pp. 3356-3361.                                                 äºˆç¨¿     &nbsp;                                    ByBERT-JP     &nbsp;                                    ByGPT-JP                                  ä½ã€…æœ¨ç¦å²,           é«˜æ©‹è‰¯å…,                é´¨ç”°è±ª,           Benjamin Heinzerling,           å‚å£æ…¶ç¥,           ä¹¾å¥å¤ªéƒŽ  (2025).  LM ã¯æ—¥æœ¬ã®æ™‚ç³»åˆ—æ§‹é€ ã‚’ã©ã†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‹.  è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š, pp. 2642-2647.                                                 äºˆç¨¿                                  ä½è—¤å®äº®,                é´¨ç”°è±ª,           Benjamin Heinzerling,           å‚å£æ…¶ç¥  (2025).  è¨€èªžãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨è¡¨ç¾ã«ãŠã‘ã‚‹æ–‡æ³•æƒ…å ±ã®å±€æ‰€æ€§ã«ã¤ã„ã¦.  è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š, pp. 697-701.                                                 äºˆç¨¿                      -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/nlp-presentation/",
        "teaser": null
      },{
        "title": "ðŸ‘‘ 2 papers received awards at NLP 2025",
        "excerpt":"è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š (NLP 2025) ã«ã¦ã€ä»¥ä¸‹ã®2ä»¶ã®è«–æ–‡ãŒãã‚Œãžã‚Œã€Œè‹¥æ‰‹å¥¨åŠ±è³žã€ã€Œæ—¥æœ¬çµŒæ¸ˆæ–°èžç¤¾ CDIOå®¤è³žã€ã‚’å—è³žã—ã¾ã—ãŸã€‚                               å°æž—æ˜¥æ–—,           åŽŸçŸ¥æ­£,                é´¨ç”°è±ª,           æ¨ªäº•ç¥¥  (2025).  å±¤ã®å†—é•·æ€§ã¨å±¤åŒå£«ã®ç‹¬ç«‹æ€§ã«åŸºã¥ãè¨€èªžãƒ¢ãƒ‡ãƒ«ã®å±¤äº¤æ›ã®æˆå¦ã®ç‰¹å¾´ã¥ã‘.  è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š, pp. 1751-1756.                 è‹¥æ‰‹å¥¨åŠ±è³ž (20/487)                                                 äºˆç¨¿                                    ä½ã€…æœ¨ç¦å²,           é«˜æ©‹è‰¯å…,                é´¨ç”°è±ª,           Benjamin Heinzerling,           å‚å£æ…¶ç¥,           ä¹¾å¥å¤ªéƒŽ  (2025).  LM ã¯æ—¥æœ¬ã®æ™‚ç³»åˆ—æ§‹é€ ã‚’ã©ã†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‹.  è¨€èªžå‡¦ç†å­¦ä¼š ç¬¬31å›žå¹´æ¬¡å¤§ä¼š, pp. 2642-2647.                 æ—¥æœ¬çµŒæ¸ˆæ–°èžç¤¾ CDIOå®¤è³ž                                                 äºˆç¨¿                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/nlp-awards/",
        "teaser": null
      },{
        "title": "ðŸ‘‘ Completed Master's course and received the Dean's Award",
        "excerpt":"æ±åŒ—å¤§å­¦ æƒ…å ±ç§‘å­¦ç ”ç©¶ç§‘ å­¦ä½æŽˆä¸Žå¼ã«ã¦ã€å­¦ä½è¨˜ (ä¿®å£«: æƒ…å ±ç§‘å­¦) ã¨ã¨ã‚‚ã«ã€Œç ”ç©¶ç§‘é•·è³žã€ã‚’ã„ãŸã ãã¾ã—ãŸã€‚  ","categories": ["News"],
        "tags": [],
        "url": "/news/dean-award/",
        "teaser": null
      },{
        "title": "ðŸŒ¸ Enrollment in SOKENDAI for Doctoral Program",
        "excerpt":"ç·åˆç ”ç©¶å¤§å­¦é™¢å¤§å­¦ (SOKENDAI) æ—¥æœ¬èªžè¨€èªžç§‘å­¦ã‚³ãƒ¼ã‚¹ã®åšå£«å¾ŒæœŸèª²ç¨‹ã«é€²å­¦ã—ã¾ã—ãŸã€‚  JST BOOST ã®æ”¯æ´ã‚’å—ã‘ã‚‹SOKENDAIç‰¹åˆ¥ç ”ç©¶å“¡ã¨ã—ã¦ã€å›½ç«‹å›½èªžç ”ç©¶æ‰€ (NINJAL) ã§ç ”ç©¶æ´»å‹•ã‚’è¡Œã„ã¾ã™ã€‚  ","categories": ["News"],
        "tags": [],
        "url": "/news/sokendai-enrollment/",
        "teaser": null
      },{
        "title": "âœ… Paper accepted to the Findings of EMNLP 2025",
        "excerpt":"We are pleased to announce that our paper has been accepted to the Findings of EMNLP 2025.                               Inaba, T.,                Kamoda, G.,           Inui, K.,           Isonuma, M.,           Miyao, Y.,           Oseki, Y.,           Takagi, Y.,      &amp;           Heinzerling, B.  (2025).  How a Bilingual LM Becomes Bilingual: Tracing Internal Representations with Sparse Autoencoders.  In Findings of the Association for Computational Linguistics: EMNLP 2025.                                                OpenReview                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/emnlp2025findings-accept/",
        "teaser": null
      },{
        "title": "âœ… Paper accepted to the 8th BlackboxNLP Workshop",
        "excerpt":"We are pleased to announce that our paper has been accepted to the 8th BlackboxNLP Workshop.                               Takahashi, R.,                Kamoda, G.,           Heinzerling, B.,           Sakaguchi, K.,      &amp;           Inui, K.  (2025).  Understanding the Side Effects of Rank-One Knowledge Editing.  In BlackboxNLP 2025: The 8th Workshop on Analyzing and Interpreting Neural Networks for NLP.                                                arXiv                 -&gt; Publications  ","categories": ["News"],
        "tags": [],
        "url": "/news/bbnlp2025-accept/",
        "teaser": null
      },{
        "title": "ðŸŽ¤ Presentation at YANS 2025",
        "excerpt":"YANS 2025 ã«ã¦ä»¥ä¸‹ã®ç™ºè¡¨ãŒã‚ã‚Šã¾ã™ã€‚                                    é´¨ç”°è±ª,           ç†Šè°·é›„ä»‹,           æ¾äº•å­å¤ª,           æ¨ªäº•ç¥¥  (2025).  å¯†åº¦æ¯”ã®ç›´æŽ¥æŽ¨å®šã«åŸºã¥ãè¨€èªžãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›è¼ƒæ­£.  ç¬¬20å›žè¨€èªžå‡¦ç†è‹¥æ‰‹ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (YANS).                   -&gt; Publications  ","categories": ["Blog"],
        "tags": [],
        "url": "/blog/yans2025-presentation/",
        "teaser": null
      },{
        "title": "Test-time Augmentation for Factual Probing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202303-nlp-kamoda",
        "teaser": null
      },{
        "title": "é•·æ–‡ç”Ÿæˆã®å¤šé¢çš„è©•ä¾¡:äººæ‰‹è©•ä¾¡ã¨è‡ªå‹•è©•ä¾¡ã®å‘ä¸Šã‚’ç›®æŒ‡ã—ã¦",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202403-nlp-kamoda",
        "teaser": null
      },{
        "title": "è¨€èªžãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®çŸ¥è­˜å‰Šé™¤ï¼šé »å‡ºå®Ÿä½“ã®çŸ¥è­˜ã¯å‰¯ä½œç”¨ãŒç ´æ»…çš„",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202403-nlp-takahashi",
        "teaser": null
      },{
        "title": "å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±æŽ¨è–¦ãƒã‚¤ã‚¢ã‚¹ã®è¼ƒæ­£",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202405-jsai-kumagae",
        "teaser": null
      },{
        "title": "æŸ”ã‚‰ã‹ã„grep/KWICã«å‘ã‘ã¦ï¼šé«˜é€Ÿå˜èªžåˆ—ãƒžãƒƒãƒãƒ³ã‚°ã®åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã«ã‚ˆã‚‹é€£ç¶šåŒ–",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202409-yans-deguchi",
        "teaser": null
      },{
        "title": "äº‹å‰å­¦ç¿’â€“æ–‡è„ˆå†…å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ç”Ÿã˜ã‚‹é »åº¦ãƒã‚¤ã‚¢ã‚¹ã®è¼ƒæ­£",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202409-yans-ito",
        "teaser": null
      },{
        "title": "å±¤åŒå£«ã®æŽ¥ç¶šå¯èƒ½æ€§ã¨å„å±¤ãŒå½±éŸ¿ã‚’ä¸Žãˆã‚‹éƒ¨åˆ†ç©ºé–“ã®é‡ãªã‚Šåº¦åˆã„ã®é–¢ä¿‚æ€§",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202409-yans-kobayashi",
        "teaser": null
      },{
        "title": "SoftMatcha: å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹æ¤œç´¢ã®ãŸã‚ã®æŸ”ã‚‰ã‹ãã‚‚é«˜é€Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒãƒ£ãƒ¼",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-deguchi",
        "teaser": null
      },{
        "title": "è¨€èªžãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰æŽ¢ã‚‹Detokenizationãƒ¡ã‚«ãƒ‹ã‚ºãƒ ",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-kamoda",
        "teaser": null
      },{
        "title": "å±¤ã®å†—é•·æ€§ã¨å±¤åŒå£«ã®ç‹¬ç«‹æ€§ã«åŸºã¥ãè¨€èªžãƒ¢ãƒ‡ãƒ«ã®å±¤äº¤æ›ã®æˆå¦ã®ç‰¹å¾´ã¥ã‘",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-kobayashi",
        "teaser": null
      },{
        "title": "æ—¥æœ¬èªžãƒã‚¤ãƒˆç¬¦å·åŒ–ãƒžã‚¹ã‚¯è¨€èªžãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã¨åˆ†æž",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-kudo",
        "teaser": null
      },{
        "title": "LM ã¯æ—¥æœ¬ã®æ™‚ç³»åˆ—æ§‹é€ ã‚’ã©ã†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‹",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-sasaki",
        "teaser": null
      },{
        "title": "è¨€èªžãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨è¡¨ç¾ã«ãŠã‘ã‚‹æ–‡æ³•æƒ…å ±ã®å±€æ‰€æ€§ã«ã¤ã„ã¦",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202503-nlp-satoh",
        "teaser": null
      },{
        "title": "å¯†åº¦æ¯”ã®ç›´æŽ¥æŽ¨å®šã«åŸºã¥ãè¨€èªžãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›è¼ƒæ­£",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202509-yans-kamoda",
        "teaser": null
      },{
        "title": "Test-time Augmentation for Factual Probing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202311-emnlp-kamoda",
        "teaser": null
      },{
        "title": "Quantifying the Influence of Evaluation Aspects on Long-Form Response Assessment",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202501-coling-kamoda",
        "teaser": null
      },{
        "title": "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-iclr-deguchi",
        "teaser": null
      },{
        "title": "Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202504-naacl-kamoda",
        "teaser": null
      },{
        "title": "Understanding the Side Effects of Rank-One Knowledge Editing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202511-blackboxnlp-takahashi",
        "teaser": null
      },{
        "title": "How a Bilingual LM Becomes Bilingual: Tracing Internal Representations with Sparse Autoencoders",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202511-emnlp-inaba",
        "teaser": null
      },{
        "title": "Can Language Models Handle a Non-Gregorian Calendar?",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/202311-arxiv-sasaki",
        "teaser": null
      },{
    "title": "Notes",
    "excerpt":"","url": "/notes/"
  },{
    "title": "Publications & Activities",
    "excerpt":"## Education - Doctoral Student [2025.04 - ] Graduate Institute for Advanced Studies, SOKENDAI.Supervisor: Assoc. Prof. Sho Yokoi - Master of Information Science [2023.04 - 2025.03]Graduate School of Information Sciences, Tohoku University.Supervisor: Prof. Jun Suzuki & Assoc. Prof. Keisuke Sakaguchi Dean Award (4/126) - Bachelor of Engineering [2020.04 - 2023.03]School of Engineering, Tohoku University.Supervisor: Prof. Kentaro Inui & Assoc. Prof. Keisuke SakaguchiEarly Graduation (1/252)  ## International Conferences          {% for publication in site.pubInternationalConferences reversed %}              {% include pub-apa-international-conf.html  %}            {% endfor %}           ## Domestic Conferences          {% for publication in site.pubDomesticConferences reversed %}              {% include pub-anlp-domestic-conf.html  %}            {% endfor %}        {% if site.pubPreprint.size > 0 %} ## Preprints         {% for publication in site.pubPreprint reversed %}              {% include pub-apa-international-conf.html  %}            {% endfor %}      {% endif %}  {% if site.experiences.size > 0 %} ## Experiences         {% for experience in site.experiences reversed %}              {% include experiences.html  %}            {% endfor %}      {% endif %}           {% for talk in site.oubInvitedTalks reversed %}                       {% for speaker in talk.speakers %}           {{ speaker.name }}           {%- if forloop.last == false -%}             ,           {% endif %}         {%- endfor -%}         .         {{ talk.title }}.         {{ talk.event_name }},         {{ talk.month }}         {{ talk.year }}.         {% if talk.links %}           [           {%- for link in talk.links -%}             {{ link.name}}{% if forloop.last == false %}, {% endif %}           {%- endfor -%}           ]         {% endif %}            {% endfor %}         ","url": "/cv/"
  },{
    "title": "",
    "excerpt":"","url": "/index.html"
  },{
    "title": " - page 2",
    "excerpt":"","url": "/page/2/index.html"
  }]
